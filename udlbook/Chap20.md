好的，请看我的翻译和解答。

***

# 第二十章
# 深度学习为何有效？

本章与前面的章节有所不同。我们不再呈现既定的成果，而是提出关于深度学习如何以及为何如此有效的问题。这些问题在教科书中很少被讨论。然而，认识到（尽管本书的书名如此）我们对深度学习的理解仍然有限，这一点很重要。

我们将论证，深度网络易于训练是令人惊讶的，它们能够泛化也是令人惊讶的。然后，我们将依次考虑这两个主题。我们会列举影响训练成功的因素，并讨论关于深度网络损失函数的已知情况。接着，我们考虑影响泛化的因素。最后，我们以讨论网络是否需要过参数化和深度来结束。

## 20.1 反对深度学习的理由

MNIST-1D 数据集（图8.1）只有四十个输入维度和十个输出维度。只要每层有足够的隐藏单元，一个两层的全连接网络就能完美地对10000个MNIST-1D训练数据点进行分类，并能相当好地泛化到未见过的样本（图8.10a）。实际上，我们现在认为理所当然的是，只要有足够的隐藏单元，深度网络几乎能完美地对任何训练集进行分类。我们也理所当然地认为，拟合后的模型将能泛化到新数据。然而，**训练过程为何应该成功，以及由此产生的模型为何应该泛化，这两点都并非显而易见**。本节将论证这两种现象都令人惊讶。

### 20.1.1 训练

在一个包含10000个MNIST-1D训练样本的任务中，一旦每层有43个隐藏单元或约4000个参数，一个两层全连接网络的性能就达到了完美（图8.10）。然而，找到一个任意非凸函数的全局最小值是**NP难 (NP-hard)** 的（Murty & Kabadi, 1987），这对于某些神经网络损失函数也是如此（Blum & Rivest, 1992）。令人瞩目的是，拟合算法并没有陷入局部最小值或卡在鞍点附近，并且它能够有效地利用多余的模型容量来拟合任何地方未解释的训练数据。

当参数远多于训练数据时，这种成功或许不那么令人惊讶。然而，这是否是普遍情况是有争议的。AlexNet有约6000万个参数，训练数据约100万个点。然而，让事情复杂化的是，每个训练样本都被增广为2048个变换。GPT-3有1750亿个参数，训练数据有3000亿个词元。没有明确的证据表明这两个模型中任何一个被过参数化了，但它们都成功地被训练了。
简而言之，我们能够可靠且高效地拟合深度网络是令人惊讶的。要么是数据、模型、训练算法，要么是这三者的某种组合，必须具有一些特殊的属性，才能使这一切成为可能。

### 20.1.2 泛化

如果说神经网络的高效拟合令人吃惊，那么它们对新数据的泛化能力则令人**瞠目结舌**。首先，**先验地看，典型的数
据集是否足以表征输入/输出映射，这一点并不明显**。**维度的诅咒 (curse of dimensionality)** 意味着训练数据集与可能的输入相比是微不足道的；如果MNIST-1D数据的40个输入中的每一个都被量化为10个可能的值，那么将有 $10^{40}$ 种可能的输入，这比训练样本的数量多了 $10^{36}$ 倍。

其次，深度网络描述了非常复杂的函数。例如，一个用于MNIST-1D、有两个宽度为400的隐藏层的全连接网络，可以创建多达 $10^{42}$ 个线性区域的映射。这大约是每个训练样本 $10^{38}$ 个区域，因此在训练的任何阶段，这些区域中只有极少数包含数据；尽管如此，那些确实遇到数据点的区域，却能约束其余区域做出合理的行为。

第三，**泛化能力随着参数增多而变好**（图8.10）。上一段中的模型有177,201个参数。假设它每个参数可以拟合一个训练样本，它就有167,201个**多余的自由度 (spare degrees of freedom)**。这种过剩给了模型在训练数据之间**几乎可以做任何事情**的余地，然而它的行为却是合理的。

### 20.1.3 深度学习不合理的有效性

总而言之，我们既不清楚为何我们应该能够拟合深度网络，也不清楚它们为何应该泛化。**先验地看，深度学习本不应该奏效。然而它却成功了。**本章将探究其原因。第20.2-20.3节描述了我们关于拟合深度网络及其损失函数的已知情况。第20.4-20.6节则审视了泛化问题。

---

> **图 20.1 拟合随机数据**
> 使用SGD在CIFAR-10数据集上训练的AlexNet架构的损失。当像素是从与原始图像数据集具有相同均值和方差的高斯随机分布中抽取时，模型仍然可以被拟合（尽管速度更慢）。当标签被随机化时，模型仍然可以被拟合（尽管速度更慢）。改编自 Zhang et al. (2017a)。

---

## 20.2 影响拟合性能的因素

图6.4显示，非线性模型的损失函数可以有局部最小值和鞍点。然而，我们能够可靠地将深度网络拟合到复杂的训练集上。例如，图8.10显示了在MNIST-1D, MNIST, 和CIFAR-100上的完美训练性能。本节将探讨可能解决这一矛盾的因素。

### 20.2.1 数据集

重要的是要认识到，我们无法学习**任何**函数。考虑一个从每个可能的 $28 \times 28$ 二值图像到一个十个类别之一的完全随机的映射。由于这个函数没有结构，唯一的办法就是记住这 $2^{784}$ 种分配。然而，在MNIST数据集（图8.10和15.15）上训练一个模型很容易，该数据集包含60,000个 $28 \times 28$ 的图像，标记为十个类别之一。对这一矛盾的一种解释可能是，找到全局最小值很容易，因为我们所近似的真实世界函数相对简单。¹

这个假设由Zhang et al. (2017a)进行了研究，他们在CIFAR-10图像分类数据集（包含50,000个 $32 \times 32 \times 3$ 的图像，标记为10个类别之一）上训练了AlexNet，当 (i) 每个图像被高斯噪声替换，以及 (ii) 十个类别的标签被随机置换时（图20.1）。这些变化减慢了学习速度，但网络仍然能够很好地拟合这个有限的数据集。这表明数据集的属性并非关键。

### 20.2.2 正则化

对于模型易于训练的另一个可能解释是，一些正则化方法，如L2正则化（权重衰减），使损失曲面更平坦、更凸。然而，Zhang et al. (2017a)发现，拟合随机数据既不需要L2正则化也不需要Dropout。这并未排除由于拟合算法的有限步长而产生的**隐式正则化**（9.2节）。然而，这种效应随着学习率的增加而增加（方程9.9），而模型拟合并不会因为学习率更大而变得更容易。

### 20.2.3 随机训练算法

第6章论证了SGD算法可能允许优化轨迹在训练期间在“山谷”之间移动。然而，Keskar et al. (2017)表明，许多模型（包括全连接和卷积网络）可以在许多数据集（包括CIFAR-100和MNIST）上，用5000-6000张图像的非常大的批次几乎完美地拟合。这消除了大部分随机性，但训练仍然成功。

图20.2显示了四个全连接模型在使用**全批量（即非随机）梯度下降**拟合4000个带有随机标签的MNIST-1D样本时的训练结果。这里没有显式正则化，学习率被设置为一个小的常数值0.0025以最小化隐式正则化。在这里，从数据到标签的真实映射没有结构，训练是确定性的，也没有正则化，然而训练误差仍然降至零。这表明这些损失函数可能真的没有局部最小值。

> ¹在本章中，我们宽泛地使用“全局最小值”一词来指代所有数据都被正确分类的任何解。我们无从得知是否存在其他损失更低的解。

---

> **图 20.2 MNIST-1D训练**
> 四个全连接网络被用来拟合4000个带有随机标签的MNIST-1D样本，使用的是全批量梯度下降、He初始化、无动量或正则化，以及学习率0.0025。具有1,2,3,4层的模型分别有298, 100, 75, 和 63个隐藏单元/层，参数量分别为15208, 15210, 15235, 和 15139。所有模型都成功训练，但更深的模型需要更少的轮次。

---

### 20.2.4 过参数化

**过参数化 (Overparameterization)** 几乎可以肯定是一个有助于训练变得容易的重要因素。它意味着存在一个庞大的退化解族，因此可能总存在一个可以修改参数以降低损失的方向。Sejnowski (2020) 建议，“……解的退化性将问题的性质从大海捞针变为草堆里找草堆。”
在实践中，网络通常被过参数化一到两个数量级（图20.3）。然而，数据增强使得精确的陈述变得困难。增强可能会将数据增加几个数量级，但这些是对现有样本的操作，而不是独立的新数据点。此外，图8.10显示，当参数数量与数据点相同或更少时，神经网络有时也能很好地拟合训练数据。这大概是由于来自同一底层函数的训练样本存在冗余。
一些理论收敛结果表明，在某些情况下，当网络充分过参数化时，SGD会收敛到全局最小值。例如，Du et al. (2019b)表明，对于具有足够多隐藏单元的浅层全连接ReLU网络和最小二乘损失，随机初始化的SGD会收敛到全局最小值。类似地，Du et al. (2019a)考虑了当激活函数是平滑且利普希茨时，深度、残差和卷积网络。Zou et al. (2020)分析了在使用hinge损失的深度全连接网络上的梯度下降收敛性。Allen-Zhu et al. (2019)考虑了带有ReLU函数的深度网络。

如果一个神经网络被充分过参数化，以至于它可以记住任何固定大小的数据集，那么所有的**驻点 (stationary points)** 都会变成全局最小值 (Livni et al., 2014; Nguyen & Hein, 2017, 2018)。其他结果表明，如果网络足够宽，损失高于全局最小值的局部最小值是罕见的（见 Choromanska et al., 2015; Pascanu et al., 2014; Pennington & Bahri, 2017）。Kawaguchi et al. (2019)证明，随着网络变得更深、更宽或两者兼有，局部最小值处的损失会更接近于全局最小值处的损失（对于平方损失函数）。

这些理论结果很有趣，但通常对网络结构做出不切实际的假设。例如，Du et al. (2019a)表明，当网络宽度 $D$（即隐藏单元数）为 $\Omega[I^4K^2]$ 时，残差网络会收敛到零训练损失，其中 $I$ 是训练数据量，$K$ 是网络深度。类似地，Nguyen & Hein (2017)假设网络的宽度大于数据集大小，这在大多数实际场景中是不现实的。过参数化似乎很重要，但理论尚不能解释经验上的拟合性能。

---

> **图 20.3 过参数化**
> ImageNet上卷积网络性能与过参数化（以数据集大小的倍数计）的函数关系。大多数模型比训练样本多10-100倍的参数。比较的模型包括ResNet (He et al., 2016a,b), DenseNet (Huang et al., 2017b), Xception (Chollet, 2017), EfficientNet (Tan & Le, 2019), Inception (Szegedy et al., 2017), ResNeXt (Xie et al., 2017), 和 AmoebaNet (Cubuk et al., 2019)。

---

### 20.2.5 激活函数

众所周知，激活函数也会影响训练难度。激活函数仅在输入范围的一小部分上发生变化的网络比ReLU（其在输入范围的一半上变化）或Leaky ReLU（其在整个输入范围上变化）更难拟合；例如，sigmoid和tanh非线性函数（图3.13a）在其尾部梯度很浅；在激活函数接近常数的地方，训练梯度接近于零，因此改进模型的机制极其微弱。

### 20.2.6 初始化

另一个可能的解释是Xavier/He初始化将参数设置到易于优化的值。当然，对于更深的网络，这种初始化对于避免梯度爆炸和消失是必要的，因此在一种平凡的意义上，初始化对于训练成功至关重要。然而，对于较浅的网络，权重的初始方差不太重要。Liu et al. (2023c) 在1000个MNIST数据点上训练了一个3层的全连接网络，每层有200个隐藏单元。他们发现，当方差从He提出的方差增加时，需要更多的迭代来拟合训练数据（图20.4），但这最终并未妨碍拟合。因此，初始化并未阐明为何拟合神经网络很容易，尽管梯度爆炸/消失确实揭示了在有限精度算术下使训练变得困难的初始化。

### 20.2.7 网络深度

当深度变得非常大时，由于梯度爆炸和消失（图7.7）以及梯度破碎（图11.3），神经网络更难拟合。然而，这些（可以说是）实际的数值问题。没有明确的证据表明，随着网络深度的增加，底层的损失函数在根本上或多或少是凸的。图20.2确实显示，对于带有随机标签和He初始化的MNIST数据，更深的网络在更少的迭代中训练。然而，这可能是因为 (i) 更深网络中的梯度更陡峭，或者 (ii) He初始化只是让更宽、更浅的网络从离最优参数更远的地方开始。

Frankle & Carbin (2019)表明，对于像VGG这样的小型网络，如果你 (i) 训练网络，(ii) 剪除幅度最小的权重，并 (iii) 从相同的初始权重重新训练，你可以获得相同或更好的性能。如果权重被随机地重新初始化，这就不起作用了。他们得出结论，原始的过参数化网络包含小的、可训练的子网络，这些子网络足以提供性能。他们称之为**彩票假设 (lottery ticket hypothesis)**，并将子网络称为**中奖彩票 (winning tickets)**。这表明子网络的有效数量可能在拟合中起着关键作用。这（或许）随网络深度对固定的参数数量而变化，但对这个想法的精确描述尚缺乏。

## 20.3 损失函数的性质

上一节讨论了有助于神经网络易于训练的因素。参数的数量（过参数化的程度）和激活函数的选择都很重要。令人惊讶的是，数据集的选择、拟合算法的随机性以及正则化的使用似乎并不重要。没有明确的证据表明（对于固定的参数数量）网络深度很重要（除了由梯度爆炸/消失/破碎引起的数值问题）。本节从一个不同的角度探讨同一个主题，通过考虑损失函数的经验性质。这些证据大部分来自全连接网络和CNNs；Transformer网络的损失函数了解得较少。

### 20.3.1 多个全局最小值

我们预期深度网络的损失函数有一个庞大的等价全局最小值族。在全连接网络中，每一层的隐藏单元及其相关联的权重可以被置换而不改变输出。在卷积网络中，适当地置换通道和卷积核不会改变输出。我们可以在任何ReLU函数之前乘以权重，在之后除以一个正数而不改变输出。使用BatchNorm会引入另一组冗余，因为每个隐藏单元或通道的均值和方差都被重置了。
上述修改都对**每个输入**产生相同的输出。然而，全局最小值仅取决于在**训练数据点**上的输出。在过参数化网络中，还会有一些解族在数据点上行为相同，但在它们之间行为不同。所有这些也都是全局最小值。

### 20.3.2 到达最小值的路径

Goodfellow et al. (2015b)考虑了初始参数和最终值之间的一条直线。他们表明，沿着这条线的损失函数通常是单调递减的（有时在开始附近有一个小凸起）。对于几种不同类型的网络和激活函数，都观察到了这种现象（图20.5a）。

当然，真实的优化轨迹并非沿直线进行。然而，Li et al. (2018b)发现它们确实位于低维子空间中。他们将此归因于损失景观中存在大的、近乎凸的区域，这些区域很早就捕获了轨迹并将其引导到少数几个重要的方向。令人惊讶的是，Li et al. (2018a)表明，如果优化被约束在一个随机的低维子空间中，网络仍然能很好地训练（图20.6）。

Li & Liang (2018)表明，在训练期间参数的相对变化随着网络宽度的增加而减小；对于更宽的网络，参数从较小的值开始，变化占这些值的比例较小，并在更少的步骤中收敛。

### 20.3.3 最小值之间的连接

Goodfellow et al. (2015b)研究了两个独立找到的最小值之间的直线上的损失函数。他们发现两者之间的损失有显著的增加（图20.5b）；好的最小值通常不是线性连接的。然而，Frankle et al. (2020)表明，如果网络最初被相同地训练，然后允许通过使用不同的SGD噪声和增强来发散，那么这种增加就消失了。这表明解在训练早期受到约束，并且某些最小值族是线性连接的。
Draxler et al. (2018)在CIFAR-10数据集上找到了具有良好（但不同）性能的最小值。然后他们表明，可以在两者之间构建一条路径，其中损失函数沿着该路径保持较低。他们得出结论，存在一个单一的、连通的低损失流形（图20.7）。随着网络宽度和深度的增加，这似乎越来越成为现实。Garipov et al. (2018)和Fort & Jastrzębski (2019)提出了其他连接最小值的方案。

---

> **图 20.4 初始化与拟合**
> 一个三层的全连接网络，每层有200个隐藏单元，在1000个MNIST样本上使用AdamW、独热目标和均方误差损失进行训练。当使用He初始化的更大倍数时，拟合网络需要更长的时间，但这不会改变结果。这可能仅仅反映了权重必须移动的额外距离。改编自 Liu et al. (2023c)。

> **图 20.5 穿过损失函数的线性切片**
> a) 一个两层的全连接ReLU网络在MNIST上进行训练。从初始参数($\delta=0$)开始到训练后参数($\delta=1$)结束的直线上，损失是单调下降的。b) 然而，在这个在MNIST上的两层全连接MaxOut网络中，一个解($\delta=0$)和另一个解($\delta=1$)之间的直线上，损失有增加。改编自 Goodfellow et al. (2015b)。

> **图 20.6 子空间训练**
> 一个有两层隐藏层、每层200个单元的全连接网络在MNIST上进行训练。参数使用标准方法初始化，但随后被约束在一个随机子空间内。当这个子空间是750维（被称为内在维度）时，性能达到无约束水平的90%，这只是原始参数的0.4%。改编自 Li et al. (2018a)。

---

## 20.4 影响泛化的因素

最后两节考虑了决定网络是否成功训练的因素以及关于神经网络损失函数的已知情况。本节考虑决定网络泛化程度的因素。这补充了关于正则化的讨论（第9章），该讨论明确旨在鼓励泛化。

### 20.4.1 训练算法

由于深度网络通常是过参数化的，训练过程的细节决定了算法收敛到哪个退化最小值族。这些细节中的一些可靠地改善了泛化。

LeCun et al. (2012)表明，SGD比全批量梯度下降泛化得更好。有人认为SGD比Adam泛化得更好（例如，Wilson et al., 2017; Keskar & Socher, 2017），但最近的研究表明，如果超参数搜索做得仔细，差异很小（Choi et al., 2019）。Keskar et al. (2017)表明，当不使用其他形式的正则化时，深度网络用较小的批次大小泛化得更好。众所周知，较大的学习率也倾向于更好地泛化（例如，图9.5）。Jastrzębski et al. (2018), Goyal et al. (2018), 和 He et al. (2019)认为批量大小/学习率的比率很重要。He et al. (2019)表明，这个比率与泛化程度之间有显著的相关性，并证明了神经网络的泛化界限，该界限与此比率有正相关（图20.10）。

这些观察与SGD隐式地向损失函数添加正则化项（9.2节）的发现是一致的，并且它们的量级取决于学习率。参数的轨迹被这个正则化改变了，它们收敛到损失函数的一个泛化良好的部分。

### 20.4.2 最小值的平坦度

至少可以追溯到Hochreiter & Schmidhuber (1997a)的推测认为，损失函数中的**平坦最小值 (flat minima)** 比**尖锐最小值 (sharp minima)** 泛化得更好（图20.11）。通俗地说，如果最小值更平坦，那么估计参数中的小误差就不那么重要了。这也可以从各种理论观点来论证。例如，**最小描述长度理论 (minimum description length theory)** 表明，用更少比特指定的模型泛化得更好（Rissanen, 1983）。对于宽的最小值，存储权重所需的精度较低，所以它们应该更好地泛化。

平坦度可以通过 (i) 最小值周围训练损失相似的连通区域的大小（Hochreiter & Schmidhuber, 1997a），(ii) 最小值周围的二阶曲率（Chaudhari et al., 2019），或 (iii) 最小值邻域内的最大损失（Keskar et al., 2017）来测量。然而，需要谨慎；由于ReLU函数的非负同质性，估计的平坦度可能会受到网络平凡的重参数化的影响（Dinh et al., 2017）。
尽管如此，Keskar et al. (2017)改变了批量大小和学习率，并表明平坦度与泛化相关。Izmailov et al. (2018)将学习轨迹中多个点的权重平均在一起。这既导致了在最小值处更平坦的测试和训练曲面，又改善了泛化。其他的正则化技术也可以通过这个视角来看待。例如，平均模型输出（集成）也可能使测试损失曲面更平坦。Kleinberg et al. (2018)表明，训练期间大的梯度方差有助于避免尖锐区域。这可能解释了为什么减小批量大小和增加噪声有助于泛化。
上述研究考虑了单个模型和训练集的平坦度。然而，仅凭锐度并不是一个预测数据集之间泛化的好标准；当CIFAR数据集中的标签被随机化（使得泛化不可能）时，最小值的锐度没有相应的增加（Neyshabur et al., 2017）。

### 20.4.3 架构

一个网络的**归纳偏置 (inductive bias)** 由其架构决定，而明智的模型选择可以极大地改善泛化。第十章介绍了卷积网络，它被设计用来处理规则网格上的数据；它们隐式地假设输入统计在整个输入中是相同的，因此它们在位置之间共享参数。类似地，Transformer适合于建模对置换不变的数据，而图神经网络适合于在不规则图上表示的数据。将架构与数据的属性相匹配，比通用的、全连接的架构更能改善泛化（见图10.8）。

## 20.7 总结

本章论证了深度学习的成功是令人惊讶的。我们讨论了优化高维损失函数的挑战，并认为**过参数化**和**激活函数的选择**是使这在深度网络中变得可行的两个最重要的因素。我们看到，在训练期间，参数通过一个**低维子空间**移动到一个**连通的全局最小值族**之一，并且局部最小值是不明显的。
神经网络的泛化也随着过参数化而改善，尽管其他因素，如**最小值的平坦度**和**架构的归纳偏置**，也很重要。似乎**大量的参数**和**多个网络层**都是良好泛化所必需的，尽管我们尚不清楚为什么。
许多问题仍未得到解答。我们目前没有任何规定性的理论，能让我们预测在什么情况下训练和泛化会成功或失败。我们不知道深度网络学习的极限，或者是否有可能存在更高效的模型。我们不知道在同一个模型中是否存在能更好地泛化的参数。深度学习的研究仍然由经验演示驱动。这些无疑是令人印象深刻的，但它们尚未与我们对深度学习机制的理解相匹配。

***

### 习题

**问题 20.1** 考虑ImageNet图像分类任务，其中输入图像包含 $224 \times 224 \times 3$ 个RGB值。考虑将这些输入粗略地量化为每个RGB值十个区间，并使用约 $10^7$ 个训练样本进行训练。每个训练数据点对应多少种可能的输入？

**思路与解答：**

1.  **总输入变量数**: $224 \times 224 \times 3 = 150,528$。
2.  **每个变量的可能性**: 10种（因为量化为10个区间）。
3.  **总可能的输入组合数**: $10^{150,528}$。
4.  **训练样本数**: $\approx 10^7$。
5.  **每个训练数据点对应的可能输入数**:
    $\frac{\text{总可能输入数}}{\text{训练样本数}} = \frac{10^{150,528}}{10^7} = 10^{150,521}$。
    这是一个天文数字，体现了**维度的诅咒**。

**问题 20.2** 考虑图20.1。你认为为什么当像素被随机化时，算法拟合数据的速度比标签被随机化时要快？

**思路与解答：**

*   **随机化标签**: 破坏了**输入和输出之间所有的语义关系**。网络必须从零开始，完全依靠暴力记忆来拟合一个没有任何规律的映射。这是最困难的学习任务。
*   **随机化像素**: 虽然破坏了图像的**空间结构**，使得卷积核的局部性假设失效，但它**保留了输入和输出之间的（现在是随机的）映射关系**。更重要的是，它保留了输入数据的**低层统计特性**，比如像素值的整体分布、均值、方差等。网络（尤其是其全连接部分）仍然可以利用这些统计特性。此外，对于同一个类别，尽管空间结构被打乱，但像素值的直方图可能仍然保留了一些类别信息。
*   **结论**: 随机化标签比随机化像素更彻底地摧毁了数据中的“可学习结构”。因此，拟合随机化标签的数据需要更长的时间。

**问题 20.3** 图20.2显示了一个使用固定学习率的非随机拟合过程成功地拟合了随机数据。这是否意味着损失函数没有局部最小值？这是否意味着函数是凸的？证明你的答案，如果你认为任一陈述是错误的，请给出一个反例。

**思路与解答：**

1.  **这是否意味着没有局部最小值？**
    **不一定**。成功拟合（训练误差降至零）只意味着优化算法从一个特定的初始化点出发，沿着一条路径到达了一个全局最小值，而**没有被任何局部最小值困住**。这并不能排除在损失曲面的其他地方存在局部最小值的可能性。也许初始化点恰好在一个通往全局最小值的“康庄大道”上。

2.  **这是否意味着函数是凸的？**
    **绝对不是**。神经网络的损失函数（除了最简单的线性网络）几乎总是**非凸的**。
    *   **理由**: 神经网络具有对称性。例如，在一个隐藏层中，如果我们交换两个神经元的所有输入和输出权重，网络的函数行为完全不变，因此损失值也不变。这意味着，如果存在一个最优解 $\boldsymbol{\phi}^*$，那么通过置换神经元得到的另一个参数配置 $\boldsymbol{\phi}'$ 也是一个最优解。一个函数如果有多个孤立的全局最小值，它就不是凸的。
    *   **反例**: 一个简单的一维函数 $f(x) = x^4 - 2x^2$ 是非凸的，它在 $x=-1$ 和 $x=1$ 处有两个全局最小值。如果我们的初始化点在 $x > 1$ 的区域，一个简单的梯度下降就能成功找到 $x=1$ 这个全局最小值，但这并不能证明函数是凸的，也不能证明 $x=-1$ 这个最小值不存在。