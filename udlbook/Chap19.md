好的，请看我的翻译和解答。

***

# 第十九章
# 强化学习

**强化学习 (Reinforcement learning, RL)** 是一个序列决策框架，其中**智能体 (agents)** 在一个**环境 (environment)** 中学习执行**动作 (actions)**，其目标是最大化收到的**奖励 (rewards)**。
例如，一个强化学习算法可以控制一个视频游戏（环境）中角色（智能体）的移动（动作），旨在最大化得分（奖励）。在机器人学中，一个强化学习算法可以控制一个机器人（智能体）在世界（环境）中的运动（动作）来完成一项任务（获得奖励）。在金融领域，一个强化学习算法可以控制一个虚拟交易员（智能体）在金融交易所（环境）中买卖资产（动作），以最大化利润（奖励）。

考虑学习下象棋。在这里，如果智能体赢了、输了或平局，游戏结束时会有一个+1、-1或0的奖励，而在其他所有时间步奖励都为0。这说明了强化学习的挑战。首先，**奖励是稀疏的**；在这里，我们必须下完整盘棋才能收到反馈。其次，奖励与导致它的动作在**时间上是分离的**；决定性的优势可能在胜利前三十步就已获得。我们必须将奖励与这个关键动作联系起来。这被称为**时间信用分配问题 (temporal credit assignment problem)**。第三，**环境是随机的**；对手在相同情况下不总是走同一着棋，所以很难知道一个动作是真的好还是仅仅是运气好。最后，智能体必须在**探索 (exploring)** 环境（例如，尝试新的开局）和**利用 (exploiting)** 已知知识（例如，坚持使用以前成功的开局）之间取得平衡。这被称为**探索-利用权衡 (exploration-exploitation trade-off)**。

强化学习是一个总括性的框架，不一定需要深度学习。然而，在实践中，最先进的系统通常使用深度网络。它们对环境（视频游戏显示、机器人传感器、金融时间序列或棋盘）进行编码，并将此直接或间接地映射到下一个动作（图1.13）。

## 19.1 马尔可夫决策过程、回报和策略

强化学习将对环境的观测映射到动作，旨在最大化一个与所获奖励相关的数值量。在最常见的情况下，我们学习一个在**马尔可夫决策过程 (Markov decision process)** 中最大化**期望回报 (expected return)** 的**策略 (policy)**。本节将解释这些术语。

---

> **图 19.1 马尔可夫过程**
> 一个马尔可夫过程由一组状态和转移概率 $\text{Pr}(s_{t+1}|s_t)$ 组成，该概率定义了在当前状态为 $s_t$ 时移动到状态 $s_{t+1}$ 的概率。a) 企鹅可以在冰上的16个不同位置（状态）访问。b) 冰面很滑，所以在每个时间点，它移动到任何相邻状态的概率是均等的。例如，在位置6，它有25%的几率移动到状态2, 5, 7和10。来自这个过程的一条轨迹 $\tau = [s_1, s_2, s_3, \dots]$ 由一个状态序列组成。

---

### 19.1.1 马尔可夫过程

**马尔可夫过程 (Markov process)** 假设世界总是处于一组可能状态中的一个。**马尔可夫**这个词意味着处于一个状态的概率仅取决于前一个状态，而不取决于之前的状态。状态之间的变化由转移概率 $\text{Pr}(s_{t+1}|s_t)$ 捕获，即在当前状态为 $s_t$ 时移动到下一个状态 $s_{t+1}$ 的概率，其中 $t$ 索引时间步。因此，一个马尔可夫过程是一个演化系统，它产生一个状态序列 $s_1, s_2, s_3, \dots$（图19.1）。

### 19.1.2 马尔可夫奖励过程

**马尔可夫奖励过程 (Markov reward process)** 扩展了马尔可夫过程，以包含一个在给定状态 $s_t$ 时，在下一个时间步收到的可能奖励 $r_{t+1}$ 的分布 $\text{Pr}(r_{t+1}|s_t)$。这产生一个状态和相关奖励的序列 $s_1, r_2, s_2, r_3, s_3, r_4, \dots$（图19.2）。马尔可夫奖励过程还包括一个**折扣因子 (discount factor)** $\gamma \in (0, 1]$，用于计算在时间 $t$ 的**回报 (return)** $G_t$：

$$
G_t = \sum_{k=0}^\infty \gamma^k r_{t+k+1}.
\tag{19.1}
$$

回报是未来累积折扣奖励的总和；它衡量了处于这条轨迹上的未来收益。小于一的折扣因子使得时间上更近的奖励比更远的奖励更有价值。

---

> **图 19.2 马尔可夫奖励过程**
> 这将一个奖励 $r_{t+1}$ 的分布 $\text{Pr}(r_{t+1}|s_t)$ 与每个状态 $s_t$ 关联起来。a) 在这里，奖励是确定性的；如果企鹅落在一条鱼上，它将获得+1的奖励，否则为0。轨迹 $\tau$ 现在由一个交替的状态和奖励序列 $s_1, r_2, s_2, r_3, s_3, r_4, \dots$ 组成，在八步后终止。序列的回报 $G_t$ 是未来折扣奖励的总和，这里的折扣因子 $\gamma=0.9$。b-c) 随着企鹅沿着轨迹前进并越来越接近奖励，回报增加。

---

### 19.1.3 马尔可夫决策过程

**马尔可夫决策过程 (Markov decision process)** 或 **MDP** 在每个时间步增加了一组可能的动作。动作 $a_t$ 改变了转移概率，现在写作 $\text{Pr}(s_{t+1}|s_t, a_t)$。奖励也可以依赖于动作，现在写作 $\text{Pr}(r_{t+1}|s_t, a_t)$。一个MDP产生一个序列 $(s_1, a_1, r_2), (s_2, a_2, r_3), (s_3, a_3, r_4), \dots$，由在后续时间步收到的状态 $s_t$、动作 $a_t$ 和奖励 $r_{t+1}$ 组成（图19.3）。执行动作的实体被称为**智能体 (agent)**。

---

> **图 19.3 马尔可夫决策过程**
> a) 智能体（企鹅）可以在每个状态执行一组动作。b) 在这里，四个动作对应于上、右、下、左移动。c) 对于任何状态（这里是状态6），动作会改变移动到下一个状态的概率。企鹅有50%的概率向预定方向移动，但冰面很滑，所以它可能会以均等的概率滑到其他相邻位置之一。因此，在(a)中，采取的动作（灰色箭头）并不总是与轨迹（橙色线）一致。通常，动作也会影响获得奖励的概率，但在这个例子中，奖励与动作无关，所以 $\text{Pr}(r_{t+1}|s_t, a_t) = \text{Pr}(r_{t+1}|s_t)$。来自一个MDP的轨迹 $\tau$ 由交替的状态 $s_t$、动作 $a_t$ 和奖励 $r_{t+1}$ 组成的序列 $s_1, a_1, r_2, s_2, a_2, r_3, s_3, a_3, r_4, \dots$ 构成。注意，这里企鹅在离开有鱼的状态时获得奖励（即，奖励是因穿过鱼方格而获得的，无论企鹅是有意还是无意到达那里的）。

---

### 19.1.4 部分可观测马尔可夫决策过程

在**部分可观测马尔可夫决策过程 (partially observable Markov decision process)** 或 **POMDP** 中，状态不是直接可见的（图19.4）。相反，智能体接收一个从 $\text{Pr}(o_t|s_t)$ 中抽取的观测 $o_t$。因此，一个POMDP生成一个状态、观测、动作和奖励的序列 $s_1, o_1, a_1, r_2, s_2, o_2, a_2, r_3, s_3, o_3, a_3, r_4, \dots$。通常，每个观测会与某些状态比其他状态更兼容，但不足以唯一地确定状态。

### 19.1.5 策略

决定智能体在每个状态下动作的规则被称为**策略 (policy)**（图19.5）。这可能是**随机的 (stochastic)**（策略为每个状态定义一个动作的分布）或**确定性的 (deterministic)**（智能体在给定状态下总是采取相同的动作）。一个随机策略 $\pi[a|s]$ 为状态 $s$ 的每个可能动作 $a$ 返回一个概率分布，从中抽样一个新的动作。一个确定性策略 $\pi[a|s]$ 为在状态 $s$ 下选择的动作 $a$ 返回1，否则返回0。一个**平稳策略 (stationary policy)** 仅依赖于当前状态。一个**非平稳策略 (non-stationary policy)** 还依赖于时间步。

环境和智能体形成一个循环（图19.6）。智能体从上一个时间步接收状态 $s_t$ 和奖励 $r_t$。基于此，如果需要，它可以修改策略 $\pi[a_t|s_t]$ 并选择下一个动作 $a_t$。然后环境根据 $\text{Pr}(s_{t+1}|s_t, a_t)$ 前进到下一个状态，并根据 $\text{Pr}(r_{t+1}|s_t, a_t)$ 发出奖励。

---

> **图 19.4 部分可观测马尔可夫决策过程 (POMDP)**
> 在POMDP中，智能体无法接触到整个状态。在这里，企鹅不知道当前的状态，只能看到附近的格子（虚线框）。不幸的是，真实状态（三）与它在状态九时所看到的无法区分。在第一种情况下，向右移动会导致掉进冰窟（-2奖励），而在后一种情况下，则会找到鱼（+3奖励）。

> **图 19.5 策略**
> a) 一个确定性策略在每个状态下总是选择相同的动作（由箭头表示）。一些策略比其他策略好。这个策略不是最优的，但仍然通常将企鹅从左上引导到右下，那里有奖励。b) 这个策略更随机。c) 一个随机策略对每个状态都有一个动作的概率分布（概率由箭头的大小表示）。它的优点是智能体能更彻底地探索状态，并且对于在部分可观测马尔可夫决策过程中获得最优性能可能是必要的。

> **图 19.6 强化学习循环**
> 智能体在时间 $t$ 根据状态 $s_t$，按照策略 $\pi[a_t|s_t]$ 采取一个动作 $a_t$。这触发了新状态 $s_{t+1}$（通过状态转移函数）和奖励 $r_{t+1}$（通过奖励函数）的生成。两者都被传递回智能体，然后它选择一个新的动作。

---

## 19.2 期望回报

上一节介绍了马尔可夫决策过程和智能体根据策略执行动作的想法。我们希望选择一个能最大化期望回报的策略。在本节中，我们将这个想法数学化。为此，我们为每个状态 $s_t$ 和状态-动作对 $\{s_t, a_t\}$ 分配一个**价值 (value)**。

### 19.2.1 状态和动作价值

回报 $G_t$ 依赖于状态 $s_t$ 和策略 $\pi[a|s]$。从这个状态开始，智能体将经过一系列的状态，采取动作并接收奖励。这个序列每次智能体从同一个地方开始时都会不同，因为通常，策略 $\pi[a_t|s_t]$、状态转移 $\text{Pr}(s_{t+1}|s_t, a_t)$ 和发出的奖励 $\text{Pr}(r_{t+1}|s_t, a_t)$ 都是随机的。

我们可以通过考虑**期望回报** $v[s_t|\pi]$ 来描述一个状态在给定策略 $\pi$ 下有多“好”。这是从这个状态开始的序列平均会收到的回报，被称为**状态价值 (state value)** 或**状态价值函数 (state-value function)**（图19.7a）：

$$
v[s_t|\pi] = \mathbb{E}[G_t|s_t, \pi].
\tag{19.2}
$$

通俗地说，状态价值告诉我们，如果我们从这个状态开始并此后遵循指定的策略，我们平均可以期望的长期奖励。对于那些很可能后续转移会很快带来大奖励的状态，它的值最高（假设折扣因子 $\gamma$ 小于1）。

类似地，**动作价值 (action value)** 或**状态-动作价值函数 (state-action value function)** $q[s_t, a_t|\pi]$ 是在状态 $s_t$ 执行动作 $a_t$ 的期望回报（图19.7b）：

$$
q[s_t, a_t|\pi] = \mathbb{E}[G_t|s_t, a_t, \pi].
\tag{19.3}
$$

动作价值告诉我们，如果我们从这个状态开始，采取这个动作，并此后遵循指定的策略，我们平均可以期望的长期奖励。通过这个量，强化学习算法将未来的奖励与当前的动作联系起来（即，解决时间信用分配问题）。

---

> **图 19.7 状态和动作价值**
> a) 一个状态 $s_t$ 的价值 $v[s_t|\pi]$（每个位置的数字）是在给定策略 $\pi$（灰色箭头）下，该状态的期望回报。它是从这个状态开始的许多轨迹上收到的折扣奖励的平均总和。在这里，离鱼更近的状态更有价值。b) 在状态 $s_t$ 的一个动作 $a_t$ 的价值 $q[s_t, a_t, \pi]$（每个位置/状态对应四个动作的四个数字）是给定在该状态下采取该特定动作的期望回报。在这种情况下，当我们更接近鱼时，它会变大，并且对于朝向鱼的动作也更大。c) 如果我们知道一个状态的动作价值，那么策略可以被修改，以便它选择这些价值中的最大值（面板b中的红色数字）。

---

### 19.2.2 最优策略

我们想要一个能最大化期望回报的策略。对于MDPs（但不是POMDPs），总存在一个确定性的、平稳的策略，它能最大化每个状态的价值。如果我们知道这个**最优策略**，那么我们就能得到**最优状态价值函数** $v^*[s_t]$：

$$
v^*[s_t] = \max_\pi [\mathbb{E}[G_t|s_t, \pi]].
\tag{19.4}
$$

类似地，在最优策略下，我们能得到**最优状态-动作价值函数**：

$$
q^*[s_t, a_t] = \max_\pi [\mathbb{E}[G_t|s_t, a_t, \pi]].
\tag{19.5}
$$

反过来看，如果我们知道最优动作价值 $q^*[s_t, a_t]$，那么我们可以通过选择具有最高价值的动作 $a_t$ 来推导出最优策略（图19.7c）：¹

$$
\pi[a_t|s_t] \leftarrow \underset{a_t}{\mathrm{argmax}}[q^*[s_t, a_t]].
\tag{19.6}
$$

实际上，一些强化学习算法是基于交替估计动作价值和策略的（见19.3节）。

> ¹符号 $\pi[a_t|s_t] \leftarrow a$ 在方程19.6, 19.12和19.13中意味着将 $\pi[a|s_t]$ 设为对动作 $a$ 为1，对其他动作为0。
> ²为简单起见，我们从现在开始只写 $v[s_t]$ 和 $q[s_t, a_t]$ 而不是 $v[s_t|\pi]$ 和 $q[s_t, a_t|\pi]$。
> ³我们也从现在开始假设奖励是确定性的，可以写成 $r[s_t, a_t]$。

### 19.2.3 贝尔曼方程

对于任何策略，我们可能不知道状态价值 $v[s_t]$ 或动作价值 $q[s_t, a_t]$。² 然而，我们知道它们必须相互一致，并且很容易写出这些量之间的关系。状态价值 $v[s_t]$ 可以通过取动作价值 $q[s_t, a_t]$ 的加权和来找到，其中权重取决于在该策略 $\pi[a_t|s_t]$ 下采取该动作的概率（图19.8）：

$$
v[s_t] = \sum_{a_t} \pi[a_t|s_t]q[s_t, a_t].
\tag{19.7}
$$

类似地，一个动作的价值是采取该动作产生的即时奖励 $r_{t+1} = r[s_t, a_t]$，加上处于后续状态 $s_{t+1}$ 的价值 $v[s_{t+1}]$ 并由 $\gamma$ 折扣（图19.9）。³ 由于 $s_{t+1}$ 的分配不是确定性的，我们根据转移概率 $\text{Pr}(s_{t+1}|s_t, a_t)$ 对价值 $v[s_{t+1}]$ 进行加权：

$$
q[s_t, a_t] = r[s_t, a_t] + \gamma \cdot \sum_{s_{t+1}} \text{Pr}(s_{t+1}|s_t, a_t)v[s_{t+1}].
\tag{19.8}
$$

将方程19.8代入方程19.7，提供了一个在时间 $t$ 和 $t+1$ 的状态价值之间的关系：

$$
v[s_t] = \sum_{a_t} \pi[a_t|s_t]\left(r[s_t, a_t] + \gamma \cdot \sum_{s_{t+1}} \text{Pr}(s_{t+1}|s_t, a_t)v[s_{t+1}]\right).
\tag{19.9}
$$

类似地，将方程19.7代入方程19.8，提供了一个在时间 $t$ 和 $t+1$ 的动作价值之间的关系：

$$
q[s_t, a_t] = r[s_t, a_t] + \gamma \cdot \sum_{s_{t+1}} \text{Pr}(s_{t+1}|s_t, a_t)\left(\sum_{a_{t+1}} \pi[a_{t+1}|s_{t+1}]q[s_{t+1}, a_{t+1}]\right).
\tag{19.10}
$$

后两个关系是**贝尔曼方程 (Bellman equations)**，是许多RL方法的支柱。简而言之，它们表明状态（动作）价值必须是自洽的。因此，当我们更新一个状态（动作）价值的估计时，这将产生一个涟漪效应，导致对所有其他状态（动作）价值的修改。

---

> **图 19.8 状态价值和动作价值之间的关系**
> 状态六的价值 $v[s_t=6]$ 是状态六处动作价值 $q[s_t=6, a_t]$ 的加权和，其中权重是采取该动作的策略概率 $\pi[a_t|s_t=6]$。

> **图 19.9 动作价值和状态价值之间的关系**
> 在状态六采取动作二的价值 $q[s_t=6, a_t=2]$ 是采取该动作的奖励 $r[s_t=6, a_t=2]$ 加上处于后继状态的折扣价值 $v[s_{t+1}]$ 的加权和，其中权重是转移概率 $\text{Pr}(s_{t+1}|s_t=6, a_t=2)$。贝尔曼方程将这个关系与图19.8的关系链式连接起来，以关联当前和下一个 (i) 状态价值和 (ii) 动作价值。

---

## 19.3 表格型强化学习

**表格型RL算法 (Tabular RL algorithms)**（即，不依赖于函数近似的算法）分为**基于模型的 (model-based)** 和**无模型的 (model-free)** 方法。基于模型的方法⁴ 明确使用MDP结构，并从转移矩阵 $\text{Pr}(s_{t+1}|s_t, a_t)$ 和奖励结构 $r[s, a]$ 中找到最佳策略。如果这些是已知的，这是一个可以直接用**动态规划 (dynamic programming)** 解决的优化问题。如果它们是未知的，它们可以（原则上）从观测到的MDP轨迹中估计。⁵

相反，无模型的方法假设底层MDP的转移矩阵和奖励结构是未知的。这些方法分为两个家族：
1.  **价值估计 (Value estimation)** 方法估计最优状态-动作价值函数，然后根据每个状态中价值最大的动作来分配策略。
2.  **策略估计 (Policy estimation)** 方法使用梯度下降技术直接估计最优策略，而无需估算模型或价值的中间步骤。

在每个家族中，**蒙特卡洛 (Monte Carlo)** 方法为给定的策略模拟许多通过MDP的轨迹，以收集有关如何改进此策略的信息。有时，在更新策略之前模拟许多轨迹是不可行或不切实际的。**时间差分 (Temporal difference, TD)** 方法在智能体遍历MDP时更新策略。
我们现在简要描述动态规划方法、蒙特卡洛价值估计方法和TD价值估计方法。19.4节描述了深度网络如何被用于TD价值估计方法。我们将在19.5节回到策略估计。

> ⁴术语**模型 (model)** 在这里指的是MDP，而不是机器学习模型。
> ⁵在RL中，**轨迹 (trajectory)** 是观测到的状态、奖励和动作的序列。**rollout** 是一个模拟的轨迹。**回合 (episode)** 是一个从初始状态开始并在终止状态结束的轨迹（例如，一盘完整的国际象棋，从标准开局开始，以赢、输或平局结束）。

## 19.4 拟合Q学习

上面描述的表格蒙特卡洛和TD算法反复遍历整个MDP并更新动作价值。然而，这仅在状态-动作空间很小的情况下才实用。不幸的是，这种情况很少见；即使对于一个受限的棋盘环境，也存在超过 $10^{40}$ 种可能的合法状态。

在**拟合Q学习 (fitted Q-learning)** 中，动作价值的离散表示 $q[s_t, a_t]$ 被一个机器学习模型 $q[s_t, a_t, \boldsymbol{\phi}]$ 所取代，其中现在状态由一个向量 $\mathbf{s}_t$ 而不仅仅是一个索引表示。然后我们基于相邻动作价值的一致性定义一个最小二乘损失（类似于Q学习中的损失，见方程19.15）：

$$
L[\boldsymbol{\phi}] = \left(r[s_t, a_t] + \gamma \cdot \max_a[q[s_{t+1}, a, \boldsymbol{\phi}]] - q[s_t, a_t, \boldsymbol{\phi}]\right)^2,
\tag{19.16}
$$

这反过来导致了更新：

$$
\boldsymbol{\phi} \leftarrow \boldsymbol{\phi} + \alpha\left(r[s_t, a_t] + \gamma \cdot \max_a[q[s_{t+1}, a, \boldsymbol{\phi}]] - q[s_t, a_t, \boldsymbol{\phi}]\right)\frac{\partial q[s_t, a_t, \boldsymbol{\phi}]}{\partial\boldsymbol{\phi}}.
\tag{19.17}
$$

拟合Q学习与Q学习的不同之处在于，不再保证收敛。对参数的改变可能会潜在地修改目标 $r[s_t, a_t] + \gamma \cdot \max_{a_{t+1}}[q[s_{t+1}, a_{t+1}, \boldsymbol{\phi}]]$（最大值可能会改变）和预测 $q[s_t, a_t, \boldsymbol{\phi}]$。这在理论上和经验上都被证明会损害收敛。

### 19.4.1 用于玩ATARI游戏的深度Q网络

深度网络非常适合从高维状态空间进行预测，因此它们是拟合Q学习中模型的自然选择。原则上，它们可以同时接收状态和动作作为输入并预测价值，但在实践中，网络只接收状态并同时预测每个动作的价值。

**深度Q网络 (Deep Q-Network)** 是一个突破性的强化学习架构，它利用深度网络来学习玩ATARI 2600游戏。观测数据包括每个像素有128种可能颜色的 $220 \times 160$ 图像（图19.13）。这被重塑为 $84 \times 84$ 的大小，并且只保留了亮度值。不幸的是，从单个帧中无法完全观察到完整状态。例如，游戏对象的速度是未知的。为了帮助解决这个问题，网络在每个时间步接收最后四帧以构成 $\mathbf{s}_t$。它通过三个卷积层，然后是一个全连接层来映射这些帧，以预测每个动作的价值（图19.14）。

对标准训练过程进行了几项修改。首先，奖励（由游戏中的分数驱动）对于负变化被裁剪为-1，对于正变化被裁剪为+1。这补偿了不同游戏之间分数的巨大差异，并允许使用相同的学习率。其次，该系统利用了**经验回放 (experience replay)**。不是基于当前步骤的元组 $<s_t, a_t, r_{t+1}, s_{t+1}>$ 或最后 $I$ 个元组的批次来更新网络，而是将所有最近的元组存储在一个缓冲区中。这个缓冲区被随机采样以在每个步骤生成一个批次。这种方法多次重用数据样本，并减少了由于相邻帧的相似性而产生的批次中样本之间的相关性。

最后，通过将目标参数固定为值 $\bar{\boldsymbol{\phi}}$ 并仅周期性地更新它们，解决了拟合Q网络中的收敛问题。这给出了更新：

$$
\boldsymbol{\phi} \leftarrow \boldsymbol{\phi} + \alpha\left(r[s_t, a_t] + \gamma \cdot \max_a[q[s_{t+1}, a, \bar{\boldsymbol{\phi}}]] - q[s_t, a_t, \boldsymbol{\phi}]\right)\frac{\partial q[s_t, a_t, \boldsymbol{\phi}]}{\partial\boldsymbol{\phi}}.
\tag{19.18}
$$

现在网络不再追逐一个移动的目标，并且更不容易振荡。

使用这些和其他启发式方法，并采用 $\epsilon$-贪婪策略，深度Q网络在一组49个游戏中（每个游戏都使用相同的网络架构，但分开训练）达到了与专业游戏测试员相当的水平。应该注意的是，训练过程是数据密集型的。学习每个游戏需要大约38个整天的经验。在一些游戏中，算法超过了人类的表现。在其他像“蒙特祖玛的复仇”这样的游戏中，它几乎没有取得任何进展。这个游戏以稀疏的奖励和外观截然不同的多个屏幕为特色。

---

> **图 19.14 深度Q网络架构**
> 输入 $\mathbf{s}_t$ 由ATARI游戏的四个相邻帧组成。每个帧被调整为 $84 \times 84$ 并转换为灰度。这些帧表示为四个通道，并通过一个步长为四的 $8 \times 8$ 卷积，然后是一个步长为2的 $4 \times 4$ 卷积，再由两个全连接层处理。最终的输出预测了这个状态下18个动作中每个动作的动作价值 $q[s_t, a_t]$。

---

## 19.5 策略梯度方法

Q学习首先估计动作价值，然后用这些来更新策略。相反，**基于策略 (policy-based)** 的方法直接学习一个随机策略 $\pi[a_t|s_t, \boldsymbol{\theta}]$。这是一个带有可训练参数 $\boldsymbol{\theta}$ 的函数，它将一个状态 $s_t$ 映射到一个我们可以从中采样的动作 $a_t$ 的分布 $\text{Pr}(a_t|s_t)$。在MDPs中，总存在一个最优的确定性策略。然而，使用随机策略有三个原因：

1.  一个随机策略自然有助于探索空间；我们不必在每个时间步都采取最好的动作。
2.  当我们修改一个随机策略时，损失会平滑地变化。这意味着即使奖励是离散的，我们也可以使用梯度下降方法。这类似于在（离散的）分类问题中使用最大似然。当模型参数改变以使真实类别更有可能时，损失会平滑地变化。
3.  MDP的假设经常是不正确的；我们通常没有关于状态的完整知识。例如，考虑一个智能体在一个它只能观察到附近位置的环境中导航（例如，图19.4）。如果两个位置看起来相同，但附近的奖励结构不同，一个随机策略允许采取不同动作的可能性，直到这个模糊性被解决。

---

> **图 19.15 策略梯度**
> 同一策略的五个回合（更亮的表示更高的奖励）。轨迹1、2和3持续产生高回报，但类似的轨迹已经频繁地通过这个策略出现，所以没有必要改变。相反，轨迹4收到低回报，所以应该修改策略以避免产生类似的轨迹。轨迹5收到高回报并且是不寻常的。这将根据方程19.25导致对策略的最大改变。

---

## 19.8 总结

强化学习是用于马尔可夫决策过程和类似系统的序列决策框架。本章回顾了RL的表格方法，包括动态规划（其中环境模型是已知的）、蒙特卡洛方法（其中运行多个回合，然后根据收到的奖励来改变动作价值和策略），以及时间差分方法（其中这些价值在回合进行中被更新）。

深度Q学习是一种时间差分方法，其中使用深度神经网络来预测每个状态的动作价值。它可以训练智能体在与人类相当的水平上玩ATARI 2600游戏。策略梯度方法直接优化策略而不是为动作分配价值。它们产生随机策略，这在环境是部分可观测时很重要。更新是嘈杂的，已经引入了许多改进来减少它们的方差。

离线强化学习在我们无法与环境交互但必须从历史数据中学习时使用。决策 Transformer 利用了深度学习的最新进展来构建状态-动作-奖励序列的模型，并预测将最大化奖励的动作。