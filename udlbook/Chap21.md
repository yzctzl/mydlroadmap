好的，请看我的翻译和解答。

***

# 第二十一章
# 深度学习与伦理

*本章由 Travis LaCroix 和 Simon J.D. Prince 共同撰写。*

人工智能（AI）正处于改变社会的风口浪尖，其影响有好有坏。这些技术在造福社会方面拥有巨大潜力（Taddeo & Floridi, 2018; Tomašev et al., 2020），包括在医疗保健领域发挥重要作用（Rajpurkar et al., 2022）以及助力应对气候变化（Rolnick et al., 2023）。然而，它们也可能被滥用或造成意想不到的伤害。这催生了人工智能伦理这一研究领域的出现。

深度学习的现代纪元始于2012年的AlexNet，但对人工智能伦理的持续关注并未随之立即到来。实际上，一个关于机器学习公平性的研讨会因材料不足而被NeurIPS 2013拒绝。直到2016年，人工智能伦理才迎来了它的“AlexNet时刻”，标志性事件是ProPublica对COMPAS累犯预测模型中偏见的揭露（Angwin et al., 2016）以及Cathy O'Neil的著作《数学杀伤性武器》（O'Neil, 2016）。自那以后，人们的兴趣急剧增长；自2018年成立以来，公平、问责与透明度会议（FAccT）的投稿量在五年内增长了近十倍。

与此同时，许多组织提出了负责任AI的政策建议。Jobin等人（2019）发现，在84份包含AI伦理原则的文件中，有88%是在2016年之后发布的。这些非立法性质的政策协议的激增，依赖于自愿、非约束性的合作，其效力备受质疑（McNamara et al., 2018; Hagendorff, 2020; LaCroix & Mohseni, 2022）。简而言之，人工智能伦理尚处于起步阶段，伦理考量往往是被动应对而非主动预防。

本章探讨了因AI系统的设计和使用而产生的潜在危害，包括算法偏见、缺乏可解释性、侵犯数据隐私、军事化、欺诈和环境问题。本章的目的并非提供如何变得更道德的建议，而是旨在阐述思想，并在哲学、政治科学和更广泛的社会科学领域中已受到关注的关键领域开启对话。

## 21.1 价值对齐

在设计AI系统时，我们希望确保它们的“价值观”（目标）与人类的价值观相一致。这有时被称为**价值对齐问题 (value alignment problem)**（Russell, 2019; Christian, 2020; Gabriel, 2020）。这个问题之所以具有挑战性，有三个原因。首先，完整且正确地定义我们的价值观非常困难。其次，将这些价值观编码为AI模型的目标很难。第三，确保模型学会实现这些目标也很难。参考：Problem 21.1

在机器学习模型中，损失函数是我们真实目标的**代理 (proxy)**，两者之间的不一致被称为**外部对齐问题 (outer alignment problem)**（Hubinger et al., 2019）。如果这个代理不够充分，系统中就会存在“漏洞”，系统可以利用这些漏洞来最小化其损失函数，却未能满足预期的目标。例如，考虑训练一个强化学习智能体下棋。如果智能体仅仅因为吃子而获得奖励，这可能会导致许多平局，而不是我们期望的行为（赢得比赛）。相比之下，**内部对齐问题 (inner alignment problem)** 是要确保即使在损失函数被明确指定的情况下，AI系统的行为也不会偏离预期目标。如果学习算法未能找到全局最小值，或者训练数据不具代表性，训练可能会收敛到一个与真实目标不一致的解，从而导致不良行为（Goldberg, 1987; Mitchell et al., 1992; Lehman & Stanley, 2008）。参考：Problem 21.2

Gabriel (2020) 将价值对齐问题分为**技术 (technical)** 和**规范 (normative)** 两个组成部分。技术部分关注我们如何将价值观编码到模型中，以便它们可靠地执行我们期望的任务。一些具体问题，如避免**奖励投机 (reward hacking)** 和**安全探索 (safe exploration)**，可能纯粹是技术性的解决方案（Amodei et al., 2016）。相比之下，规范部分关注的是，首先，**什么是正确的价值观**。鉴于不同文化和社会所珍视的事物范围广泛，这个问题可能没有单一的答案。重要的是，编码的价值观必须代表每一个人，而不仅仅是文化上占主导地位的社会子集。

另一种思考价值对齐的方式是，当人类**委托人 (principal)** 将任务委派给人工智能**代理人 (agent)** 时，会出现一个结构性问题（LaCroix, 2022）。这类似于经济学中的**委托-代理问题 (principal-agent problem)**（Laffont & Martimort, 2002），该问题承认在任何一方期望为另一方最佳利益行事的关系中，都存在着固有的利益冲突。在AI的背景下，当 (i) 目标被错误指定或 (ii) 委托人与代理人之间存在**信息不对称 (informational asymmetry)** 时，这种利益冲突就会出现（图21.1）。

AI伦理中的许多主题都可以从这种价值对齐的结构性视角来理解。以下各节将讨论偏见与公平问题、人工智能道德代理问题（两者都与指定目标有关）以及透明度与可解释性问题（两者都与信息不对称有关）。

---

> **图 21.1 价值对齐问题的结构性描述**
> 问题源于 a) 目标不一致（例如，偏见）或 b) （人类）委托人与（人工智能）代理人之间的信息不对称（例如，缺乏可解释性）。改编自 LaCroix (2025)。

---

### 21.1.1 偏见与公平

从纯科学的角度来看，**偏见 (bias)** 指的是与某个规范的统计偏差。在AI中，当这种偏差依赖于影响输出的**不正当因素 (illegitimate factors)** 时，它可能是有害的。例如，性别与工作表现无关，因此以性别为由聘用候选人是不正当的。同样，种族与犯罪无关，因此在累犯预测中使用种族作为特征也是不正当的。

AI模型中的偏见可以通过多种方式引入（Fazelpour & Danks, 2021）：

*   **问题规约 (Problem specification)**：选择模型的目标需要对我们认为重要的事物进行价值判断，这为偏见的产生创造了条件（Fazelpour & Danks, 2021）。如果我们未能成功地将这些选择操作化，并且问题规约未能捕捉到我们的预期目标，那么可能会出现进一步的偏见（Mitchell et al., 2021）。
*   **数据 (Data)**：当数据集不具代表性或不完整时，可能会导致算法偏见（Danks & London, 2017）。例如，PULSE面部超分辨率算法（Menon et al., 2020）是在一个主要由白人名人照片组成的数据库上训练的。当它被应用于巴拉克·奥巴马的一张低分辨率肖像时，它生成了一张白人男性的照片（Vincent, 2020）。
    如果生成训练数据的社会在结构上对边缘化社区存在偏见，那么即使是完整且具代表性的数据集也会引发偏见（Mayson, 2018）。例如，在美国，黑人比白人更频繁地受到警察的监视和监禁。因此，用于训练累犯预测模型的历史数据已经对黑人社区存在偏见。
*   **建模与验证 (Modeling and validation)**：选择一个数学定义来衡量模型的公平性需要进行价值判断。存在着截然不同但同样直观的定义，它们在逻辑上是不一致的（Kleinberg et al., 2017; Chouldechova, 2017; Berk et al., 2021）。这表明需要从纯粹的数学公平概念，转向对算法是否在实践中促进正义的更实质性的评估（Green, 2022）。
*   **部署 (Deployment)**：部署的算法可能会与社会中的其他算法、结构或机构相互作用，形成复杂的反馈循环，从而固化现有的偏见（O'Neil, 2016）。例如，像GPT-3（Brown et al., 2020）这样的大型语言模型是在网络数据上训练的。然而，当GPT-3的输出被发布到网上时，未来模型的训练数据就会被降质。这可能会加剧偏见，并产生新的社会危害（Falbo & LaCroix, 2022）。参考：Problem 21.3

不公平性可以因**交叉性 (intersectionality)** 的考量而加剧；社会类别可以结合起来，形成重叠和相互依赖的压迫体系。例如，一个有色人种酷儿女性所经历的歧视，并不仅仅是她作为酷儿、作为女性或作为有色人种所可能经历的歧视的总和（Crenshaw, 1991）。在AI领域，Buolamwini & Gebru (2018)的研究表明，主要在肤色较浅的人脸上训练的面部分析算法，在肤色较深的人脸上表现不佳。然而，当肤色和性别等特征结合时，它们的表现比独立考虑这些特征时预期的还要差。

当然，可以采取措施确保数据是多样的、有代表性的和完整的。但如果生成训练数据的社会在结构上对边缘化社区存在偏见，那么即使是完全准确的数据集也会引发偏见。鉴于上述算法偏见的潜力和训练数据集中代表性的缺乏，还有必要考虑这些系统的输出失败率，可能会如何加剧对本已边缘化的社区的歧视（Buolamwini & Gebru, 2018; Raji & Buolamwini, 2019; Raji et al., 2022）。由此产生的模型可能会固化和加强权力和压迫的体系，包括资本主义和阶级主义；性别歧视、厌女症和父权制；殖民主义和帝国主义；种族主义和白人至上主义；残疾歧视；以及顺性别和异性恋霸权。一个对偏见保持对权力动态敏感的视角，要求我们考虑数据中编码的历史不平等和劳动条件（Micelli et al., 2022）。

为了防止这种情况，我们必须积极确保我们的算法是公平的。一种天真的方法是**通过无知实现公平 (fairness through unawareness)**，即简单地从输入特征中移除受保护的属性（例如，种族、性别）。不幸的是，这是无效的；其余的特征仍然可以携带关于受保护属性的信息。更实际的方法首先为公平性定义一个数学标准。例如，二元分类中的**分离度量 (separation measure)** 要求预测 $\hat{y}$ 在给定真实标签 $y$ 的情况下，与受保护变量 $a$（例如，种族）条件独立。然后，他们以各种方式进行干预，以最小化与该度量的偏差（图21.2）。

一个更复杂的因素是，除非我们能确定社群成员身份，否则我们无法判断一个算法是否对某个社群不公，也无法采取措施避免这种情况。大多数关于算法偏见和公平性的研究都集中在表面上可观察的特征上，这些特征可能存在于训练数据中（例如，性别）。然而，边缘化社群的特征可能是**不可观察的 (unobservable)**，这使得偏见缓解变得更加困难。例子包括酷儿身份（Tomasev et al., 2021）、残疾状况、神经类型、阶级和宗教。当可观察的特征已从训练数据中删除以防止模型利用它们时，也会出现类似的问题。

---

> **图 21.2 偏见缓解**
> 已经提出了在训练流程的各个阶段（从数据收集到已训练模型的后处理）补偿偏见的方法。参见 Barocas et al. (2023) 和 Mehrabi et al. (2022)。

---

### 21.1.2 人工道德主体

许多决策空间不包含具有道德分量的行动。例如，选择下一步棋的走法没有明显的道德后果。然而，在其他地方，行动可能具有道德分量。例子包括自动驾驶汽车中的决策（Awad et al., 2018; Evans et al., 2020）、致命性自主武器系统（Arkin, 2008a,b），以及用于儿童保育、老年护理和医疗保健的专业服务机器人（Anderson & Anderson, 2008; Sharkey & Sharkey, 2012）。随着这些系统变得越来越自主，它们可能需要独立于人类输入做出道德决策。

这引出了**人工道德主体 (artificial moral agency)** 的概念。一个人工道德主体是一个能够做出道德判断的自主AI系统。道德主体可以根据复杂性的增加进行分类（Moor, 2006）：
1.  **伦理影响主体 (Ethical impact agents)** 是指那些对情况产生正面或负面影响，但并非出于伦理考虑而设计的主体。因此，几乎任何部署在社会中的技术都可以算作伦理影响主体。
2.  **内隐伦理主体 (Implicit ethical agents)** 是指包含一些内置安全特性的伦理影响主体。
3.  **外显伦理主体 (Explicit ethical agents)** 能够根据情境遵循一般的道德原则或伦理行为准则。
4.  **完全伦理主体 (Full ethical agents)** 是指具有信念、欲望、意图、自由意志和对其行为有意识的主体。

**机器伦理 (machine ethics)** 领域寻求创建人工道德主体的方法。这些方法可以分为**自上而下 (top-down)**、**自下而上 (bottom-up)** 或**混合 (hybrid)**（Allen et al., 2005）。自上而下（理论驱动）的方法直接实现并按层级排列基于某种道德理论的具体规则，以指导伦理行为。阿西莫夫的“机器人三定律”是这种方法的一个简单例子。

在自下而上（学习驱动）的方法中，模型从数据中学习道德规律，而无需显式编程（Wallach et al., 2008）。例如，Noothigattu等人（2018）设计了一个基于投票的伦理决策系统，该系统使用从人类在道德困境中的偏好中收集的数据来学习社会偏好；然后该系统总结和聚合结果以做出“伦理”决策。混合方法结合了自上而下和自下而上的方法。

一些研究者对人工道德主体的概念本身提出了质疑，并认为道德主体对于确保安全并非必要（van Wynsberghe & Robbins, 2019）。关于人工道德主体的最新综述，请参见 Cervantes et al. (2019)；关于人工道德主体的技术方法的最新综述，请参见 Tolmeijer et al. (2020)。

### 21.1.3 透明度与不透明性

一个复杂的计算系统如果其所有操作细节都是已知的，那么它就是**透明的 (transparent)**。如果人类能够理解一个系统如何做出决策，那么它就是**可解释的 (explainable)**。在缺乏透明度或可解释性的情况下，用户和AI系统之间存在信息不对称，这使得确保价值对齐变得困难。参考：Problem 21.4

Creel (2020) 在几个粒度级别上描述了透明度。**功能透明度 (Functional transparency)** 指的是对系统算法功能的了解（即，将输入映射到输出的逻辑规则）。本书中的方法是在这个细节级别上描述的。**结构透明度 (Structural transparency)** 要求了解一个程序如何执行算法。当用高级编程语言编写的命令由机器代码执行时，这一点可能会变得模糊。最后，**运行透明度 (run transparency)** 要求了解一个程序在特定实例中是如何执行的。对于深度网络，这包括关于硬件、输入数据、训练数据及其相互作用的知识。这些都无法通过审查代码来确定。

例如，GPT-3是功能透明的；其架构在 Brown et al. (2020) 中有描述。然而，它不具备结构透明度，因为我们无法访问其代码；它也不具备运行透明度，因为我们无法访问其学习到的参数、硬件或训练数据。后续版本GPT-4则完全不透明。这个商业产品如何工作的细节是未知的。

### 21.1.4 可解释性与可理解性

即使一个系统是透明的，这并不意味着我们能理解一个决策是如何做出的，或者这个决策是基于什么信息的。深度网络可能包含数十亿个参数，所以我们无法仅通过检查来理解它们的工作原理。然而，在某些司法管辖区，公众可能有权获得解释。欧盟《通用数据保护条例》第22条建议，在完全基于自动化流程做出决策的情况下，所有数据主体都应有权“获得对所做决策的解释”。¹

这些困难导致了**可解释AI (explainable AI)** 这一子领域的产生。一个中等成功的领域是产生**局部解释 (local explanations)**。虽然我们无法解释整个系统，但我们有时可以描述一个特定的输入是如何被分类的。例如，**局部可解释模型无关的解释 (Local interpretable model-agnostic explanations)** 或 **LIME** (Ribeiro et al., 2016) 在附近的输入点上采样模型输出，并使用这些样本来构建一个更简单的模型（图21.3）。这为分类决策提供了洞察，即使原始模型既不透明也不可解释。参考：Notebook 21.2 Explainability

是否有可能构建既能被其用户甚至其创造者完全理解的复杂决策系统，仍有待观察。关于一个系统可解释、可理解或可阐释意味着什么的争论也仍在进行中（Erasmus et al., 2021）；目前对这些概念还没有具体的定义。更多信息请参见 Molnar (2022)。

> ¹第22条是否真的强制要求这样的权利是有争议的（见 Wachter et al., 2017）。

---

> **图 21.3 LIME**
> 深度网络的输出函数是复杂的；在高维空间中，很难知道一个决策为什么会做出，或者如何在不访问模型的情况下修改输入来改变它。a) 考虑试图理解为什么在白色十字处的 $\text{Pr}(y=1|\mathbf{x})$ 很低。LIME在附近的点上探测网络，看它是否将这些点识别为 $\text{Pr}(y=1|\mathbf{x})<0.5$（青色点）或 $\text{Pr}(y=1|\mathbf{x}) \ge 0.5$（灰色点）。它根据与兴趣点的接近程度对这些点进行加权（权重由圆圈大小表示）。b) 加权的点被用来训练一个更简单的模型（这里是逻辑回归——一个通过sigmoid传递的线性函数）。c) 在白色十字附近，这个近似值接近于 d) 原始函数。即使我们无法访问原始模型，我们也可以从这个近似模型的参数中推断出，如果我们增加 $x_1$ 或减少 $x_2$，$\text{Pr}(y=1|\mathbf{x})$ 将会增加，输出类别也会改变。改编自 Prince (2022)。

---

## 21.2 蓄意滥用

上一节中的问题源于指定不当的目标和信息不对称。然而，即使一个系统功能正常，它也可能导致不道德的行为或被蓄意滥用。本节重点介绍了由于滥用AI系统而引起的一些具体伦理问题。

### 21.2.1 面部识别与分析

面部识别技术具有特别高的滥用风险。专制国家可以用它来识别和压制抗议者，从而危及言论自由和抗议权的民主理想。Smith & Miller (2022) 认为，自由民主的价值观（例如，安全、隐私、自主和问责）与这些技术的潜在用例（例如，边境安全、刑事调查和警务、国家安全以及商业化）之间存在不匹配。因此，一些研究人员、活动家和政策制定者质疑这项技术是否应该存在（Barrett, 2020）。参考：Problem 21.5

此外，这些技术往往并不像它们声称的那样有效（Raji et al., 2022）。例如，纽约大都会运输署在一项概念验证试验报告了100%的失败率后，仍然推进并扩大了其面部识别的使用范围，该试验未能检测到在可接受参数范围内的面孔（Berger, 2019）。同样，面部分析工具经常夸大其能力（Raji & Fried, 2020），可疑地声称能够推断个人的性取向（Leuner, 2019）、情绪（Stark & Hoey, 2021）、可雇佣性（Fetscherin et al., 2020）或犯罪倾向（Wu & Zhang, 2016）。Stark & Hutson (2022) 强调，计算机视觉系统已经导致了“科学上毫无根据、种族主义和声名狼藉的伪科学领域”——如面相学和颅相学——的复兴。

### 21.2.2 军事化与政治干预

各国政府在以国家安全和国家建设的名义资助AI研究方面有既得利益。这有导致国家之间军备竞赛的风险，伴随着“高额的投资、缺乏透明度、相互猜疑和恐惧，以及被认为是意图先发制人”的部署（Sisson et al., 2020）。参考：Problem 21.6

**致命性自主武器系统**受到极大关注，因为它们很容易想象，而且确实有许多此类系统正在开发中（Heikkilä, 2022）。然而，AI也为网络攻击和虚假信息宣传活动（即，意图欺骗而分享的不准确或误导性信息）提供了便利。AI系统允许创建高度逼真的虚假内容，并促进信息的传播，通常是针对特定受众（Akers et al., 2018）和大规模传播（Bontridder & Poullet, 2021）。

Kosinski等人（2013）提出，仅通过社交媒体上的“点赞”就可以预测包括性取向、种族、宗教和政治观点、人格特质、智力、幸福感、成瘾物质使用、父母离异、年龄和性别在内的敏感变量。从这些信息中，像“开放性”这样的人格特质可以被用于操纵目的（例如，改变投票行为）。

### 21.2.3 欺诈

不幸的是，AI是自动化欺诈活动的有用工具（例如，发送大量电子邮件或短信，诱骗人们透露敏感信息或汇款）。生成式AI可以被用来欺骗人们，让他们以为自己正在与一个合法的实体互动，或者生成误导或欺骗人们的假文件。此外，AI可以增加网络攻击的复杂性，例如通过生成更具说服力的钓鱼邮件或适应目标组织的防御。参考：Problem 21.7

这凸显了要求机器学习系统透明化的弊端：这些系统越开放和透明，它们就越容易受到安全风险或被恶意行为者利用。例如，像ChatGPT这样的生成式语言模型已被用来编写可用于间谍活动、勒索软件和其他恶意软件的软件和电子邮件（Goodin, 2023）。

将计算机行为**拟人化**，特别是将意义投射到符号串上的倾向，被称为**ELIZA效应**（Hofstadter, 1995）。这导致了在与复杂的聊天机器人互动时产生一种虚假的安全感，使人们更容易受到基于文本的欺诈，如恋爱诈骗或商业邮件入侵计划的影响（Abrahams, 2023）。Véliz (2023) 强调了一些聊天机器人中表情符号的使用是如何具有内在的操纵性，利用了对情感图像的本能反应。

## 21.3 其他社会、伦理和专业问题

前一节确定了AI可能被蓄意滥用的领域。本节描述了AI广泛采用的其他潜在副作用。

### 21.3.1 知识产权

**知识产权 (Intellectual property, IP)** 可以被描述为非物质财产，是原创思想的产物（Moore & Himma, 2022）。实际上，许多AI模型是在受版权保护的材料上训练的。因此，这些模型的部署可能会带来法律和伦理风险，并与知识产权相冲突（Henderson et al., 2023）。参考：Problem 21.9

有时，这些问题是明确的。当语言模型被提示输入受版权保护材料的摘录时，它们的输出可能逐字逐句地包含受版权保护的文本，在扩散模型中图像生成的情境下也存在类似的问题（Henderson et al., 2023; Carlini et al., 2022, 2023）。即使训练属于“合理使用”，在某些情况下，这也可能侵犯内容创作者的道德权利（Weidinger et al., 2022）。更微妙的是，生成模型（第12,14-18章）提出了关于AI和知识产权的新问题。机器学习模型的输出（例如，艺术、音乐、代码、文本）能否获得版权或专利？在特定艺术家的作品上微调一个模型以重现该艺术家的风格，在道德上或法律上是否可以接受？知识产权法是一个突显了现有立法在创建时并未考虑到机器学习模型的领域。尽管政府和法院可能在不久的将来会设定先例，但这些问题在撰写本文时仍然是开放的。

## 21.8 总结

本章探讨了深度学习和人工智能的伦理影响。**价值对齐问题**是确保AI系统的目标与人类目标一致的任务。偏见、可解释性、人工道德主体和其他主题都可以通过这个视角来看待。AI可以被蓄意滥用，本章详细介绍了一些可能发生的方式。AI的进展在知识产权法和气候变化等不同领域产生了进一步的影响。

伦理AI是一个**集体行动问题 (collective action problem)**，本章最后呼吁科学家们考虑他们工作的道德和伦理影响。每个伦理问题并非都在每个计算机科学家的控制范围之内。然而，这并不意味着研究人员完全没有责任去考虑——并在他们力所能及的范围内减轻——他们所创建的系统可能被滥用的潜力。

***

### 习题

**问题 21.1** 有人提出，AI价值对齐问题最常见的规约是“确保AI系统的价值观与人类价值观对齐的问题”。讨论这个问题陈述在哪些方面是不明确的。

**思路与解答：**

这个陈述在多个层面存在模糊性：
1.  **“人类价值观”是什么？**：不存在一个单一、普适的“人类价值观”。不同文化、社会、社群甚至个体之间，价值观都存在巨大差异甚至冲突。这个陈述没有指明应该对齐**哪些人**的价值观。是对齐平均值？还是某个主导群体的价值观？
2.  **如何定义和量化价值观？**：价值观（如公平、正义、幸福）是高度抽象和复杂的概念，很难被精确地定义和量化为机器可以理解的目标函数。
3.  **对齐的程度如何衡量？**：“对齐”是一个模糊的词。是要求完全一致，还是允许一定程度的偏差？如何衡量对齐的程度？
4.  **价值观的动态性**：人类的价值观是随时间和情境变化的。一个静态的“对齐”目标可能很快就会过时或在新的情境下变得不适用。

**问题 21.2** 古德哈特定律指出“当一个度量变成一个目标，它就不再是一个好的度量”。考虑到损失函数只是我们真实目标的一个代理，思考如何将这一定律重新表述以适用于人工智能的价值对齐。

**思路与解答：**

**重新表述**：
“当一个**损失函数**被设定为AI系统的优化**目标**时，它本身就不再是衡量系统是否实现**人类真实意图**的一个好的**度量**。”

**解释**：
损失函数只是我们复杂、多维、有时甚至是模糊的真实目标（如“写一首好诗”或“安全驾驶”）的一个**简化的、可量化的代理**。当AI系统**过度优化 (over-optimizing)** 这个代理目标时，它会找到最小化损失的“捷径”或“漏洞”，而这些捷径往往会偏离甚至违背我们最初的意图。
例如，一个被设定为“最大化点击率”的新闻推荐AI，可能会学会推荐耸人听闻的、分裂性的内容，因为它发现这能最有效地达成目标，但这违背了我们“提供高质量、有益信息”的真实意图。这个定律警示我们，外部对齐问题是AI安全中一个根本性的挑战。

**问题 21.3** 假设一所大学使用过去学生的数据来构建预测“学生成功”的模型，这些模型可以为政策和实践的明智变革提供支持。思考偏见可能会如何影响这个模型开发和部署的四个阶段。

**思路与解答：**

1.  **问题规约 (Problem Specification)**：
    *   “学生成功”的定义本身就带有价值偏见。如果定义为“毕业时的起薪”，模型可能会偏向于那些来自富裕家庭或选择高薪专业的学生，而忽视了那些在学术上取得巨大进步但选择低薪公共服务事业的学生。
2.  **数据 (Data)**：
    *   历史数据可能存在偏见。例如，如果历史上某些少数族裔学生因为系统性原因（如缺乏支持、经济压力）退学率较高，模型可能会错误地学到“某个族裔与低成功率相关”，并可能在资源分配上对该族裔的新生产生歧视。
3.  **建模与验证 (Modeling & Validation)**：
    *   如果模型只追求整体预测准确率，它可能会牺牲在少数群体上的表现。例如，一个在95%的主流学生群体上表现良好，但在5%的残障学生群体上表现很差的模型，可能整体准确率很高，但却是不公平的。
4.  **部署 (Deployment)**：
    *   如果学校根据模型的预测来分配奖学金或辅导资源，可能会形成一个**负反馈循环**。被模型错误地预测为“低成功率”的学生可能得不到所需资源，从而真的导致他们不成功，这反过来又“验证”了模型的错误偏见。

**问题 21.4** 我们可以认为功能透明度、结构透明度和运行透明度是正交的。提供一个例子，说明一种透明度的增加可能不会导致另一种透明度的相应增加。

**思路与解答：**

**例子**：一家公司开源了一个AI模型（比如一个图像分类器）的完整**源代码**。
*   **结构透明度增加了**：现在任何人都可以审查代码，了解模型的每一层是如何实现的，算法是如何执行的。
*   **功能透明度没有增加**：仅仅有代码，我们仍然不知道这个模型具体**学会了什么**。它的权重和偏置仍然是一个黑箱。我们无法从代码中直接看出它将如何把输入映射到输出，即它的“逻辑规则”。
*   **运行透明度没有增加**：我们没有该公司用来训练这个模型的具体**训练数据集**，不知道他们使用的**硬件**，更没有最终训练好的**模型权重**。我们无法复现或理解任何一个特定预测实例的完整执行过程。
因此，开源代码（增加结构透明度）并不必然带来功能或运行透明度的增加。

**问题 21.5** 如果一个计算机科学家写了一篇关于AI的研究论文或将代码推送到一个公共仓库，你认为他们是否对他们工作的未来滥用负责？

**思路与解答：**

这是一个复杂的伦理问题，没有简单的“是”或“否”。
*   **支持“负责”的论点 (Pro-responsibility)**：
    *   **可预见性**: 科学家有责任预见其技术最明显的滥用风险（例如，开发更逼真的Deepfake技术，很明显会被用于制造虚假信息）。他们应该在论文中讨论这些风险，并可能采取措施（如添加水印）来缓解。
    *   **双重用途**: 许多技术是双重用途的。科学家不能以“我只关心技术”为由，完全忽视其社会影响。他们是社会的一员，对其创造物的后果负有部分道德责任。
*   **反对“负责”的论点 (Anti-responsibility)**：
    *   **意图**: 科学家的意图是推进知识，而不是作恶。滥用是使用者的问题，而非创造者的问题，就像不能因为有人用刀伤人就去责怪刀匠。
    *   **不可预测性**: 许多滥用方式是不可预测的。要求科学家预见所有可能的未来滥用，是一种不切实际的、过高的道德要求。
    *   **开放科学**: 限制知识的传播（因为担心滥用）可能会阻碍科学进步和有益的应用。开放源代码是科学发展的基石。
*   **我的观点**: 科学家负有**有限的、道德上的责任**。他们不能对所有未来的滥用负责，但他们有责任**积极思考和警示**其工作最直接、最严重的潜在危害，并在可能的情况下，参与到关于如何监管和安全部署这些技术的公共讨论中。完全的“价值中立”是一种逃避。

**问题 21.6** 你认为AI的军事化在多大程度上是不可避免的？

**思路与解答：**

在很大程度上，AI的军事化似乎是**不可避免的**。
*   **历史先例**: 历史上，几乎所有具有革命性的技术（火药、核能、互联网）都被迅速地军事化了。
*   **国家安全困境**: 在一个无政府状态的国际体系中，国家之间存在安全竞争。一个国家如果不在AI军事应用上投入，就会面临在竞争中落后的风险。这种“军备竞赛”的逻辑很难打破。
*   **技术的吸引力**: AI在情报分析、后勤、自主武器、网络战等方面提供了巨大的潜在军事优势，这对任何国家的国防部门都具有不可抗拒的吸引力。

然而，“不可避免”不等于“我们无能为力”。国际社会仍然可以而且应该努力制定关于AI武器使用的**国际条约和规范**（类似于《禁止化学武器公约》），以限制其最危险和最不人道的应用，例如完全自主的致命武器系统。

**问题 21.7** 鉴于21.2节中强调的AI可能被滥用的情况，请分别提出支持和反对深度学习研究开源文化的论点。

**思路与解答：**

*   **支持开源的论点**:
    1.  **加速科学进步**: 开源使得全球的研究者可以站在彼此的肩膀上，快速地验证、改进和创新，这是深度学习在过去十年飞速发展的主要原因。
    2.  **民主化与公平**: 开源降低了研究和应用的门槛，使得小公司、学术界和发展中国家也能参与到AI发展中，防止技术被少数巨头垄断。
    3.  **安全与透明**: 开放的审查使得模型中的漏洞、偏见和安全风险更容易被发现和修复。一个开放的模型比一个不透明的商业黑箱更值得信赖。
*   **反对开源的论点**:
    1.  **助长滥用**: 开源使得恶意行为者（如恐怖分子、诈骗犯、专制政府）可以轻易地获取最先进的技术，并将其用于制造虚假信息、发动网络攻击或进行监控。
    2.  **扩散风险**: 对于那些未来可能具有巨大潜在风险的“超智能”模型，开源可能意味着将一个极其强大的、我们尚不完全理解的技术，无控制地扩散到全世界。
    3.  **责任缺失**: 在开源模式下，当滥用发生时，很难追究责任。代码的贡献者是分散的，每个人都只负有微小的责任。

**问题 21.8** 有人提出，个人数据是拥有它的人的权力来源。讨论个人数据对利用深度学习的公司有价值的方式，并思考隐私的丧失是集体而非个人经历的主张。

**思路与解答：**

1.  **数据的价值**:
    *   **训练模型**: 个人数据是训练深度学习模型的“燃料”。用户行为数据（点击、购买、观看历史）被用来训练推荐系统；照片被用来训练面部识别；文本被用来训练语言模型。
    *   **精准广告**: 公司通过分析用户数据，可以构建极其精确的用户画像，从而进行精准的广告投放，这是许多科技公司的核心商业模式。
    *   **预测与洞察**: 聚合的用户数据可以用来预测社会趋势、金融市场变化等，从而获得商业优势。
2.  **隐私丧失的集体性**:
    *   **个体同意的幻觉**: 当我们“同意”一个隐私政策时，我们往往认为这只是我们个人的决定。
    *   **网络效应**: 但当成千上万的人都分享他们的数据时，AI模型可以从中学习到关于**整个社群**的模式。即使**你**没有分享你的敏感信息（如健康状况），模型也可能通过你的朋友、邻居和你其他非敏感的数据，以很高的概率**推断**出你的敏感信息。
    *   **集体后果**: 隐私的丧失不仅仅是个人信息的泄露。它会改变社会权力结构，导致集体性的监控、社会评分系统的出现，以及对特定群体的歧视。因此，一个人的隐私选择，会影响到整个社群的安全和自由。隐私本质上是一个**集体利益**，而非纯粹的个人权利。

**问题 21.9** 生成式AI对创意产业有何影响？你认为应该如何修改知识产权法以应对这一新发展？

**思路与解答：**

1.  **影响**:
    *   **正面**: 极大地提高了创意工作的效率（如辅助绘画、写作、编曲）；降低了创意表达的门槛，让更多人可以创作。
    *   **负面**: 对人类艺术家的工作构成**替代威胁**，可能导致大量工作岗位流失；引发了关于**作品原创性**和**版权归属**的争议；未经许可使用受版权保护的作品进行训练，侵犯了原创作者的权益。
2.  **知识产权法修改方向**:
    *   **训练数据**: 需要明确使用受版权保护的数据进行AI模型训练的法律边界。是属于“合理使用”，还是需要获得许可并支付报酬？
    *   **AI生成内容的版权**: AI生成的作品是否受版权保护？如果受保护，版权应该归属于谁？是AI的使用者、开发者，还是AI本身？目前的共识倾向于只有人类创作才能获得版权。
    *   **风格模仿**: 法律应如何界定AI对特定艺术家风格的模仿？这是否构成侵权？
    *   **透明度要求**: 是否应该要求公司披露其模型训练所使用的数据集，以方便原创者维权？

**问题 21.10** 一个好的预测必须 (i)足够具体以知道何时出错，(ii)考虑到可能的认知偏见，(iii)允许理性地更新信念。思考最近媒体上关于未来AI的任何主张，并讨论它是否满足这些标准。

**思路与解答：**

**主张示例**：“AGI（通用人工智能）将在未来5年内实现。”
1.  **(i) 不够具体**: “AGI”的定义本身就非常模糊。如何衡量一个系统是否达到了AGI？没有一个公认的标准。因此，即使5年后我们没有看到一个公认的AGI，该主张的支持者也可以说“它正在到来”或“我们对AGI的定义太狭隘了”。这个预测是**不可证伪的**。
2.  **(ii) 未考虑认知偏见**: 这个预测往往受到**乐观偏见**和**技术炒作**的影响。提出者可能是AI领域的既得利益者，或者对技术发展的指数级增长有过分乐观的预期。
3.  **(iii) 不允许理性更新**: 这种预测往往是一种“信念宣告”，而非一个可以根据新证据进行概率更新的科学假设。当出现反面证据时（例如，某些AI能力的发展停滞），这类预测很少被系统地修正。

**结论**: 许多媒体上关于未来AI的宏大预测，往往不满足一个好的预测的标准，它们更多的是一种**观点表达或投机**，而非严谨的科学预测。

**问题 21.11** 一些批评者认为，要求AI民主化的呼声过于关注民主的参与性方面...反思以下几点：AI的哪些方面应该被民主化？为什么？如何实现？

**思路与解答：**

1.  **什么应该被民主化？**
    *   **目标设定与价值观**: 决定一个AI系统（尤其是在公共领域使用的）应该优化什么目标、遵循哪些价值观，这应该是一个公共的、民主的决策过程。
    *   **数据治理**: 关于收集什么数据、如何使用、谁有权访问的决策，应该受到民主监督。
    *   **部署决策**: 决定在哪些高风险领域（如司法、警务、招聘）部署AI系统，应该经过公共辩论和民主授权。
    *   **利益分配**: AI发展带来的巨大经济利益应该如何被公平地分配，这是一个核心的民主问题。
2.  **为什么应该民主化？**
    *   **正当性**: AI系统正在深刻地影响社会。只有当其目标和部署经过受其影响的人们的同意或参与决策，其权力才具有正当性。
    *   **防止滥用**: 将权力集中在少数技术精英或公司手中，会增加滥用和偏见的风险。民主化是一种权力制衡。
3.  **如何实现？**
    *   **不仅仅是参与**: 民主化不等于让每个人都来投票决定模型架构。
    *   **多层次治理**: 需要建立包含公民、专家、政策制定者、受影响社群代表的多层次审议机构。
    *   **教育与普及**: 提升公众对AI的基本理解，使他们能够参与有意义的讨论。
    *   **开源与开放科学**: 促进技术的透明度和可及性，是技术层面民主化的基础。

**问题 21.12** 2023年3月，生命未来研究所发表了一封信，“暂停巨型AI实验”...讨论这封信的动机、公众反应和暂停的影响。

**思路与解答：**

1.  **动机**:
    *   **表面动机**: 对AI（特别是比GPT-4更强大的系统）的快速、无节制发展表示担忧，认为其可能带来“对社会和人类的深远风险”。呼吁在制定和实施一套共享的安全协议之前，暂停训练。
    *   **深层动机**: 可能混合了对**长期存在风险**（如失控的超智能）的真实担忧，以及一些签署者（可能包括一些竞争公司的领导者）希望**减缓领先者的发展步伐**的商业考量。
2.  **公众反应**:
    *   引起了极大的公众关注和辩论。
    *   **支持者**认为这是对AI安全问题的一次必要且及时的警示。
    *   **批评者**（如本题引用的 Gebru 等人）认为，这封信主要由关注长期、假设性风险的“AI末日论者”主导，而**转移了对当前已经存在的、具体的AI危害**（如偏见、歧视、劳工剥削）的注意力。他们认为这是一种**精英主义的、脱离实际的**恐慌。
3.  **影响与集体行动**:
    *   **实际影响**: 几乎没有实验室真的暂停了开发。这封信更像是一次**公共关系的事件**，而非一个有约束力的行动。
    *   **集体行动问题**: 这完美地诠释了AI伦理作为一个集体行动问题。即使所有实验室都同意暂停是“集体最优”的（为了安全），但对于**任何一个单独的实验室**来说，遵守暂停而竞争对手继续开发，都是一个巨大的劣势。因此，在没有强制性外部监管的情况下，每个参与者都有不合作的激励。

**问题 21.13** 讨论21.7节中四点的优点。你同意它们吗？

**思路与解答：**

这四点是：1. 机器学习研究无法避免伦理。2. 纯技术决策也可能蕴含价值。3. 我们应该质疑AI工作的结构。4. 社会和伦理问题不一定需要技术解决方案。

我**完全同意**这四点，它们构成了负责任AI研究的基石。

1.  **研究无法避免伦理**: 同意。随着AI越来越强大并与现实世界紧密结合，任何研究（即使是理论研究）都可能产生社会影响。研究者不能再躲在“象牙塔”里。
2.  **技术决策蕴含价值**: 同意。选择一个损失函数、一个数据集、一个公平性度量，都不是纯粹的“客观”数学决策，背后都包含了关于“什么是好的”、“什么是重要的”的价值判断。
3.  **质疑结构**: 同意。仅仅在现有社会和政治结构下讨论“如何让AI更公平”是不够的。有时我们需要质疑这些结构本身，并思考技术是否在固化不公正的权力关系。
4.  **不一定需要技术方案**: 同意。这是对“技术沙文主义”的有力批判。许多AI试图解决的问题，其根源是深层次的社会问题（如不平等、歧视）。试图用一个“技术补丁”来解决这些问题，往往是治标不治本，甚至可能使问题恶化。有时，最好的解决方案可能不是一个更好的算法，而是一个更好的社会政策。