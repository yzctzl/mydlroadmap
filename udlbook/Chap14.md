# 第十四章
# 无监督学习

第2至9章详细介绍了监督学习的整个流程。我们定义了将观测数据 $\mathbf{x}$ 映射到输出值 $\mathbf{y}$ 的模型，并引入了损失函数来衡量该映射对于训练数据集 $\{\mathbf{x}_i, \mathbf{y}_i\}$ 的质量。然后，我们讨论了如何拟合这些模型并评估其性能。第10至13章则介绍了更复杂的模型架构，这些架构融合了参数共享机制，并允许并行的计算路径。

**无监督学习 (unsupervised learning)** 模型的决定性特征是，它们是从一组没有标签的观测数据 $\{\mathbf{x}_i\}$ 中学习的。所有无监督模型都共享这一特性，但它们的目标各不相同。它们可以用来从数据集中**生成**看似合理的新样本，或者对样本进行**操纵、去噪、插值或压缩**。它们也可以用来揭示数据集的内部结构（例如，通过将其划分为连贯的**簇 (clusters)**），或者用来区分新样本是属于同一数据集还是**异常值 (outliers)**。

本章首先对无监督学习模型进行分类，然后讨论了理想模型的特性以及如何衡量其性能。接下来的四章将讨论四种具体的模型：**生成对抗网络 (generative adversarial networks, GANs)**、**归一化流 (normalizing flows)**、**变分自编码器 (variational autoencoders, VAEs)** 和**扩散模型 (diffusion models)**。¹

> ¹至此，几乎所有相关的数学知识都已嵌入正文中。然而，接下来的四章需要扎实的概率论基础。附录C涵盖了相关内容。

## 14.1 无监督学习模型的分类

无监督学习的一个常用策略是定义数据样本 $\mathbf{x}$ 与一组未见的**潜变量 (latent variables)** $\mathbf{z}$ 之间的映射。这些潜变量捕捉了数据集中的底层结构，并且通常比原始数据具有更低的维度；从这个意义上说，潜变量 $\mathbf{z}$ 可以被看作是数据样本 $\mathbf{x}$ 的一个压缩版本，它抓住了其本质特征（图1.9-1.10）。

原则上，观测变量和潜变量之间的映射可以是双向的。一些模型将数据 $\mathbf{x}$ 映射到潜变量 $\mathbf{z}$。例如，著名的 **k-均值 (k-means)** 算法将数据 $\mathbf{x}$ 映射到一个簇分配 $\mathbf{z} \in \{1, 2, \dots, K\}$。其他模型则将潜变量 $\mathbf{z}$ 映射到数据 $\mathbf{x}$。考虑在这些模型中定义潜变量 $\mathbf{z}$ 上的一个分布 $\text{Pr}(\mathbf{z})$。现在可以通过 (i) 从这个分布中抽样，以及 (ii) 将样本映射到数据空间 $\mathbf{x}$ 来生成新的样本。因此，这些模型被称为**生成模型 (generative models)**（见图14.1）。

第15至18章中的四个模型都是使用潜变量的生成模型。**生成对抗网络**（第15章）学习从潜变量 $\mathbf{z}$ 生成数据样本 $\mathbf{x}^*$，其使用的损失函数旨在鼓励生成的样本与真实样本无法区分（图14.2a）。

**归一化流**、**变分自编码器**和**扩散模型**（第16-18章）是**概率生成模型 (probabilistic generative models)**。除了生成新样本外，它们还为每个数据点 $\mathbf{x}$ 分配一个概率 $\text{Pr}(\mathbf{x}|\boldsymbol{\phi})$。这个概率将取决于模型参数 $\boldsymbol{\phi}$，在训练中，我们最大化观测数据 $\{\mathbf{x}_i\}$ 的概率，因此损失是负对数似然的总和（图14.2b）：

$$
L[\boldsymbol{\phi}] = -\sum_{i=1}^I \log[\text{Pr}(\mathbf{x}_i|\boldsymbol{\phi})].
\tag{14.1}
$$

由于概率分布的和必须为一，这隐式地降低了远离观测数据的样本的概率。除了提供训练标准外，分配概率本身也很有用；测试集上的概率可以用来定量比较两个模型，而一个样本的概率可以被阈值化，以确定它是否属于同一数据集，或者是否是一个**异常值 (outlier)**。²

---

> **图 14.1 无监督学习模型的分类**
> 无监督学习指任何在无标签数据集上训练的模型。生成模型可以合成（生成）与训练数据统计特性相似的新样本。其中的一个子集是概率性的，并定义了数据上的一个分布。我们从这个分布中抽样来生成新样本。潜变量模型定义了一个潜在的解释性（潜）变量与数据之间的映射，并且可能属于上述任何一类。

---

## 14.2 怎样才算好的生成模型？

基于潜变量的生成模型应具备以下特性：

*   **高效采样 (Efficient sampling)**：从模型生成样本应该计算成本低廉，并能利用现代硬件的并行性。
*   **高质量采样 (High-quality sampling)**：样本应该与训练模型的真实数据无法区分。
*   **覆盖度 (Coverage)**：样本应该能代表整个训练分布。仅仅生成看起来像训练样本子集的样本是不够的。
*   **行为良好的潜空间 (Well-behaved latent space)**：每个潜变量 $\mathbf{z}$ 都对应一个看似合理的数据样本 $\mathbf{x}$。$\mathbf{z}$ 的平滑变化对应于 $\mathbf{x}$ 的平滑变化。
*   **解耦的潜空间 (Disentangled latent space)**：操纵 $\mathbf{z}$ 的每个维度应该对应于改变数据的一个可解释的属性。例如，在一个语言模型中，它可能会改变主题、时态或冗长程度。
*   **高效的似然计算 (Efficient likelihood computation)**：如果模型是概率性的，我们希望能够高效、准确地计算新样本的概率。

这自然引出了一个问题：我们所考虑的生成模型是否满足这些特性？答案是主观的，但图14.3提供了一些指导。具体的分配是有争议的，但大多数从业者会同意，没有一个单一的模型能满足所有这些特性。

> ²请注意，并非所有概率生成模型都依赖于潜变量。Transformer解码器（12.7节）是在没有标签的情况下学习的，可以生成新样本，并可以为这些样本分配概率，但它是基于自回归公式（方程12.15）的。

---

> **图 14.2 拟合生成模型**
> a) 生成对抗模型提供了一种生成样本（橙色点）的机制。随着训练的进行（从左到右），损失函数鼓励这些样本变得越来越难以与真实样本（青色点）区分。b) 概率模型（包括变分自编码器、归一化流和扩散模型）学习训练数据上的一个概率分布。随着训练的进行（从左到右），真实样本在该分布下的可能性增加，这可以用来抽取新样本和评估新数据点的概率。

---

## 14.3 量化性能

上一节讨论了生成模型的理想特性。我们现在考虑对生成模型成功与否的量化度量。由于数据的广泛可用性和定性判断样本的便捷性，许多生成模型的实验都使用了图像。因此，其中一些度量仅适用于图像。

**测试集似然 (Test likelihood)**：比较概率模型的一种方法是测量它们在测试数据集上的似然。测量训练数据似然是无效的，因为一个模型可以为每个训练点分配一个非常高的概率，而在点之间分配非常低的概率。这样的模型会有非常高的训练似然，但只能复现训练数据。测试集似然捕捉了模型从训练数据泛化的能力以及**覆盖度**；如果模型只对训练数据的一个子集分配高概率，它必须在其他地方分配较低的概率，因此一部分测试样本将具有低概率。
测试集似然是量化概率模型的一种合理方式，但不幸的是，它与生成对抗模型（不分配概率）无关，并且对于变分自编码器和扩散模型来说计算成本高昂（尽管可以计算对数似然的一个下界）。归一化流是唯一一种可以精确高效地计算似然的模型类型。

**初始得分 (Inception score, IS)**：初始得分专门用于图像，理想情况下是为在ImageNet数据库上训练的生成模型设计的。该分数是使用一个预训练的分类模型计算的，通常是“Inception”模型，其名称也由此而来。它基于两个标准。首先，每个生成的图像 $\mathbf{x}^*$ 应该看起来像ImageNet数据库中1000个可能类别 $\mathbf{y}$ 中的一个且仅一个。因此，概率分布 $\text{Pr}(\mathbf{y}|\mathbf{x}^*)$ 应该在正确的类别上高度集中。其次，整个生成的图像集应该被等概率地分配到各个类别，因此在所有生成的样本上平均时，$\text{Pr}(\mathbf{y})$ 应该是平坦的。

初始得分衡量了这两种分布在生成集上的平均距离。如果一个分布是集中的，而另一个是平坦的，这个距离就会很大（图14.4）。更准确地说，它返回的是 $\text{Pr}(\mathbf{y}|\mathbf{x}_i^*)$ 和 $\text{Pr}(\mathbf{y})$ 之间期望**KL散度 (KL-divergence)** 的指数：参考：Appendix C.5.1 KL divergence

$$
\text{IS} = \exp\left[\frac{1}{I}\sum_{i=1}^I D_{KL}[\text{Pr}(\mathbf{y}|\mathbf{x}_i^*) || \text{Pr}(\mathbf{y})]\right],
\tag{14.2}
$$

其中 $I$ 是生成的样本数量，且：

$$
\text{Pr}(\mathbf{y}) = \frac{1}{I} \sum_{i=1}^I \text{Pr}(\mathbf{y}|\mathbf{x}_i^*).
\tag{14.3}
$$

这个度量仅对ImageNet数据库的生成模型有意义，并且对特定的分类模型很敏感；重新训练这个模型会给出相当不同的数值结果。此外，它不奖励对象类别内的多样性；如果模型只为每个类别生成一个逼真的样本，它也会返回一个很高的值。

**弗雷歇初始距离 (Fréchet inception distance, FID)**：这个度量也旨在用于图像，并计算生成的样本和真实样本分布之间的对称距离。这必须是近似的，因为很难表征任一分布（实际上，表征真实样本的分布正是生成模型的任务）。因此，弗雷歇初始距离通过多元高斯分布来近似这两种分布，并（如其名所示）使用**弗雷歇距离 (Fréchet distance)** 来估计它们之间的距离。参考：Appendix C.5.4 Fréchet distance
然而，它不是针对原始数据来建模距离，而是针对初始分类网络最深层的激活值。这些隐藏单元与对象类别最相关，因此比较是在语义层面上进行的，忽略了图像更精细的细节。这个度量确实考虑了类别内的多样性，但严重依赖于初始网络中特征所保留的信息；任何被网络丢弃的信息都不会对结果产生贡献。其中一些被丢弃的信息可能仍然对生成逼真的样本很重要。

**流形精确率/召回率 (Manifold precision/recall)**：弗雷歇初始距离对样本的逼真度和多样性都很敏感，但不能区分这两个因素。为了解耦这些特性，我们考虑**数据流形 (data manifold)**（即真实样本所在的数据空间子集）和**模型流形 (model manifold)**（即生成的样本所在的子集）之间的重叠。**精确率 (precision)** 是落入数据流形的模型样本的比例。这衡量了生成的样本中有多少是逼真的。**召回率 (recall)** 是落入模型流形的真实数据样本的比例。这衡量了模型能生成的真实数据的比例（图14.5）。
为了估计流形，我们在每个数据样本周围放置一个超球面，其半径是到第 $k$ 个最近邻的距离。这些球体的并集是流形的一个近似，并且很容易确定一个新点是否在其中。这个流形通常也是在分类器的特征空间中计算的，具有随之而来的优点和缺点。

---

> **图 14.3 四种生成模型的特性**
> 无论是生成对抗网络（GANs）、变分自编码器（VAEs）、归一化流（Flows），还是扩散模型（diffusion），都没有完全具备所有理想的特性。

> **图 14.4 初始得分**
> a) 一个预训练的网络对生成的图像进行分类。如果图像是逼真的，得到的类别概率 $\text{Pr}(\mathbf{y}|\mathbf{x}_i^*)$ 应该在正确的类别上是集中的。b) 如果模型平等地生成所有类别，边际（平均）类别概率应该是平坦的。初始得分衡量了(a)中的分布和(b)中的分布之间的平均距离。图片来自 Deng et al. (2009)。

> **图 14.5 流形精确率/召回率**
> a) 真实样本和由生成模型合成的样本的真实分布。b) 重叠可以用精确率（与真实样本的分布或流形重叠的合成样本的比例）和 c) 召回率（与合成样本的流形重叠的真实样本的比例）来概括。d) 合成样本的流形可以通过取以每个样本为中心的一组超球体的并集来近似。这里，它们的半径是恒定的，但更常见的是，半径基于到第k个最近邻的距离。e) 真实样本的流形也以类似的方式近似。f) 精确率可以计算为位于真实样本的近似流形内的样本的比例。类似地，召回率计算为位于样本的近似流形内的真实样本的比例（未显示）。改编自 Kynkäänniemi et al. (2019)。

---

## 14.4 总结

无监督模型在没有标签的情况下学习数据集的结构。这些模型的一个子集是**生成式**的，可以合成新的数据样本。还有一个更小的子集是**概率性**的，它们既可以生成新样本，也可以为观测数据分配一个概率。接下来的四章中考虑的模型始于一个具有已知分布的潜变量 $\mathbf{z}$。一个深度神经网络然后从潜变量映射到观测数据空间。我们考虑了生成模型的理想特性，并介绍了一些试图量化其性能的度量。

### 注释

流行的生成模型包括生成对抗网络 (Goodfellow et al., 2014)、变分自编码器 (Kingma & Welling, 2014)、归一化流 (Rezende & Mohamed, 2015)、扩散模型 (Sohl-Dickstein et al., 2015; Ho et al., 2020)、自回归模型 (Bengio et al., 2000; Van den Oord et al., 2016b) 和基于能量的模型 (LeCun et al., 2006)。除了基于能量的模型外，本书都讨论了这些模型。Bond-Taylor et al. (2022) 提供了生成模型的近期综述。

**评估**：Salimans et al. (2016) 引入了初始得分，Heusel et al. (2017) 引入了弗雷歇初始距离，两者都基于 Inception V3 模型 (Szegedy et al., 2016) 的 Pool-3 层。Nash et al. (2021) 使用了同一网络的更早层，这些层保留了更多的空间信息，以确保图像的空间统计也被复制。Kynkäänniemi et al. (2019) 引入了流形精确率/召回率方法。Barratt & Sharma (2018) 详细讨论了初始得分并指出了其弱点。Borji (2022) 讨论了评估生成模型的不同方法的优缺点。