## Adam：一种随机优化方法

**摘要**
我们引入了 Adam，这是一种用于随机目标函数一阶梯度优化的算法，其基于对低阶矩的自适应估计。Adam 算法易于实现、计算高效、内存需求少，对梯度的对角缩放具有不变性，并且非常适用于数据和/或参数量庞大的问题。该方法也适用于非平稳目标和梯度非常嘈杂和/或稀疏的问题。算法的超参数具有直观的解释，并且通常只需很少的调优。本文还讨论了 Adam 受到启发并与之相关的算法。我们还分析了该算法的理论收敛特性，并提供了收敛速度的遗憾界（regret bound），其可与在线凸优化框架下已知的最佳结果相媲美。实证结果表明，Adam 在实践中表现良好，并且优于其他随机优化方法。最后，我们讨论了 AdaMax，一种基于无穷范数的 Adam 变体。

**1 引言**
基于随机梯度的优化在许多科学和工程领域中都具有核心的实际重要性。这些领域中的许多问题都可以被视为优化某个标量参数化目标函数，需要对其参数进行最大化或最小化。如果该函数对其参数是可微分的，梯度下降是一种相对有效的优化方法，因为计算相对于所有参数的一阶偏导数与仅评估该函数的计算复杂度相同。通常，目标函数是随机的。例如，许多目标函数由在不同数据子样本上评估的子函数之和组成；在这种情况下，可以通过对单个子函数进行梯度步长来使优化更有效，即随机梯度下降（SGD）或上升。SGD 被证明是一种高效且有效的优化方法，在许多机器学习的成功案例中起到了核心作用，例如深度学习的最新进展。目标函数也可能有其他来源的噪声，比如 dropout 正则化。对于所有这些嘈杂的目标，都需要有效的随机优化技术。本文的重点是优化具有高维参数空间的随机目标。在这种情况下，高阶优化方法并不适用，因此本文的讨论仅限于一阶方法。

我们提出了 Adam，一种高效的随机优化方法，它只需要一阶梯度且内存需求很小。该方法根据对梯度的一阶矩和二阶矩的估计，为不同的参数计算独立的自适应学习率；Adam 这个名字来源于自适应矩估计（**ada**ptive **m**oment estimation）。我们的方法旨在结合两种近期流行方法的优点：AdaGrad（Duchi et al., 2011）善于处理稀疏梯度，而 RMSProp（Tieleman & Hinton, 2012）则在线性和非平稳设置中表现良好；我们将在第 5 节中阐明 Adam 与这些方法以及其他随机优化方法的重要联系。Adam 的一些优点包括：参数更新的幅度对梯度的重缩放具有不变性，其步长近似受步长超参数的约束，它不需要平稳的目标，能与稀疏梯度配合良好，并且自然地执行一种步长退火。

**2 算法**

关于我们提出的 Adam 算法的伪代码，请参阅算法 1。令 $f(\theta)$ 为一个嘈杂的目标函数：一个对参数 $\theta$ 可微的随机标量函数。我们感兴趣的是最小化该函数关于其参数 $\theta$ 的期望值 $\mathbb{E}[f(\theta)]$。我们用 $f_{1}(\theta),...,f_{T}(\theta)$ 表示在后续时间步长 $1,...,T$ 的随机函数的实现。随机性可能来自于对数据点随机子样本（小批量）的评估，或者源于固有的函数噪声。我们用 $g_{t}=\nabla_{\theta}f_{t}(\theta)$ 表示梯度，即在时间步长 t 评估的 $f_{t}$ 对参数 $\theta$ 的偏导数向量。

该算法更新梯度（$m_{t}$）和平方梯度（$v_{t}$）的指数移动平均值，其中超参数 $\beta_{1},\beta_{2}\in[0,1)$ 控制这些移动平均值的指数衰减率。这些移动平均值本身是梯度的一阶矩（均值）和二阶原始矩（未中心化方差）的估计。然而，这些移动平均值被初始化为（零向量），导致矩估计有偏向于零，尤其是在初始时间步长，当衰减率较小（即 $\beta$ 接近 1）时尤为明显。好消息是，这种初始化偏差可以轻松抵消，从而得到偏差校正的估计 $\hat{m}_{t}$ 和 $\hat{v}_{t}$。更多细节请参阅第 3 节。

请注意，算法 1 的效率可以通过改变计算顺序来提高，尽管会牺牲清晰度，例如，用以下行替换循环中的最后三行：
$\alpha_{t}=\alpha\cdot\sqrt{1-\beta_{2}^{t}}/(1-\beta_{1}^{t})$ 和 $\theta_{t}\leftarrow\theta_{t-1}-\alpha_{t}\cdot m_{t}/(\sqrt{v_{t}}+\epsilon)$。

**算法 1：我们提出的随机优化算法 Adam。**
有关详细信息，请参阅第 2 节，以及略微更高效（但不太清晰）的计算顺序。$g_{t}^{2}$ 表示元素级别的平方 $g_{t}\odot g_{t}$。对于测试的机器学习问题，良好的默认设置为 $\alpha=0.001$，$\beta_{1}=0.9$，$\beta_{2}=0.999$ 和 $\epsilon=10^{-8}$。所有向量操作都是元素级别的。我们用 $\beta_{1}^{t}$ 和 $\beta_{2}^{t}$ 表示 $\beta_{1}$ 和 $\beta_{2}$ 的 t 次方。

**要求:** $\alpha$：步长；
**要求:** $\beta_{1},\beta_{2}\in[0,1)$：矩估计的指数衰减率；
**要求:** $f(\theta)$：带有参数 $\theta$ 的随机目标函数；
**要求:** $\theta_{0}$：初始参数向量；

$m_{0}\leftarrow0$ （初始化一阶矩向量）；
$v_{0}\leftarrow0$ （初始化二阶矩向量）；
$t\leftarrow0$ （初始化时间步）；

**当** $\theta_{t}$ 未收敛 **时**：
- $t\leftarrow t+1$
- $g_{t}\leftarrow\nabla_{\theta}f_{t}(\theta_{t-1})$ （获取时间步 t 随机目标关于 $\theta$ 的梯度）；
- $m_{t}\leftarrow\beta_{1}\cdot m_{t-1}+(1-\beta_{1})\cdot g_{t}$ （更新有偏的一阶矩估计）；
- $v_{t}\leftarrow\beta_{2}\cdot v_{t-1}+(1-\beta_{2})\cdot g_{t}^{2}$ （更新有偏的二阶原始矩估计）；
- $\hat{m}_{t}\leftarrow m_{t}/(1-\beta_{1}^{t})$ （计算偏差校正后的一阶矩估计）；
- $\hat{v}_{t}\leftarrow v_{t}/(1-\beta_{2}^{t})$ （计算偏差校正后的二阶原始矩估计）；
- $\theta_{t}\leftarrow\theta_{t-1}-\alpha\cdot\hat{m}_{t}/(\sqrt{\hat{v}_{t}}+\epsilon)$ （更新参数）；
**结束**
**返回** $\theta_{t}$ （最终参数）；

**2.1 Adam 的更新规则**

Adam 更新规则的一个重要特性是其对步长的细致选择。假设 $\epsilon=0$，在时间步 t 采取的有效参数空间步长是 $\Delta_{t}=\alpha\cdot\hat{m}_{t}/\sqrt{\hat{v}_{t}}$。有效步长有两个上限：在最严重的稀疏情况下（当某个梯度除了当前时间步外在所有时间步都为零时），$|\Delta_{t}|\le\alpha\cdot(1-\beta_{1})/\sqrt{1-\beta_{2}}$；否则，有效步长将更小，即 $|\Delta_{t}|\le\alpha$。

在更常见的情况下，$\hat{m}_{t}/\sqrt{\hat{v}_{t}}\approx\pm1$，因为 $[\mathbb{E}[g]/\sqrt{\mathbb{E}[g^{2}]}|\le1$。在每个时间步中，参数空间中采取的步长幅度近似受到步长设置 $\alpha$ 的约束，即 $|\Delta_{t}|\cong\alpha$。这可以理解为在当前参数值周围建立了一个信任域，超出该区域，当前的梯度估计无法提供足够的信息。这使得通常很容易预先知道 $\alpha$ 的合适范围。例如，对于许多机器学习模型，我们通常预先知道好的最优解很可能在参数空间的某个区域内；例如，对参数有一个先验分布并不少见。由于 $\alpha$ 设定了参数空间中步长幅度的（上限），我们通常可以推断出 $\alpha$ 的正确数量级，以便从 $\theta_{0}$ 在一定数量的迭代内达到最优解。

为了简化术语，我们将比率 $\hat{m}_{t}/\sqrt{\hat{v}_{t}}$ 称为信噪比（**SNR**）。信噪比越小，有效步长 $\Delta_{t}$ 就会越接近零。这是一个理想的特性，因为较小的信噪比意味着关于 $\hat{m}_{t}$ 方向是否与真实梯度方向一致存在更大的不确定性。例如，信噪比值通常在接近最优解时变得更接近 0，从而导致参数空间中的有效步长变小：这是一种自动退火的形式。有效步长 $\Delta_{t}$ 对梯度的尺度也具有不变性；用因子 $c$ 重缩放梯度 $g$ 会将 $\hat{m}_{t}$ 缩放一个因子 $c$，将 $\hat{v}_{t}$ 缩放一个因子 $c^{2}$，这两个因子会相互抵消：$(c\cdot\hat{m}_{t})/(\sqrt{c^{2}\cdot\hat{v}_{t}})=\hat{m}_{t}/\sqrt{\hat{v}_{t}}$。

**3 初始化偏差校正**

正如第 2 节所解释的，Adam 利用了初始化偏差校正项。我们将在这里推导二阶矩估计的校正项；一阶矩估计的推导是完全类似的。

令 $g$ 为随机目标函数 $f$ 的梯度，我们希望使用平方梯度的指数移动平均值来估计其二阶原始矩（未中心化方差），衰减率为 $\beta_{2}$。令 $g_{1},...,g_{T}$ 为后续时间步长的梯度，每个梯度都是从底层梯度分布 $g_{t}\sim p(g_{t})$ 中抽取。我们将指数移动平均值初始化为 $v_{0}=0$（一个零向量）。首先注意，时间步长 t 的指数移动平均值更新 $v_{t}=\beta_{2}\cdot v_{t-1}+(1-\beta_{2})\cdot g_{t}^{2}$（其中 $g_{t}^{2}$ 表示元素级别的平方 $g_{t}\odot g_{t}$）可以写成所有先前时间步长的梯度的函数：
$$v_{t}=(1-\beta_{2})\sum_{i=1}^{t}\beta_{2}^{t-i}\cdot g_{i}^{2} \quad(1)$$
我们希望知道在时间步长 t，指数移动平均值的期望值 $\mathbb{E}[v_{t}]$ 与真实二阶矩 $\mathbb{E}[g_{t}^{2}]$ 的关系，以便我们可以校正两者之间的差异。对等式 (1) 的左右两边取期望：
$$\mathbb{E}[v_{t}]=\mathbb{E}\left[(1-\beta_{2})\sum_{i=1}^{t}\beta_{2}^{t-i}\cdot g_{i}^{2}\right] \quad(2)$$
$$=\mathbb{E}[g_{t}^{2}]\cdot(1-\beta_{2})\sum_{i=1}^{t}\beta_{2}^{t-i}+\zeta \quad(3)$$
$$=\mathbb{E}[g_{t}^{2}]\cdot(1-\beta_{2}^{t})+\zeta \quad(4)$$
其中，如果真实二阶矩 $\mathbb{E}[g_{i}^{2}]$ 是平稳的，则 $\zeta=0$；否则，由于指数衰减率 $\beta_{1}$ 可以（并且应该）被选择得使得指数移动平均值对太久远的梯度分配较小的权重，$\zeta$ 可以保持很小。剩下的项 $(1-\beta_{2}^{t})$ 是由将移动平均值初始化为零引起的。因此，在算法 1 中，我们用这个项来除以，以校正初始化偏差。

在稀疏梯度的情况下，为了获得对二阶矩的可靠估计，需要通过选择较小的 $\beta_{2}$ 值对许多梯度进行平均；然而，正是在这种 $\beta_{2}$ 较小的情况下，缺乏初始化偏差校正会导致初始步长大得多。

**4 收敛性分析**

我们使用（Zinkevich, 2003）提出的在线学习框架来分析 Adam 的收敛性。给定一个任意的、未知的凸成本函数序列 $f_{1}(\theta)$, $f_{2}(\theta),...,f_{T}(\theta)$。在每个时间步 t，我们的目标是预测参数 $\theta_{t}$ 并在一个先前未知的成本函数 $f_{t}$ 上进行评估。由于序列的性质事先未知，我们使用遗憾（regret）来评估我们的算法，即所有先前步骤中在线预测 $f_{t}(\theta_{t})$ 与可行集 X 中最佳固定点参数 $f_{t}(\theta^{*})$ 之间的差异之和。具体来说，遗憾定义为：
$$R(T)=\sum_{t=1}^{T}[f_{t}(\theta_{t})-f_{t}(\theta^{*})] \quad(5)$$
其中 $\theta^{*}=\arg\min_{\theta\in X}\sum_{t=1}^{T}f_{t}(\theta)$。我们证明了 Adam 有一个 $O(\sqrt{T})$ 的遗憾界，并在附录中给出了证明。我们的结果与这种一般凸在线学习问题已知的最佳界限相当。

我们还使用了一些定义来简化我们的符号，其中 $g_{t}\triangleq\nabla f_{t}(\theta_{t})$ 并且 $g_{t,i}$ 是第 $i$ 个元素。我们将 $g_{1:t,i}\in\mathbb{R}^{t}$ 定义为一个向量，它包含直到时间 t 的所有迭代中第 $i$ 个维度的梯度，$g_{1:t,i}=[g_{1,i},g_{2,i},\cdot\cdot\cdot,g_{t,i}]$。此外，我们定义 $\gamma\triangleq\frac{\beta_{1}^{2}}{\sqrt{\beta_{2}}}$。当学习率 $\alpha_{t}$ 以 $t^{-\frac{1}{2}}$ 的速率衰减，并且一阶矩移动平均系数 $\beta_{1,t}$ 以 $\lambda$ 指数衰减时（$\lambda$ 通常接近 1，例如 $1-10^{-8}$），我们的以下定理成立。

**定理 4.1：** 假设函数 $f_{t}$ 具有有界梯度，$\|f_{t}(\theta)\|_{2}\le G$，$||\nabla f_{t}(\theta)||_{\infty}\le G_{\infty}$ 对于所有 $\theta\in R^{d}$，并且由 Adam 生成的任何 $\theta_{t}$ 之间的距离是有界的，$||\theta_{n}-\theta_{m}||_{2}\le D$， $||\theta_{m}-\theta_{n}||_{\infty}\le D_{\infty}$ 对于任何 $m,n\in\{1,...,T\}$，并且 $\beta_{1}$，$\beta_{2}\in[0,1)$ 满足 $\frac{\beta_{1}^{2}}{\sqrt{\beta_{2}}}<1$。设 $\alpha_{t}=\frac{\alpha}{\sqrt{t}}$ 和 $\beta_{1,t}=\beta_{1}\lambda^{t-1},\lambda\in(0,1)$。对于所有 $T\ge1$，Adam 实现了以下保证：
$$R(T)\le\frac{D^{2}}{2\alpha(1-\beta_{1})}\sum_{i=1}^{d}\sqrt{T\hat{v}_{T,i}}+\frac{\alpha(1+\beta_{1})G_{\infty}}{(1-\beta_{1})\sqrt{1-\beta_{2}}(1-\gamma)^{2}}\sum_{i=1}^{d}||g_{1:T,i}||_{2}+\sum_{i=1}^{d}\frac{D_{\infty}^{2}G_{\infty}\sqrt{1-\beta_{2}}}{2\alpha(1-\beta_{1})(1-\lambda)^{2}}$$
我们的定理 4.1 表明，当数据特征稀疏且梯度有界时，求和项可以远小于其上限 $\sum_{i=1}^{d}||g_{1:T,i}||_{2}<<dG_{\infty}\sqrt{T}$ 和 $\sum_{i=1}^{d}\sqrt{T\hat{v}_{T,i}}<<dG_{\infty}\sqrt{T}$，特别如果函数类别和数据特征的形式如（Duchi et al., 2011）第 1.2 节所述。他们关于期望值 $\mathbb{E}[\sum_{i=1}^{d}||g_{1:T,i}||_{2}]$ 的结果也适用于 Adam。特别是，自适应方法，如 Adam 和 Adagrad，可以实现 $O(log~d\sqrt{T})$，这是对非自适应方法 $O(\sqrt{dT})$ 的改进。在我们的理论分析中，将 $\beta_{1,t}$ 衰减到零是重要的，这也与之前的实证发现相符，例如（Sutskever et al., 2013）建议在训练结束时减小动量系数可以改善收敛。

最后，我们可以证明 Adam 的平均遗憾收敛。

**推论 4.2：** 假设函数 $f_{t}$ 具有有界梯度。$||\nabla f_{t}(\theta)||_{2}\le G,||\nabla f_{t}(\theta)||_{\infty}\le G_{\infty}$ 对于所有 $\theta\in R^{d}$，并且由 Adam 生成的任何 $\theta_{t}$ 之间的距离是有界的，$||\theta_{n}-\theta_{m}||_{2}\le D$，$||\theta_{m}-\theta_{n}||_{\infty}\le D_{\infty}$ 对于任何 m, $n\in\{1,...,T\}$。对于所有 $T\ge1$，Adam 实现了以下保证：
$$\frac{R(T)}{T}=O\left(\frac{1}{\sqrt{T}}\right)$$
这个结果可以通过使用定理 4.1 和 $\sum_{i=1}^{d}||g_{1:T,i}||_{2}\le dG_{\infty}\sqrt{T}$ 得到。因此，$lim_{T\rightarrow\infty}\frac{R(T)}{T}=0.$

**5 相关工作**

与 Adam 直接相关的优化方法是 RMSProp（Tieleman & Hinton, 2012; Graves, 2013）和 AdaGrad（Duchi et al., 2011）；这些关系将在下面讨论。其他随机优化方法包括 vSGD（Schaul et al., 2012）、AdaDelta（Zeiler, 2012）和 Roux & Fitzgibbon (2010) 的自然牛顿法，所有这些方法都通过估计一阶信息来设置步长。函数和优化器（SFO）（Sohl-Dickstein et al., 2014）是一种基于小批量的拟牛顿法，但（与 Adam 不同）其内存需求与数据集中小批量分区的数量呈线性关系，这在内存受限的系统（如 GPU）上通常是不可行的。

像自然梯度下降（NGD）（Amari, 1998）一样，Adam 采用了一个适应数据几何的预处理器，因为 $\hat{v_{t}}$ 是费舍尔信息矩阵对角线的一个近似（Pascanu & Bengio, 2013）；然而，Adam 的预处理器（如 AdaGrad 的）在适应性方面比原始的 NGD 更保守，因为它使用费舍尔信息矩阵近似的平方根逆来预处理。

**RMSProp：** 一个与 Adam 密切相关的优化方法是 RMSProp（Tieleman & Hinton, 2012）。带有动量的一个版本有时也会被使用（Graves, 2013）。带有动量的 RMSProp 和 Adam 之间有一些重要的区别：带有动量的 RMSProp 使用重新缩放梯度的动量来生成其参数更新，而 Adam 的更新是直接使用梯度的一阶和二阶矩的移动平均值来估计的。RMSProp 还缺少一个偏差校正项；这在 $\beta_{2}$ 值接近 1（稀疏梯度情况下需要）时最重要，因为在这种情况下不校正偏差会导致非常大的步长并经常导致发散，正如我们在第 6.4 节中通过实验证明的那样。

**AdaGrad：** AdaGrad（Duchi et al., 2011）是一种对稀疏梯度效果良好的算法。其基本版本将参数更新为 $\theta_{t+1}=\theta_{t}-\alpha\cdot g_{t}/\sqrt{\sum_{i=1}^{t}g_{t}^{2}}$。请注意，如果我们选择 $\beta_{2}$ 无限接近于 1，那么 $lim_{\beta_{2}\rightarrow1}\hat{v}_{t}=t^{-1}\cdot\sum_{i=1}^{t}g_{t}^{2}$。AdaGrad 对应于 Adam 的一个版本，其中 $\beta_{1}=0$、$(1-\beta_{2})$ 无限小，并且 $\alpha$ 被一个退火版本 $\alpha_{t}=\alpha\cdot t^{-1/2}$ 替换，即 $\theta_{t}-\alpha\cdot t^{-1/2}\cdot\hat{m}_{t}/\sqrt{lim_{\beta_{2}\rightarrow1}\hat{v}_{t}}=\theta_{t}-\alpha\cdot t^{-1/2}\cdot g_{t}/\sqrt{t^{-1}\cdot\sum_{i=1}^{t}g_{t}^{2}}=\theta_{t}-\alpha\cdot g_{t}/\sqrt{\sum_{i=1}^{t}g_{t}^{2}}$。请注意，当移除偏差校正项时，Adam 和 Adagrad 之间这种直接对应关系不成立；没有偏差校正，就像在 RMSProp 中一样，一个无限接近于 1 的 $\beta_{2}$ 会导致无限大的偏差和无限大的参数更新。

**6 实验**

为了对所提出的方法进行实证评估，我们研究了不同的流行机器学习模型，包括逻辑回归、多层全连接神经网络和深度卷积神经网络。通过使用大型模型和数据集，我们证明了 Adam 可以有效地解决实际的深度学习问题。在比较不同的优化算法时，我们使用相同的参数初始化。超参数，如学习率和动量，在密集网格上进行搜索，并报告最佳超参数设置下的结果。

**6.1 实验：逻辑回归**

我们在 MNIST 数据集上评估了我们提出的方法在 L2 正则化的多类别逻辑回归上的表现。逻辑回归具有一个经过充分研究的凸目标，这使得它适合用于比较不同的优化器，而无需担心局部最小值问题。在我们的逻辑回归实验中，步长 $\alpha$ 通过 $1/\sqrt{t}$ 衰减进行调整，即 $\alpha_{t}=\frac{\alpha}{\sqrt{t}}$，这与我们从第 4 节的理论预测相符。逻辑回归直接在 784 维图像向量上对类别标签进行分类。我们使用小批量大小为 128，将 Adam 与带有 Nesterov 动量的加速 SGD 和 Adagrad 进行比较。根据图 1，我们发现 Adam 产生了与带有动量的 SGD 相似的收敛性，并且两者都比 Adagrad 收敛得更快。

正如（Duchi et al., 2011）所讨论的，Adagrad 可以有效地处理稀疏特征和梯度，这是其主要理论结果之一，而 SGD 在学习稀有特征方面表现不佳。从理论上讲，步长为 $1/\sqrt{t}$ 衰减的 Adam 应该与 Adagrad 的性能相匹配。我们使用来自（Maas et al., 2011）的 IMDB 电影评论数据集来检验稀疏特征问题。我们将 IMDB 电影评论预处理为词袋（BoW）特征向量，其中包括最常见的 10,000 个词。每个评论的 10,000 维词袋特征向量都是高度稀疏的。正如（Wang & Manning, 2013）所建议的，可以在训练期间对 BoW 特征应用 50% 的 dropout 噪声以防止过拟合。在图 1 中，无论有没有 dropout 噪声，Adagrad 都以很大的优势优于带有 Nesterov 动量的 SGD。Adam 收敛得和 Adagrad 一样快。Adam 的实证性能与我们在第 2 节和第 4 节的理论发现一致。与 Adagrad 类似，Adam 可以利用稀疏特征并获得比普通带有动量的 SGD 更快的收敛速度。

---

**图1:** MNIST 图像和 IMDB 电影评论（包含 10,000 个词袋（BoW）特征向量）上的逻辑回归训练负对数似然。

---

**6.2 实验：多层神经网络**

多层神经网络是具有非凸目标函数的强大模型。尽管我们的收敛性分析不适用于非凸问题，但我们通过实验发现 Adam 在这种情况下通常优于其他方法。在我们的实验中，我们选择的模型与该领域之前发布的论文一致；该实验使用了两个全连接隐藏层，每个隐藏层有 1000 个隐藏单元和 ReLU 激活函数，小批量大小为 128。

首先，我们使用标准的确定的交叉熵目标函数，对参数进行 $L_{2}$ 权重衰减以防止过拟合，研究了不同优化器。函数和优化器（SFO）方法（Sohl-Dickstein et al., 2014）是一种最近提出的拟牛顿法，可与小批量数据配合使用，并在优化多层神经网络方面表现出良好的性能。我们使用了他们的实现，并与 Adam 进行比较以训练此类模型。图 2 显示，Adam 在迭代次数和挂钟时间方面都取得了更快的进展。由于更新曲率信息的成本，SFO 的每次迭代比 Adam 慢 5-10 倍，并且其内存需求与小批量的数量呈线性关系。

随机正则化方法，例如 dropout，是防止过拟合的有效方法，并且由于其简单性而在实践中经常使用。SFO 假定子函数是确定的，并且确实无法在具有随机正则化的成本函数上收敛。我们比较了 Adam 在使用 dropout 噪声训练的多层神经网络上与其他随机一阶方法的有效性。图 2 显示了我们的结果；Adam 显示出比其他方法更好的收敛性。

---

**图2:** MNIST 图像上的多层神经网络训练。(a) 使用 dropout 随机正则化的神经网络。(b) 具有确定性成本函数的神经网络。我们与函数和优化器（SFO）优化器（Sohl-Dickstein et al., 2014）进行了比较。

---

**6.3 实验：卷积神经网络**

具有多层卷积、池化和非线性单元的卷积神经网络（CNN）在计算机视觉任务中取得了相当大的成功。与大多数全连接神经网络不同，CNN 中的权重共享导致不同层中的梯度差异巨大。在应用 SGD 时，实践中通常对卷积层使用较小的学习率。我们展示了 Adam 在深度 CNN 中的有效性。我们的 CNN 架构有三个交替的 5x5 卷积滤波器和 3x3 最大池化阶段，步长为 2，之后是一个包含 1000 个整流线性隐藏单元（ReLU）的全连接层。输入图像通过白化进行预处理，并且 dropout 噪声应用于输入层和全连接层。小批量大小也设置为 128，与之前的实验类似。

有趣的是，尽管 Adam 和 Adagrad 在训练的初始阶段都取得了快速的成本降低，如图 3（左）所示。但 Adam 和 SGD 最终比 Adagrad 在 CNN 上的收敛速度快得多，如图 3（右）所示。我们注意到，二阶矩估计在几个 epoch 后趋于零，并且在算法 1 中由 $\epsilon$ 主导。因此，与第 6.2 节中的全连接网络相比，二阶矩估计在 CNN 中对成本函数几何的近似较差。然而，通过一阶矩减少小批量方差在 CNN 中更为重要，并有助于加速。因此，在这个特定实验中，Adagrad 的收敛速度比其他方法慢得多。尽管 Adam 相对于带有动量的 SGD 仅显示出微小的改进，但它为不同层自适应地调整学习率，而不是像在 SGD 中那样手动选择。

---

**图3:** 卷积神经网络训练成本。(左) 前三个 epoch 的训练成本。(右) 45 个 epoch 的训练成本。CIFAR-10 数据集上的 c64-c64-c128-1000 架构。

---

**6.4 实验：偏差校正项**

我们还通过实验评估了第 2 节和第 3 节中解释的偏差校正项的效果。如第 5 节所讨论的，移除偏差校正项会得到一个带有动量的 RMSProp 版本（Tieleman & Hinton, 2012）。我们在训练变分自动编码器（VAE）时改变了 $\beta_{1}$ 和 $\beta_{2}$，该 VAE 的架构与（Kingma & Welling, 2013）中的相同，具有一个隐藏层，其中包含 500 个带有 softplus 非线性的隐藏单元和一个 50 维的球面高斯潜在变量。我们遍历了广泛的超参数选择范围，即 $\beta_{1}\in[0,0.9]$ 和 $\beta_{2}\in[0.99,0.999,0.9999]$，以及 $log_{10}(\alpha)\in[-5,...,-1]$。

接近 1 的 $\beta_{2}$ 值（在对稀疏梯度具有鲁棒性时需要）会导致更大的初始化偏差；因此，我们期望偏差校正项在这种缓慢衰减的情况下很重要，可以防止对优化的不利影响。在图 4 中，接近 1 的 $\beta_{2}$ 值确实在没有偏差校正项时导致训练不稳定，尤其是在训练的前几个 epoch。当存在偏差校正时，最好的结果是在小的 $(1-\beta_{2})$ 值下获得的；在优化结束时，当隐藏单元专门处理特定模式时，梯度往往变得更稀疏，这一点更加明显。总之，无论超参数设置如何，Adam 的表现都等于或优于 RMSProp。

---

**图4:** 偏差校正项（红线）与无偏差校正项（绿线）的效果，在训练变分自动编码器（VAE）（Kingma & Welling, 2013）时，在 10 个 epoch（左）和 100 个 epoch（右）后的损失（y 轴）对比不同步长 $\alpha$（x 轴）和超参数 $\beta_{1}$ 和 $\beta_{2}$ 的设置。

---

**7 扩展**

**7.1 AdaMax**

在 Adam 中，单个权重的更新规则是将其梯度与当前和过去梯度的（缩放）$L^{2}$ 范数成反比地进行缩放。我们可以将基于 $L^{2}$ 范数的更新规则推广到基于 $L^{p}$ 范数的更新规则。这类变体对于大的 $p$ 来说在数值上变得不稳定。然而，在我们将 $p\rightarrow\infty$ 的特殊情况下，出现了一种惊人简单且稳定的算法；请参阅算法 2。我们现在将推导该算法。

在 $L^{p}$ 范数的情况下，令时间 t 的步长与 $v_{t}^{1/p}$ 成反比，其中：
$$v_{t}=\beta_{2}^{p}v_{t-1}+(1-\beta_{2}^{p})|g_{t}|^{p} \quad(6)$$
$$=(1-\beta_{2}^{p})\sum_{i=1}^{t}\beta_{2}^{p(t-i)}\cdot|g_{i}|^{p} \quad(7)$$
请注意，这里的衰减项被等价地参数化为 $\beta_{2}^{P}$ 而不是 $\beta_{2}$。现在令 $p\rightarrow\infty$，并定义 $u_{t}=lim_{p\rightarrow\infty}(v_{t})^{1/p}$，那么：
$$u_{t}=lim_{p\rightarrow\infty}(v_{t})^{1/p}=lim_{p\rightarrow\infty}\left((1-\beta_{2}^{p})\sum_{i=1}^{t}\beta_{2}^{p(t-i)}\cdot|g_{i}|^{p}\right)^{1/p} \quad(8)$$
$$=lim_{p\rightarrow\infty}(1-\beta_{2}^{p})^{1/p}\left(\sum_{i=1}^{t}\beta_{2}^{p(t-i)}\cdot|g_{i}|^{p}\right)^{1/p} \quad(9)$$
$$=lim_{p\rightarrow\infty}\left(\sum_{i=1}^{t}(\beta_{2}^{(t-i)}\cdot|g_{i}|)^{p}\right)^{1/p} \quad(10)$$
$$=max(\beta_{2}^{t-1}|g_{1}|,\beta_{2}^{t-2}|g_{2}|,...,\beta_{2}|g_{t-1}|,|g_{t}|) \quad(11)$$
这对应于一个非常简单的递归公式：
$$u_{t}=max(\beta_{2}\cdot u_{t-1},|g_{t}|) \quad(12)$$
初始值为 $u_{0}=0$。请注意，在这种情况下，我们不需要校正初始化偏差，这非常方便。还要注意，AdaMax 的参数更新幅度比 Adam 有一个更简单的界限，即：$|\Delta_{t}|\le\alpha$。

**算法 2：AdaMax，一种基于无穷范数的 Adam 变体。**
有关详细信息，请参阅第 7.1 节。对于测试的机器学习问题，良好的默认设置为 $\alpha=0.002$、$\beta_{1}=0.9$ 和 $\beta_{2}=0.999$。我们用 $\beta_{1}^{t}$ 表示 $\beta_{1}$ 的 t 次方。这里，$(\alpha/(1-\beta_{1}^{t}))$ 是带有第一矩偏差校正项的学习率。所有向量操作都是元素级别的。

**要求:** $\alpha$：步长；
**要求:** $\beta_{1},\beta_{2}\in[0,1)$：指数衰减率；
**要求:** $f(\theta)$：带有参数 $\theta$ 的随机目标函数；
**要求:** $\theta_{0}$：初始参数向量；

$m_{0}\leftarrow0$ （初始化一阶矩向量）；
$u_{0}\leftarrow0$ （初始化指数加权无穷范数）；
$t\leftarrow0$ （初始化时间步）；

**当** $\theta_{t}$ 未收敛 **时**：
- $t\leftarrow t+1$
- $g_{t}\leftarrow\nabla_{\theta}f_{t}(\theta_{t-1})$ （获取时间步 t 随机目标关于 $\theta$ 的梯度）；
- $m_{t}\leftarrow\beta_{1}\cdot m_{t-1}+(1-\beta_{1})\cdot g_{t}$ （更新有偏的一阶矩估计）；
- $u_{t}\leftarrow max(\beta_{2}\cdot u_{t-1},|g_{t}|)$ （更新指数加权无穷范数）；
- $\theta_{t}\leftarrow\theta_{t-1}-(\alpha/(1-\beta_{1}^{t}))\cdot m_{t}/u_{t}$ （更新参数）；
**结束**
**返回** $\theta_{t}$ （最终参数）；

**7.2 时间平均**

由于随机近似，最后的迭代是嘈杂的，通过平均通常可以获得更好的泛化性能。此前，在 Moulines & Bach (2011) 中，Polyak-Ruppert 平均（Polyak & Juditsky, 1992; Ruppert, 1988）已被证明可以改善标准 SGD 的收敛性，其中 $\overline{\theta}_{t}=\frac{1}{t}\sum_{k=1}^{n}\theta_{k}$。

或者，可以使用参数的指数移动平均值，给更近期的参数值更高的权重。这可以通过在算法 1 和 2 的内循环中添加一行来简单实现：$\theta_{t}\leftarrow\beta_{2}\cdot\theta_{t-1}+(1-\beta_{2})\theta_{t}$，其中 $\theta_{0}=0$。初始化偏差也可以再次通过估计器 $\theta_{t}=\theta_{t}/(1-\beta_{2}^{t})$ 来校正。

**8 结论**

我们引入了一种简单且计算高效的基于梯度的随机目标函数优化算法。我们的方法旨在解决具有大型数据集和/或高维参数空间的机器学习问题。该方法结合了两种近期流行的优化方法的优点：AdaGrad 处理稀疏梯度的能力和 RMSProp 处理非平稳目标的能力。该方法易于实现且内存需求少。实验证实了其在凸问题中的收敛速度分析。总的来说，我们发现 Adam 是一种鲁棒且非常适合各种机器学习领域非凸优化问题的算法。

**9 致谢**

没有 Google Deepmind 的支持，这篇论文可能就不会存在。我们要特别感谢 Ivo Danihelka 和 Tom Schaul 提出了 Adam 这个名字。感谢杜克大学的 Kai Fan 指出了原始 AdaMax 推导中的一个错误。本文中的实验部分是在 SURF Foundation 的支持下，使用荷兰国家电子基础设施进行的。Diederik Kingma 得到了 Google 欧洲深度学习博士奖学金的支持。

**参考文献**
略