好的，请看我的翻译和解答。

***

# 深度稀疏修正神经网络

**Xavier Glorot**
蒙特利尔大学，DIRO
加拿大，魁北克省，蒙特利尔
`glorotxa@iro.umontreal.ca`

**Antoine Bordes**
Heudiasyc, UMR CNRS 6599
贡比涅技术大学，法国贡比涅
及
蒙特利尔大学，DIRO
加拿大，魁北克省，蒙特利尔
`antoine.bordes@hds.utc.fr`

**Yoshua Bengio**
蒙特利尔大学，DIRO
加拿大，魁北克省，蒙特利尔
`bengioy@iro.umontreal.ca`

### **摘要**

尽管逻辑S型神经元比双曲正切神经元更具生物学合理性，但在训练多层神经网络时，后者的效果更佳。本文表明，**修正神经元 (rectifying neurons)** 是一个更好的生物神经元模型，并且尽管其在零点存在硬非线性和不可微性，但其性能与双曲正切网络相当或更优，能够创建带有**真正零值**的**稀疏表示 (sparse representations)**，这对于天然稀疏的数据似乎非常适用。尽管它们可以利用带有额外未标记数据的半监督设置，但在拥有大型标记数据集的纯监督任务上，深度修正网络**无需任何无监督预训练**即可达到最佳性能。因此，这些结果可以被视为在理解“为何深度但纯监督的神经网络难以训练”这一问题上的新里程碑，并缩小了使用和不使用无监督预训练学习的神经网络之间的性能差距。

### **1 引言**

机器学习研究者使用的神经网络模型与计算神经科学家使用的模型之间存在许多差异。这部分是由于前者的目标是获得计算高效、能很好地泛化到新样本的学习器，而后者的目标是抽象出神经科学数据，同时获得对所涉原理的解释，为未来的生物学实验提供预测和指导。因此，两者目标重合的领域特别值得研究，这指向了大脑中受计算启发的运作原理，同时也能促进人工智能的研究。在本文中，我们展示了计算神经科学模型和机器学习神经网络模型之间的两个常见差距，可以通过使用以下**分段线性激活 (linear by part activation)**：$\max(0, x)$，称为**修正 (rectifier)**（或**铰链 (hinge)**）激活函数来弥合。实验结果将显示这种激活函数引人入胜的训练行为，特别是对于深度架构（即神经网络中隐藏层数为3或更多的情况）（参见Bengio (2009)的综述）。

最近在统计机器学习领域的理论和实证工作已经证明了深度架构学习算法的重要性。这部分受到了对哺乳动物视觉皮层的观察的启发，该皮层由一系列处理单元组成，每个处理单元都与原始视觉输入的不同表示相关联。这在灵长类动物的视觉系统中尤其明显（Serre et al., 2007），其处理阶段序列为：边缘检测、原始形状检测，然后逐步上升到更复杂的视觉形状。有趣的是，研究发现深度架构中学到的特征与在这些阶段的前两个阶段（视觉皮层的V1和V2区域）观察到的特征相似（Lee et al., 2008），并且它们在更高层中对变化因素（如相机移动）变得越来越**不变 (invariant)**（Goodfellow et al., 2009）。

关于深度网络的训练，2006年随着**深度信念网络 (Deep Belief Networks)**（Hinton et al., 2006）的引入，以及更普遍地，通过**无监督学习 (unsupervised learning)** 来初始化每一层的想法（Bengio et al., 2007; Ranzato et al., 2007），可以认为是一个突破。一些作者试图理解为什么这个无监督过程有帮助（Erhan et al., 2010），而另一些作者则研究了为什么深度神经网络的原始训练程序会失败（Bengio and Glorot, 2010）。从机器学习的角度来看，本文在这些研究方向上带来了额外的结果。

我们提议探索使用修正非线性作为深度人工神经网络中双曲正切或S型函数的替代方案，此外还在激活值上使用 $L_1$ 正则化器以促进稀疏性并防止无界激活可能带来的数值问题。Nair和Hinton (2010) 在**受限玻尔兹曼机 (Restricted Boltzmann Machines)** 的背景下，展示了这类单元与逻辑S型激活相比，在图像分类任务上的影响，并取得了有希望的结果。我们的工作将此扩展到了使用**去噪自编码器 (denoising auto-encoders)**（Vincent et al., 2008）进行预训练的情况，并在图像分类基准测试中，对修正激活函数与双曲正切函数进行了广泛的实证比较，同时还为文本应用中的情感分析提供了一个原创的推导。

我们在图像和文本数据上的实验表明，当人工神经元要么处于关闭状态，要么主要在线性区域工作时，训练进展得更好。令人惊讶的是，修正激活使得深度网络无需无监督预训练即可达到其最佳性能。因此，我们的工作为理解和缩小有无无监督预训练学习的深度网络之间的性能差距（Erhan et al., 2010; Bengio and Glorot, 2010）这一趋势提出了新的贡献。尽管如此，在有大量未标记数据可用的半监督学习背景下，修正网络仍然可以从无监督预训练中受益。此外，由于修正单元自然地导致稀疏网络，并且在其主要工作区域更接近生物神经元的响应，这项工作也在激活函数和稀疏性方面，（部分地）弥合了机器学习与神经科学之间的差距。

本文的组织结构如下。第2节介绍了一些启发这项工作的神经科学和机器学习背景。第3节介绍了修正神经元，并解释了它们在深度网络中的潜在好处和缺点。然后，我们提出了一个实证研究，包括在第4.1节的图像识别和第4.2节的情感分析上的实证结果。第5节给出了我们的结论。

### **2 背景**

#### **2.1 神经科学观察**

对于生物神经元模型，激活函数是作为突触处传入信号产生的总输入的函数的**期望发放率 (expected firing rate)**（Dayan and Abott, 2001）。当一个激活函数对强兴奋性输入模式的相反输入的响应分别是强抑制性或兴奋性的时，它分别被称为**反对称的 (antisymmetric)** 或**对称的 (symmetric)**；当这种响应为零时，则称为**单边的 (one-sided)**。我们希望考虑的计算神经科学模型和机器学习模型之间的主要差距如下：

*   关于大脑能量消耗的研究表明，神经元以一种**稀疏且分布式**的方式编码信息（Attwell and Laughlin, 2001），估计同时活跃的神经元百分比在1%到4%之间（Lennie, 2003）。这对应于表示的丰富性和小的动作电位能量消耗之间的权衡。没有额外的正则化，比如 $L_1$ 惩罚，普通的前馈神经网络不具有这个属性。例如，S型激活在 $0.5$ 附近有一个稳态区域，因此，在用小权重初始化后，所有神经元都会以其饱和区域的一半发放。这在生物学上是不合理的，并且损害了基于梯度的优化（LeCun et al., 1998; Bengio and Glorot, 2010）。

*   生物模型和机器学习模型之间的重要分歧涉及非线性激活函数。一种常见的神经元生物模型，**漏电积分-发放 (leaky integrate-and-fire, LIF)** 模型（Dayan and Abott, 2001），给出了发放率和输入电流之间的以下关系，如图1（左）所示：
    $$
    f(I) = \begin{cases} \left[\tau \log\frac{E+RI-V_r}{E+RI-V_{th}} + t_{ref}\right]^{-1}, & \text{if } E+RI > V_{th} \\ 0, & \text{if } E+RI \le V_{th} \end{cases}
    $$
    其中 $t_{ref}$ 是不应期（两次动作电位之间的最小时间），$I$ 是输入电流，$V_r$ 是静息电位，$V_{th}$ 是阈电位（且 $V_{th}>V_r$），而 $R, E, \tau$ 是膜电阻、电位和时间常数。在深度学习和神经网络文献中最常用的激活函数是标准的逻辑S型函数和双曲正切函数（见图1，右），它们在经过线性变换后是等价的。双曲正切函数在0处有一个稳态，因此从优化的角度来看是首选（LeCun et al., 1998; Bengio and Glorot, 2010），但它强制要求在0附近具有反对称性，这在生物神经元中是不存在的。

#### **2.2 稀疏性的优势**

稀疏性已成为一个备受关注的概念，不仅在计算神经科学和机器学习领域，还在统计学和信号处理领域（Candes and Tao, 2005）。它最早是在视觉系统中稀疏编码的背景下被引入计算神经科学的（Olshausen and Field, 1997）。它已成为利用一种自编码器变体（Ranzato et al., 2007, 2008; Mairal et al., 2009）的深度卷积网络的一个关键要素，该变体具有稀疏的分布式表示，并且也已成为深度信念网络的一个关键成分（Lee et al., 2008）。稀疏性惩罚已在一些计算神经科学（Olshausen and Field, 1997; Doi et al., 2006）和机器学习模型（Lee et al., 2007; Mairal et al., 2009）中使用，特别是对于深度架构（Lee et al., 2008; Ranzato et al., 2007, 2008）。然而，在后者中，神经元最终会取到小的但非零的激活或发放概率。我们在这里表明，使用修正非线性会产生真正的零激活，从而产生真正稀疏的表示。从计算的角度来看，这样的表示因以下原因而具有吸引力：

*   **信息解耦 (Information disentangling)**：深度学习算法的一个声称目标（Bengio, 2009）是解开解释数据变化的因素。一个稠密的表示是高度纠缠的，因为输入的几乎任何变化都会修改表示向量中的大多数条目。相反，如果一个表示既稀疏又对小的输入变化鲁棒，那么非零特征的集合几乎总是通过输入的微小变化而大致保持不变。

*   **高效的可变大小表示 (Efficient variable-size representation)**：不同的输入可能包含不同量的信息，并且用可变大小的数据结构来表示会更方便，这在计算机信息表示中很常见。改变活动神经元的数量允许模型控制给定输入的表示的有效维度和所需的精度。

*   **线性可分性 (Linear separability)**：稀疏表示也更可能是线性可分的，或者用更少的非线性机制更容易地分离，仅仅因为信息是在一个高维空间中表示的。此外，这可以反映原始数据的格式。例如，在文本相关的应用中，原始的原始数据已经非常稀疏（见第4.2节）。

*   **分布式但稀疏 (Distributed but sparse)**：稠密的分布式表示是最丰富的表示，可能比纯粹的局部表示（Bengio, 2009）在效率上呈指数级增长。稀疏表示的效率仍然是指数级增长的，其指数的幂是非零特征的数量。它们可能在上述标准方面代表了一个很好的权衡。

然而，强迫过多的稀疏性可能会损害同等数量神经元的预测性能，因为它降低了模型的有效容量。

<br>

**图 1：左：由生物数据驱动的常见神经激活函数。右：神经网络文献中常用的激活函数：逻辑S型函数和双曲正切函数(tanh)。**

<br>

### **3 深度修正网络**

#### **3.1 修正神经元**

神经科学文献（Bush and Sejnowski, 1995; Douglas and al., 2003）指出，皮层神经元很少处于其最大饱和状态，并表明它们的激活函数可以用一个修正器来近似。大多数先前涉及修正激活函数的神经网络研究都关注循环网络（Salinas and Abbott, 1996; Hahnloser, 1998）。

修正函数 $\text{rectifier}(x) = \max(0, x)$ 是单边的，因此不强制要求符号对称性¹或反对称性¹：相反，对兴奋性输入模式的相反输入的响应是0（无响应）。然而，我们可以通过组合共享参数的两个修正单元来获得对称性或反对称性。

**优势** 修正激活函数允许网络轻松获得稀疏表示。例如，在权重的均匀初始化之后，大约50%的隐藏单元连续输出值为实零，并且这个比例可以通过引入稀疏性的正则化轻松增加。除了更具生物学合理性之外，稀疏性还带来了数学上的优势（见前一节）。

如图2（左）所示，网络中唯一的非线性来自于与单个神经元是否激活相关的路径选择。对于给定的输入，只有一部分神经元是激活的。在这个子集上，计算是**线性的**：一旦选择了这个神经元子集，输出就是输入的线性函数（尽管足够大的变化可以触发活动神经元集合的离散变化）。因此，每个神经元或网络输出根据网络输入计算的函数是**分段线性的 (linear by parts)**。我们可以将该模型视为共享参数的指数级数量的线性模型的集合（Nair and Hinton, 2010）。由于这种线性，梯度在神经元的活动路径上流动得很好（没有由于S型或tanh单元的激活非线性导致的**梯度消失效应**），并且数学研究更容易。计算也更便宜：不需要在激活中计算指数函数，并且可以利用稀疏性。

**潜在问题** 有人可能会假设，在0处的硬饱和可能会通过阻塞梯度反向传播来损害优化。为了评估这种效应的潜在影响，我们还研究了**softplus激活**：$\text{softplus}(x) = \log(1+e^x)$（Dugas et al., 2001），这是修正非线性的一个平滑版本。我们失去了精确的稀疏性，但可能希望获得更容易的训练。然而，实验结果（见第4.1节）倾向于反驳这一假设，表明硬零实际上可以帮助监督训练。我们假设，只要梯度可以沿着某些路径传播，即每一层中的一些隐藏单元是非零的，硬非线性就不会造成损害。由于功劳被分配给了这些**开启 (ON)** 的单元，而不是更均匀地分布，我们假设优化更容易。另一个问题可能源于激活的无界行为；因此，人们可能希望使用一个正则化器来防止潜在的数值问题。因此，我们在激活值上使用 $L_1$ 惩罚，这也促进了额外的稀疏性。还要回顾一下，为了有效地表示数据中的对称/反对称行为，一个修正网络需要两倍于对称/反对称激活函数网络的隐藏单元。

最后，修正网络受到参数化的**病态条件 (ill-conditioning)** 的影响。偏置和权重可以以不同（且一致）的方式进行缩放，同时保持相同的整体网络功能。更精确地说，考虑网络深度为 $i$ 的每一层的一个标量 $\alpha_i$，并将参数缩放为 $\mathbf{W}'_i = \mathbf{W}_i / \alpha_i$ 和 $b'_i = b_i / \alpha_i$。输出单元值则按如下方式变化：$s' = s / \prod_{j=1}^l \alpha_j$。然后，变化为：$s' = s \prod_{j=1}^l \alpha_j$。因此，只要 $\prod_{j=1}^l \alpha_j$ 是1，网络函数就是相同的。

> ¹Jarrett等人(2009)使用的双曲正切绝对值非线性 $|tanh(x)|$ 强制要求符号对称性。一个 $tanh(x)$ 非线性强制要求符号反对称性。

<br>

**图 2：左：修正单元网络中激活和梯度的稀疏传播。输入选择一个活动神经元的子集，计算在这个子集中是线性的。右：修正和softplus激活函数。第二个是第一个的平滑版本。**

<br>

#### **3.2 无监督预训练**

本文特别受到在自编码器变体背景下学习到的稀疏表示的启发，因为它们被发现在训练深度架构（Bengio, 2009）中非常有用，特别是对于神经网络的无监督预训练（Erhan et al., 2010）。

尽管如此，当人们想要将修正激活引入到**堆叠去噪自编码器 (stacked denoising auto-encoders)**（Vincent et al., 2008）中时，会出现某些困难。首先，修正函数阈值以下的硬饱和不适用于重构单元。实际上，每当网络恰好在一个非零目标的位置重构一个零时，重构单元就无法反向传播任何梯度。² 其次，修正激活的无界行为也需要被考虑。在下文中，我们用 $\tilde{\mathbf{x}}$ 表示输入的损坏版本，用 $\sigma(\cdot)$ 表示逻辑S型函数，用 $\boldsymbol{\theta}$ 表示模型参数 ($\mathbf{W}_{enc}, \mathbf{b}_{enc}, \mathbf{W}_{dec}, \mathbf{b}_{dec}$)，并定义线性重构函数为：
$$
\mathbf{f}(\tilde{\mathbf{x}}, \boldsymbol{\theta}) = \mathbf{W}_{dec} \max(\mathbf{W}_{enc}\tilde{\mathbf{x}} + \mathbf{b}_{enc}, 0) + \mathbf{b}_{dec}.
$$
以下是我们实验过的几种策略：
1.  对重构层使用softplus激活函数，并配合二次代价：
    $L(\mathbf{x}, \boldsymbol{\theta}) = ||\mathbf{x} - \log(1 + \exp(\mathbf{f}(\tilde{\mathbf{x}}, \boldsymbol{\theta})))||^2$。
2.  将来自前一个编码层的修正激活值缩放到0和1之间，然后对重构层使用一个S型激活函数，并配合一个交叉熵重构代价：
    $L(\mathbf{x}, \boldsymbol{\theta}) = -\mathbf{x}\log(\sigma(\mathbf{f}(\tilde{\mathbf{x}}, \boldsymbol{\theta}))) - (1-\mathbf{x})\log(1-\sigma(\mathbf{f}(\tilde{\mathbf{x}}, \boldsymbol{\theta})))$。
3.  对重构层使用一个线性激活函数，并配合一个二次代价。我们尝试使用修正非线性之前或之后的输入单元值作为重构目标。（对于第一层，直接使用原始输入。）
4.  对重构层使用一个修正激活函数，并配合一个二次代价。

第一种策略已被证明在图像数据上能产生更好的泛化，而第二种策略在文本数据上效果更好。因此，接下来的实验研究将使用这两种策略呈现结果。

> ²为什么这对隐藏层不是问题？我们假设是因为梯度仍然可以通过活动的（非零）路径流动，这可能有助于而不是损害信用的分配。

#### **4 实验研究**

本节讨论我们对深度网络中修正单元的实证评估。我们首先将它们与双曲正切和softplus激活在有无预训练的图像基准测试上进行比较，然后将它们应用于情感分析的文本任务。

##### **4.1 图像识别**

**实验设置** 我们考虑了下面详述的图像数据集。它们每个都有一个训练集（用于调整参数），一个验证集（用于调整超参数）和一个测试集（用于报告泛化性能）。它们按照训练/验证/测试样本的数量、各自的图像大小以及类别数量来呈现：

*   **MNIST** (LeCun et al., 1998): 50k/10k/10k, $28 \times 28$ 数字图像, 10个类别。
*   **CIFAR10** (Krizhevsky and Hinton, 2009): 50k/5k/5k, $32 \times 32 \times 3$ RGB图像, 10个类别。
*   **NISTP**: 81,920k/80k/20k, $32 \times 32$ 字符图像，来自NIST数据库19，带有随机失真（Bengio and al, 2010），62个类别。这个数据集比原始的NIST（Grother, 1995）更大、更难。
*   **NORB**: 233,172/58,428/58,320, 来自Jittered-Cluttered NORB（LeCun et al., 2004）。在杂乱背景上的玩具的立体对图像，6个类别。数据已按类似于（Nair and Hinton, 2010）的方式进行预处理：我们将原始的 $2 \times 108 \times 108$ 的立体对图像下采样到 $2 \times 32 \times 32$，并在线性上将图像缩放到 $[-1, 1]$ 范围内。我们遵循Nair和Hinton (2010)使用的程序来创建验证集。

除了NORB数据（LeCun et al., 2004）外，我们使用的所有实验模型都是堆叠去噪自编码器（Vincent et al., 2008），具有三个隐藏层和每层1000个单元。Nair和Hinton (2010)的架构已在NORB上使用：两个隐藏层，分别有4000和2000个单元。我们对tanh网络使用交叉熵重构代价，对修正和softplus网络使用在softplus重构层上的二次代价。我们选择掩码噪声作为损坏过程：每个像素有0.25的概率被人工设置为0。无监督学习率是恒定的，并探索了以下值：{.1, .01, .001, .0001}。我们选择具有最低重构误差的模型。
对于监督微调，我们选择了与无监督学习率相同范围内的恒定学习率，并根据监督验证误差进行选择。训练代价是负对数似然 $-\log P(\text{correct class}|\text{input})$，其中概率从输出层（实现softmax逻辑回归）获得。我们对无监督和监督训练阶段都使用大小为10的小批量进行随机梯度下降。

考虑到修正单元不以0为中心对称的潜在问题，我们使用了一个激活函数的变体，其中一半单元的输出值乘以-1。这旨在抵消每层的平均激活值，并且可以解释为抑制性神经元，或者仅仅是一种在数值上均衡激活的方法。此外，在预训练和微调期间，对激活值施加了一个系数为0.001的 $L_1$ 惩罚，以增加学习到的表示中的稀疏量。

**主要结果** 表1总结了在每个有3个隐藏层、每层1000个隐藏单元的网络上的结果，比较了所有神经元类型³在所有数据集上，有无无监督预训练的情况。在后一种情况下，监督训练阶段使用了与上述微调相同的实验设置。我们得出的主要观察如下：

*   尽管在0处有硬阈值，用修正激活函数训练的网络可以找到与使用其平滑对应物softplus获得的局部最小值质量相当或更优的局部最小值。在NORB上，我们测试了一个由 $\text{softplus}(\alpha x)$ 定义的重缩放版本的softplus，它允许在softplus ($\alpha=1$)和修正器($\alpha=\infty$)之间以平滑的方式进行插值。我们获得了以下 $\alpha$/测试误差对：1/17.68%, 1.3/17.53%, 2/16.9%, 3/16.66%, 6/16.54%, $\infty$/16.40%。这些激活函数之间没有权衡。修正器不仅在生物学上更合理，它们在计算上也更高效。

*   使用修正激活时，与使用tanh或softplus时相比，几乎没有通过无监督预训练带来的改进。纯监督的修正网络在所有4个数据集上都保持竞争力，甚至与预训练的tanh或softplus模型相比也是如此。

*   修正网络是真正的深度稀疏网络。在MNIST上，隐藏层的平均精确稀疏度（零的比例）为83.4%，在CIFAR10上为72.0%，在NISTP上为68.0%，在NORB上为73.8%。图3更好地理解了稀疏性的影响。它显示了深度修正网络（无预训练）在MNIST上的测试误差，根据通过改变激活上的 $L_1$ 惩罚获得的不同平均稀疏度。网络似乎对此相当鲁棒，因为具有70%到几乎85%的真零的模型可以达到相似的性能。

有了标记数据，深度修正网络似乎是吸引人的模型。它们在生物学上是可信的，与它们的标准对应物相比，它们似乎不那么依赖于无监督预训练，同时最终产生稀疏的表示。
这最后一个结论与（Nair and Hinton, 2010）中报告的略有不同，后者证明了使用受限玻尔兹曼机进行无监督预训练和使用修正单元是有益的。特别是，该论文报告说，预训练的修正深度信念网络可以在NORB上达到低于16%的测试误差。然而，我们相信我们的结果与那些是兼容的：我们将实验框架扩展到一种不同类型的模型（堆叠去噪自编码器）和不同的数据集（在这些数据集上结论似乎不同）。此外，请注意，我们在NORB上未经预训练的修正模型非常有竞争力（16.4%的误差），并且优于Nair和Hinton (2010)的非预训练模型的17.6%误差，这基本上就是我们用非预训练的softplus单元得到的结果（17.68%的误差）。

**半监督设置** 图4呈现了在NORB数据集上进行的半监督实验的结果。我们改变了用于 rectifier 和 hyperbolic tangent 网络监督训练阶段的原始标记训练集的百分比，并评估了无监督预训练（使用整个训练集，未标记）的效果。证实了 Erhan et al. (2010) 的结论，带有 hyperbolic tangent 激活的网络在任何标记集大小下都通过无监督预训练得到改善（即使所有训练集都被标记）。
然而，对于修正激活，情况有所改变。在半监督设置中（标记数据很少），预训练非常有益。但标记集越大，有无预训练的模型就越接近。最终，当所有可用数据都被标记时，两种模型达到相同的性能。修正网络可以最大限度地利用标记和未标记的信息。

> ³我们还测试了LIF和$\max(\tanh(x), 0)$的重缩放版本作为激活函数。我们获得的泛化性能比表1中的差，并选择不报告它们。

<br>

**表 1：深度为3的网络的测试误差。粗体结果表示在 $p=0.05$ 的配对检验的原假设下，有无预训练的相似实验之间的统计等价性。**

| 神经元 | MNIST | CIFAR10 | NISTP | NORB |
| :--- | :--- | :--- | :--- | :--- |
| **有无监督预训练** |
| Rectifier | **1.20%** | **49.96%** | **32.86%** | **16.46%** |
| Tanh | **1.16%** | 50.79% | 35.89% | 17.66% |
| Softplus | 1.17% | 49.52% | 33.27% | 19.19% |
| **无无监督预训练** |
| Rectifier | 1.43% | **50.86%** | **32.64%** | **16.40%** |
| Tanh | 1.57% | 52.62% | 36.46% | 19.29% |
| Softplus | 1.77% | 53.20% | 35.48% | 17.68% |

**图 3：最终稀疏度对准确率的影响。** 200个随机初始化的深度修正网络在MNIST上用各种 $L_1$ 惩罚（从0到0.01）进行训练，以获得不同的稀疏水平。结果显示，强制激活的稀疏性直到大约85%的真零之前都不会损害最终性能。

**图 4：无监督预训练的效果。** 在NORB上，我们比较了有无无监督预训练的hyperbolic tangent和rectifier网络，并仅在训练集大小不断增加的子集上进行微调。

<br>

##### **4.2 情感分析**

Nair和Hinton (2010)也证明了修正单元对于与图像相关的任务是高效的。他们提到了**强度等变性 (intensity equivariance)** 属性（即没有偏置参数，网络函数对输入的强度变化是线性变化的）作为解释这一观察的论据。这表明修正激活主要对图像数据有用。在本节中，我们研究了一种不同的模态，以便为修正单元提供新的见解。
最近的一项研究（Zhou et al., 2010）表明，具有二元单元的深度信念网络在情感分析方面与最先进的方法具有竞争力。这表明深度学习适用于这个文本任务，因此似乎是观察修正单元在不同模态上行为的理想选择，并为修正网络特别适用于稀疏输入向量（如在NLP中发现的）的假设提供数据点。情感分析是一个文本挖掘领域，旨在确定作者对给定主题的判断（参见Pang and Lee, 2008的综述）。基本任务包括对评论的极性进行分类，要么预测所表达的意见是正面的还是负面的，要么给它们分配3、4或5星的星级。

遵循Snyder和Barzilay (2007)最初提出的任务，我们的数据包括从餐厅评论网站 www.opentable.com 提取的餐厅评论。我们有10,000个标记的和300,000个未标记的训练评论，而测试集包含10,000个例子。目标是预测5星级的评级，性能通过**均方根误差 (Root Mean Squared Error, RMSE)** 进行评估。⁴
图5显示了数据集的一些样本。评论文本被视为**词袋 (bag of words)** 并转换为编码术语存在/不存在的二元向量。出于计算原因，词汇表中只有5000个最频繁的术语被保留在特征集中⁵。得到的预处理数据非常稀疏：平均只有0.6%的非零特征。网络的无监督预训练同时使用标记和未标记的训练评论，而监督微调阶段则在标记的训练样本上进行10折交叉验证。

模型是堆叠的去噪自编码器，有1或3个隐藏层，每层5000个隐藏单元，使用修正或tanh激活，并以贪婪的逐层方式进行训练。预测的评级由使用多类（多项式，softmax）逻辑回归输出概率计算的期望星值定义。对于修正网络，当堆叠新层时，前一层的激活值被缩放到区间 $$ 内，并使用带有交叉熵代价的S型重构层。我们还在预训练和微调期间向代价中添加了 $L_1$ 惩罚。由于是二元输入，我们对第一层的无监督训练使用“椒盐噪声”（即，将一些输入掩蔽为零，另一些掩蔽为一）。对于更高层，使用零掩蔽（如Vincent et al., 2008）。我们根据分类性能选择了噪声水平，其他超参数根据重构误差选择。

<br>

**图 5：来自 www.opentable.com 数据集的餐厅评论示例。学习器必须预测相关的5星级评级（右列）。**

| 顾客评论 | 评级 |
| :--- | :--- |
| "Overpriced, food small portions, not well described on menu." | \* |
| "Food quality was good, but way too many flavors and textures going on in every single dish. Didn't quite all go together." | \*\* |
| "Calameri was lightly fried and not oily good job they need to learn how to make desserts better as ours was frozen." | \*\*\* |
| "The food was wonderful, the service was excellent and it was a very vibrant scene. Only complaint would be that it was a bit noisy." | \*\*\*\* |
| "We had a great time there for Mother's Day. Our server was great! Attentive, funny and really took care of us!" | \*\*\*\*\* |

**表 2：在OpenTable数据上通过10折交叉验证获得的测试RMSE和稀疏水平。**

| 网络 | RMSE | 稀疏度 |
| :--- | :--- | :--- |
| 无隐藏层 | $0.885 \pm 0.006$ | $99.4\% \pm 0.0$ |
| Rectifier (1层) | $0.807 \pm 0.004$ | $28.9\% \pm 0.2$ |
| Rectifier (3层) | $0.746 \pm 0.004$ | $53.9\% \pm 0.7$ |
| Tanh (3层) | $0.774 \pm 0.008$ | $00.0\% \pm 0.0$ |

<br>

结果显示在表2中。有趣的是，当我们向修正神经网络添加隐藏层时，RMSE显著下降。这些实验证实，在半监督设置中，修正网络在无监督预训练阶段后有所改进：没有预训练，3层模型无法获得低于0.833的RMSE。此外，尽管我们无法复制训练数据的原始非常高的稀疏度，但3层网络仍然可以达到超过50%的整体稀疏度。最后，在这种具有特殊性质（二元，高稀疏度）的数据上，具有tanh激活函数的3层网络（使用完全相同的预训练+微调设置学习）明显表现不佳。深度修正网络的稀疏行为在这种情况下似乎特别适合，因为原始输入非常稀疏，并且其非零项的数量变化很大。后者也可以通过稀疏的内部表示来实现，而不是稠密的表示。

由于在OpenTable数据上从未发表过任何结果，我们为了评估我们的网络相对于文献方法的质量，将我们的模型应用于Amazon情感分析基准测试（Blitzer et al., 2007）。该数据集提出了对4种Amazon产品的评论，必须预测其极性（正面或负面）。我们遵循了Zhou et al. (2010)定义的实验设置。在他们的论文中，最好的模型达到了73.72%的测试准确率（在4种产品上的平均值），而我们的3层修正网络获得了78.95%。

> ⁴尽管我们的任务是相同的，但我们的数据库比Snyder和Barzilay (2007)的要大得多。
> ⁵初步实验表明，更大的词汇量并没有显著改变结果。

#### **5 结论**

稀疏性和主要在线性区域工作的神经元可以在更具生物学合理性的深度神经网络中结合在一起。修正单元有助于弥合无监督预训练和无预训练之间的差距，这表明它们可能有助于在训练期间找到更好的最小值。这一发现在四个不同规模的图像分类数据集上得到了验证，尽管它们存在固有的问题，如梯度中的零，或参数化的病态条件。获得了相当稀疏的网络（对于最佳泛化模型，稀疏度从50%到80%，而大脑被假设有95%到99%的稀疏度），这可能解释了使用修正器的一些好处。

此外，修正激活函数已被证明非常适合情感分析，这是一个具有非常高数据稀疏度的基于文本的任务。这一有希望的结果倾向于表明，深度稀疏修正网络不仅对图像分类任务有益，而且可能在未来产生强大的文本挖掘工具。

***

_（参考文献部分为标准格式，故不作翻译，以保留原文准确性。）_