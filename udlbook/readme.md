## Understanding Deep Learning

pdf version: 20250529

### Content

**前言** ix

**致谢** xi

**1 导论** 1
<dd>1.1 监督学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1</dd>
<dd>1.2 无监督学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7</dd>
<dd>1.3 强化学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11</dd>
<dd>1.4 伦理道德 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12</dd>
<dd>1.5 本书结构 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15</dd>
<dd>1.6 其他相关书籍 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15</dd>
<dd>1.7 如何阅读本书 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16</dd>

**2 监督学习** 17
<dd>2.1 监督学习概述 . . . . . . . . . . . . . . . . . . . . . . . . . 17</dd>
<dd>2.2 线性回归示例 . . . . . . . . . . . . . . . . . . . . . . . . . . 18</dd>
<dd>2.3 总结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22</dd>

**3 浅层神经网络** 25
<dd>3.1 神经网络示例 . . . . . . . . . . . . . . . . . . . . . . . . . . 25</dd>
<dd>3.2 普遍近似定理 . . . . . . . . . . . . . . . . . . . . . . . . . . 29</dd>
<dd>3.3 多变量输入与输出 . . . . . . . . . . . . . . . . . . . . . . . . 30</dd>
<dd>3.4 浅层神经网络：一般情况 . . . . . . . . . . . . . . . . . . . 33</dd>
<dd>3.5 术语 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35</dd>
<dd>3.6 总结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36</dd>

**4 深度神经网络** 41
<dd>4.1 复合神经网络 . . . . . . . . . . . . . . . . . . . . . . . . . 41</dd>
<dd>4.2 从复合网络到深度网络 . . . . . . . . . . . . . . . . . . . . 43</dd>
<dd>4.3 深度神经网络 . . . . . . . . . . . . . . . . . . . . . . . . . . 45</dd>
<dd>4.4 矩阵表示法 . . . . . . . . . . . . . . . . . . . . . . . . . . . 48</dd>
<dd>4.5 浅层与深度神经网络之比较 . . . . . . . . . . . . . . . . . 49</dd>
<dd>4.6 总结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52</dd>

**5 损失函数** 56
<dd>5.1 最大似然估计 . . . . . . . . . . . . . . . . . . . . . . . . . . . 56</dd>
<dd>5.2 构建损失函数的范式 . . . . . . . . . . . . . . . . . . . . . . . 60</dd>
<dd>5.3 示例一：单变量回归 . . . . . . . . . . . . . . . . . . . . . . . 61</dd>
<dd>5.4 示例二：二元分类 . . . . . . . . . . . . . . . . . . . . . . . 64</dd>
<dd>5.5 示例三：多类别分类 . . . . . . . . . . . . . . . . . . . . . . . 67</dd>
<dd>5.6 多输出问题 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69</dd>
<dd>5.7 交叉熵损失 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71</dd>
<dd>5.8 总结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72</dd>

**6 模型拟合** 77
<dd>6.1 梯度下降 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77</dd>
<dd>6.2 随机梯度下降 . . . . . . . . . . . . . . . . . . . . . . . . . . 83</dd>
<dd>6.3 动量法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86</dd>
<dd>6.4 Adam 算法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88</dd>
<dd>6.5 训练算法的超参数 . . . . . . . . . . . . . . . . . . . . . . . 91</dd>
<dd>6.6 总结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91</dd>

**7 梯度与初始化** 96
<dd>7.1 问题定义 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96</dd>
<dd>7.2 导数计算 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97</dd>
<dd>7.3 简易示例 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100</dd>
<dd>7.4 反向传播算法 . . . . . . . . . . . . . . . . . . . . . . . . . . 103</dd>
<dd>7.5 参数初始化 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107</dd>
<dd>7.6 训练代码示例 . . . . . . . . . . . . . . . . . . . . . . . . . . . 111</dd>
<dd>7.7 总结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111</dd>

**8 性能评估** 118
<dd>8.1 训练一个简单模型 . . . . . . . . . . . . . . . . . . . . . . . . 118</dd>
<dd>8.2 误差的来源 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120</dd>
<dd>8.3 减小误差 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124</dd>
<dd>8.4 双重下降现象 . . . . . . . . . . . . . . . . . . . . . . . . . . 127</dd>
<dd>8.5 超参数选择 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132</dd>
<dd>8.6 总结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133</dd>

**9 正则化** 138
<dd>9.1 显式正则化 . . . . . . . . . . . . . . . . . . . . . . . . . . . 138</dd>
<dd>9.2 隐式正则化 . . . . . . . . . . . . . . . . . . . . . . . . . . . 141</dd>
<dd>9.3 提升性能的启发式方法 . . . . . . . . . . . . . . . . . . . . . 143</dd>
<dd>9.4 总结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154</dd>

**10 卷积网络** 161
<dd>10.1 不变性与等变性 . . . . . . . . . . . . . . . . . . . . . . . . . 161</dd>
<dd>10.2 一维输入的卷积网络 . . . . . . . . . . . . . . . . . . . . . . 163</dd>
<dd>10.3 二维输入的卷积网络 . . . . . . . . . . . . . . . . . . . . . . 170</dd>
<dd>10.4 下采样与上采样 . . . . . . . . . . . . . . . . . . . . . . . . . 171</dd>
<dd>10.5 应用 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174</dd>
<dd>10.6 总结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179</dd>

**11 残差网络** 186
<dd>11.1 序列化处理 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186</dd>
<dd>11.2 残差连接与残差块 . . . . . . . . . . . . . . . . . . . . . . . 189</dd>
<dd>11.3 残差网络中的梯度爆炸问题 . . . . . . . . . . . . . . . . . . 192</dd>
<dd>11.4 批量归一化 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192</dd>
<dd>11.5 常见的残差架构 . . . . . . . . . . . . . . . . . . . . . . . . . 195</dd>
<dd>11.6 为何带有残差连接的网络表现如此优异？. . . . . . . . . . . . . . . 199</dd>
<dd>11.7 总结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199</dd>

**12 Transformer** 207
<dd>12.1 处理文本数据 . . . . . . . . . . . . . . . . . . . . . . . . . . . 207</dd>
<dd>12.2 点积自注意力机制 . . . . . . . . . . . . . . . . . . . . . . . . 208</dd>
<dd>12.3 点积自注意力机制的扩展 . . . . . . . . . . . . . . . . . . . . . . 213</dd>
<dd>12.4 Transformer 层 . . . . . . . . . . . . . . . . . . . . . . . . . . . 215</dd>
<dd>12.5 Transformer 在自然语言处理中的应用 . . . . . . . . . . . . . . . 216</dd>
<dd>12.6 编码器模型示例：BERT . . . . . . . . . . . . . . . . . . . . . . 219</dd>
<dd>12.7 解码器模型示例：GPT-3 . . . . . . . . . . . . . . . . . . . . . . 222</dd>
<dd>12.8 编码器-解码器模型示例：机器翻译 . . . . . . . . . . . . . . . . 226</dd>
<dd>12.9 用于长序列的 Transformer . . . . . . . . . . . . . . . . . . . . . . 227</dd>
<dd>12.10 用于图像的 Transformer . . . . . . . . . . . . . . . . . . . . . . 228</dd>
<dd>12.11 总结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232</dd>

**13 图神经网络** 240
<dd>13.1 什么是图？ . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240</dd>
<dd>13.2 图的表示 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243</dd>
<dd>13.3 图神经网络、任务与损失函数 . . . . . . . . . . . . . . . . . . 245</dd>
<dd>13.4 图卷积网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248</dd>
<dd>13.5 示例：图分类 . . . . . . . . . . . . . . . . . . . . . . . . . . 251</dd>
<dd>13.6 归纳式与直推式模型 . . . . . . . . . . . . . . . . . . . . . . 252</dd>
<dd>13.7 示例：节点分类 . . . . . . . . . . . . . . . . . . . . . . . . . 253</dd>
<dd>13.8 图卷积网络的层级结构 . . . . . . . . . . . . . . . . . . . . . . 256</dd>
<dd>13.9 边图 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 260</dd>
<dd>13.10 总结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261</dd>

**14 无监督学习** 269
<dd>14.1 无监督学习模型的分类 . . . . . . . . . . . . . . . . . . . . . 269</dd>
<dd>14.2 何为好的生成模型？ . . . . . . . . . . . . . . . . . . . . . 270</dd>
<dd>14.3 性能量化 . . . . . . . . . . . . . . . . . . . . . . . . . . . 272</dd>
<dd>14.4 总结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274</dd>

**15 生成对抗网络** 276
<dd>15.1 作为信号的判别过程 . . . . . . . . . . . . . . . . . . . . . . . 276</dd>
<dd>15.2 提升稳定性 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281</dd>
<dd>15.3 渐进式生长、小批量判别与截断技巧 . . . . . . . . . . . . . . . . 287</dd>
<dd>15.4 条件生成 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289</dd>
<dd>15.5 图像翻译 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 291</dd>
<dd>15.6 StyleGAN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296</dd>
<dd>15.7 总结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298</dd>

**16 标准化流** 304
<dd>16.1 一维示例 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304</dd>
<dd>16.2 一般情况 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307</dd>
<dd>16.3 可逆网络层 . . . . . . . . . . . . . . . . . . . . . . . . . . . 309</dd>
<dd>16.4 多尺度流 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317</dd>
<dd>16.5 应用 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 318</dd>
<dd>16.6 总结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321</dd>

**17 变分自编码器** 327
<dd>17.1 隐变量模型 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327</dd>
<dd>17.2 非线性隐变量模型 . . . . . . . . . . . . . . . . . . . . . . . . 328</dd>
<dd>17.3 训练 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 331</dd>
<dd>17.4 ELBO 的性质 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 334</dd>
<dd>17.5 变分近似 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336</dd>
<dd>17.6 变分自编码器 . . . . . . . . . . . . . . . . . . . . . . . . . . . 336</dd>
<dd>17.7 重参数化技巧 . . . . . . . . . . . . . . . . . . . . . . . . . . . 339</dd>
<dd>17.8 应用 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 340</dd>
<dd>17.9 总结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 343</dd>

**18 扩散模型** 349
<dd>18.1 概述 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349</dd>
<dd>18.2 编码器（前向过程） . . . . . . . . . . . . . . . . . . . . . . . . 350</dd>
<dd>18.3 解码器模型（逆向过程） . . . . . . . . . . . . . . . . . . . . . . 356</dd>
<dd>18.4 训练 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357</dd>
<dd>18.5 损失函数的重参数化 . . . . . . . . . . . . . . . . . . . . . . . 361</dd>
<dd>18.6 实现 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 363</dd>
<dd>18.7 总结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 368</dd>

**19 强化学习** 374
<dd>19.1 马尔可夫决策过程、回报与策略 . . . . . . . . . . . . . . . . . 374</dd>
<dd>19.2 期望回报 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 378</dd>
<dd>19.3 表格型强化学习 . . . . . . . . . . . . . . . . . . . . . . . . . 382</dd>
<dd>19.4 拟合Q学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . 386</dd>
<dd>19.5 策略梯度方法 . . . . . . . . . . . . . . . . . . . . . . . . . . 389</dd>
<dd>19.6 行动者-评论家方法 . . . . . . . . . . . . . . . . . . . . . . . . 394</dd>
<dd>19.7 离线强化学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . 395</dd>
<dd>19.8 总结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 396</dd>

**20 深度学习为何有效？** 402
<dd>20.1 对深度学习的质疑 . . . . . . . . . . . . . . . . . . . . . . . . 402</dd>
<dd>20.2 影响拟合性能的因素 . . . . . . . . . . . . . . . . . . . . . . 403</dd>
<dd>20.3 损失函数的性质 . . . . . . . . . . . . . . . . . . . . . . . . . 407</dd>
<dd>20.4 决定泛化能力的因素 . . . . . . . . . . . . . . . . . . . . . . . 411</dd>
<dd>20.5 我们真的需要如此多的参数吗？ . . . . . . . . . . . . . . . . . . . . 415</dd>
<dd>20.6 网络必须是“深”的吗？ . . . . . . . . . . . . . . . . . . . . . . . 418</dd>
<dd>20.7 总结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 419</dd>

**21 深度学习与伦理** 421
<dd>21.1 价值对齐 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 421</dd>
<dd>21.2 蓄意滥用 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 427</dd>
<dd>21.3 其他社会、伦理与职业问题 . . . . . . . . . . . . . . . . . . . 429</dd>
<dd>21.4 案例研究 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 431</dd>
<dd>21.5 科学的价值无涉理想 . . . . . . . . . . . . . . . . . . . . . . . 432</dd>
<dd>21.6 作为集体行动问题的负责任AI研究 . . . . . . . . . . . . . . . 433</dd>
<dd>21.7 前进之路 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 434</dd>
<dd>21.8 总结 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 435</dd>

**附录 A 符号** 437

**附录 B 数学基础** 440
<dd>B.1 函数 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 440</dd>
<dd>B.2 二项式系数 . . . . . . . . . . . . . . . . . . . . . . . . . . . 442</dd>
<dd>B.3 向量、矩阵与张量 . . . . . . . . . . . . . . . . . . . . . . . . 443</dd>
<dd>B.4 特殊类型的矩阵 . . . . . . . . . . . . . . . . . . . . . . . . . 446</dd>
<dd>B.5 矩阵微积分 . . . . . . . . . . . . . . . . . . . . . . . . . . 448</dd>

**附录 C 概率论** 449
<dd>C.1 随机变量与概率分布 . . . . . . . . . . . . . . . . . . . . 449</dd>
<dd>C.2 期望 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 453</dd>
<dd>C.3 正态概率分布 . . . . . . . . . . . . . . . . . . . . . . . . 457</dd>
<dd>C.4 采样 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 460</dd>
<dd>C.5 概率分布之间的距离 . . . . . . . . . . . . . . . . . . . . 460</dd>

**参考文献** 463

**索引** 513

## reading

### 第1-4章：基础构件 (Introduction, Supervised Learning, Shallow/Deep Networks)

*   **与AI前沿的关系与重要性**：
    **（重要性：★★★★★ - 奠基石）**
    这几章是整个深度学习大厦的地基。虽然“浅层网络”本身已不是前沿，但它们所阐述的**核心概念**——**参数、损失函数、训练、泛化、过参数化、分段线性本质**——至今仍然是理解和诊断所有SOTA（State-of-the-Art）模型的基础。没有这些基础，后续所有复杂的架构都无从谈起。特别是第四章对深度网络“折叠空间”和“增加线性区域”的直观解释，对于理解深度为何有效至关重要。

*   **注释中值得阅读的论文分析**：
    *   `Minsky & Papert (1969)` (Perceptrons): 这篇论文在历史上曾一度“终结”了神经网络研究，了解它能让你理解为什么**非线性激活函数**和**多层结构**是神经网络的生命线。
    *   `Cybenko (1989)` / `Hornik (1991)` (Universal approximation theorem): 这为神经网络为何能拟合任意函数提供了理论基石，是理解其强大表达能力的源头。
    *   `Glorot et al. (2011)` (Deep sparse rectifier neural networks): 重新推广了**ReLU激活函数**，是现代深度学习成功的关键里程碑之一，解释了如何缓解梯度消失问题。

*   **必读论文清单**：
    1.  **Glorot, X., Bordes, A., & Bengio, Y. (2011). Deep sparse rectifier neural networks.** (理解ReLU为何有效)

---

### 第5-9章：训练与优化 (Loss, Fitting, Gradients, Performance, Regularization)

*   **与AI前沿的关系与重要性**：
    **（重要性：★★★★★ - 引擎与调校手册）**
    这部分内容是AI的“引擎室”和“性能调校手册”。无论模型架构如何演进，它们最终都必须通过一个**损失函数**和**优化算法**来训练。
    *   **最大似然范式**（第5章）是所有标准损失函数（MSE, Cross-entropy）的理论源头。
    *   **Adam/AdamW**（第6章）至今仍是训练几乎所有大模型的**默认优化器**。
    *   **反向传播**（第7章）是驱动一切学习的核心算法。
    *   **欠拟合/过拟合、双下降**（第8章）是诊断和理解模型性能的核心理论框架。
    *   **正则化**（第9章）的各种技术，特别是**隐式正则化**，是解释为何过参数化的大模型能够成功泛化的前沿理论焦点。

*   **注释中值得阅读的论文分析**：
    *   `Kingma & Ba (2015)` (Adam): 提出了Adam优化器，是深度学习领域被引用次数最多的论文之一，**必读中的必读**。
    *   `Ioffe & Szegedy (2015)` (Batch Normalization): 提出了批归一化，是稳定和加速深度网络训练的革命性技术。
    *   `Zhang et al. (2017a)` (Rethinking Generalization): 一篇里程碑式的实验论文，通过拟合随机数据，挑战了传统统计学习理论，开启了对深度学习泛化能力的新一轮思考。
    *   `He et al. (2019)` (Bag of Tricks for Image Classification): 虽然讲的是CNN，但其对学习率、权重衰减等超参数的深入分析，对理解LLM的训练同样具有指导意义。

*   **必读论文清单**：
    1.  **Kingma, D. P., & Ba, J. (2015). Adam: A method for stochastic optimization.** (理解现代优化器的核心)
    2.  **Ioffe, S., & Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift.** (理解现代网络为何能堆深)
    3.  **Zhang, C., Bengio, S., Hardt, M., Recht, B., & Vinyals, O. (2017a). Understanding deep learning requires rethinking generalization.** (理解现代泛化理论的起点)

---

### 第10-11章：卷积与残差网络 (CNNs, ResNets)

*   **与AI前沿的关系与重要性**：
    **（重要性：★★★★☆ - 视觉基石与架构思想源泉）**
    虽然在很多前沿领域（特别是NLP和部分CV任务）Transformer已经成为主导，但CNNs仍然是**计算机视觉领域的基石**，并且其核心思想——**局部性、参数共享、层次化特征**——具有永恒的价值。
    **残差网络 (ResNet)** 的重要性则超越了CNN本身。它提出的**残差连接 (跳跃连接)** 是一种普适的、极其强大的架构思想，它解决了深度网络训练中的梯度消失和模型退化问题，使得训练**成百上千层**的网络成为可能。这一思想在后来的Transformer、U-Net等架构中被广泛继承和发扬。

*   **注释中值得阅读的论文分析**：
    *   `Krizhevsky et al. (2012)` (AlexNet): 开启了深度学习现代革命的论文，历史地位极高。
    *   `He et al. (2016a)` (ResNet): 提出了残差连接，**必读中的必读**，其思想是理解现代深度架构的关键。
    *   `Huang et al. (2017b)` (DenseNet): 提出了另一种极致利用特征的连接方式，有助于拓宽对网络设计的思考。
    *   `Ronneberger et al. (2015)` (U-Net): 提出了经典的编码器-解码器+跳跃连接结构，在医学图像分割等领域至今仍是黄金标准，其思想也被扩散模型等生成模型广泛借鉴。

*   **必读论文清单**：
    1.  **He, K., Zhang, X., Ren, S., & Sun, J. (2016a). Deep residual learning for image recognition.** (理解“跳跃连接”为何能让网络变得非常深)

---

### 第12章：Transformer

*   **与AI前沿的关系与重要性**：
    **（重要性：★★★★★★ - 当前时代的绝对核心）**
    **这一章是全书与当前AI前沿联系最紧密、最核心的部分。** Transformer架构不仅统一了NLP，催生了ChatGPT等大语言模型，还在计算机视觉（ViT）、蛋白质折叠（AlphaFold2）、语音识别等领域取得了SOTA成果。可以说，不理解Transformer，就无法理解今天的AI。自注意力、多头注意力、位置编码、编码器-解码器等概念，是每个AI从业者都必须掌握的核心知识。

*   **注释中值得阅读的论文分析**：
    *   `Vaswani et al. (2017)` (Attention Is All You Need): **本书乃至整个领域最重要的论文之一，必须逐字逐句精读。** 它是所有现代大模型的技术源头。
    *   `Devlin et al. (2019)` (BERT): 提出了双向编码器表示，开创了基于大规模无监督预训练+微调的NLP新范式，影响力巨大。
    *   `Brown et al. (2020)` (GPT-3): 展示了超大规模解码器模型惊人的“少样本学习 (few-shot learning)”能力，直接引爆了当前的大模型浪潮。
    *   `Dosovitskiy et al. (2021)` (ViT): 成功地将Transformer架构应用于视觉领域，证明了其通用性，开启了CV领域架构变革的序幕。

*   **必读论文清单**：
    1.  **Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need.** (圣经)
    2.  **Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). Bert: Pre-training of deep bidirectional transformers for language understanding.** (理解预训练范式和编码器模型)
    3.  **Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., ... & Amodei, D. (2020). Language models are few-shot learners.** (理解大模型能力的涌现)

---

### 第13-21章：高级主题 (GNNs, Generative Models, RL, Ethics)

*   **与AI前沿的关系与重要性**：
    **（重要性：★★★★★ - 前沿拓展与未来方向）**
    这部分内容代表了深度学习在核心模型之外的**广阔应用和前沿探索**。
    *   **GNNs**：在处理关系数据（如社交网络、分子、知识图谱）方面是不可替代的工具。
    *   **生成模型（GANs, VAEs, Flows, Diffusion Models）**：是理解和创造数据的核心技术。**扩散模型**更是当前图像、视频、音频生成领域的绝对王者（如Sora, Midjourney）。
    *   **RL**：如前所述，是实现AI“对齐”、让模型与人类价值观协同的关键。
    *   **伦理**：随着AI能力越来越强，伦理问题已从边缘讨论，成为所有顶级AI实验室和公司**必须严肃对待的核心问题**。

*   **注释中值得阅读的论文分析**：
    *   `Goodfellow et al. (2014)` (GAN): 开创了生成对抗网络这一强大的生成模型范式。
    *   `Kingma & Welling (2014)` (VAE): 提出了变分自编码器，是深度概率生成模型的基石。
    *   `Ho et al. (2020)` (DDPM): 提出了去噪扩散概率模型，直接引爆了当前的扩散模型热潮，**必读**。
    *   `Ouyang et al. (2022)` (InstructGPT): 详细阐述了如何使用**RLHF**来训练ChatGPT的前身，是理解大模型对齐的**必读**论文。
    *   `Karras et al. (2019)` (StyleGAN): GAN领域的里程碑，其对生成过程的精巧控制至今仍有借鉴意义。

*   **必读论文清单**：
    1.  **Ho, J., Jain, A., & Abbeel, P. (2020). Denoising diffusion probabilistic models.** (理解现代视觉生成的核心)
    2.  **Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., ... & Lowe, R. (2022). Training language models to follow instructions with human feedback.** (理解RLHF和LLM对齐)

---
### 总结与最终建议

您的学习方式非常扎实。这本书为您提供了一个无与伦比的理论框架。我的建议是：

**以这本书的结构为骨架，将上面列出的“必读论文”作为血肉，进行主题式学习。**

*   **完成一个章节的阅读 -> 精读该主题下1-2篇必读论文 -> （如果可能）跑一下相关的开源代码。**

通过这种方式，您不仅能建立起系统、连贯的知识体系，还能深入到每个关键思想的源头，理解其来龙去脉，并紧跟当前AI前沿的脉搏。这将使您在NLP乃至整个AI领域的理解深度，远远超越只停留在表面概念的学习者。

## log
25/02: Solutions Done!

25/09: 重读一遍，使用 Gemini 2.5 Pro 制作中译文，Prompt 为：
```
你是母语为中英文双语的人工智能教授，作家和编辑，将我发给你的内容翻译为中文，并为习题作答；
译文要求保持专业，要做到信达雅，注意语境，表达地道，语义直白，优化数学表达使得更专业；
输出优美格式，注意不要遗漏正文内容，可以忽略文档中的页眉页脚等无关信息；
保留图片注释，忽略图片本身，无需识别图片内容，但图片注释要放在最合适位置，不一定在原文安排的位置；
公式准确规范，尽可能保留原样，如果公式超出常规页面宽度，则要换行但需注意对齐；
章节后面的习题，需要深入思考，然后给出准确又简明的思路与解答，不要长篇大论，但格式要优美。
其他细节：输出 url 后留一个空格，防止后文被识别为链接的一部分；行内公式与符号也要使用标准 Latex 格式，要尽可能的使用；侧边栏的内容放在对应句子的末尾，并使用“参考：”作为前缀表明；
最后还要优化行文使得自然流畅，宁可多用笔墨也要表达清楚，为了连贯通畅可以做必要的润色不必严格对比原文，译文要看起来像人翻译的。
```
**注意：习题部分由 LLM 生成，错误率高，仅供参考**


