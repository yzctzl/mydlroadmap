好的，教授。我将为您翻译这份经典的自然语言处理论文。

***

# GloVe：用于词语表示的全局向量

**作者：Jeffrey Pennington, Richard Socher, Christopher D. Manning**
斯坦福大学计算机科学系，斯坦福，加利福尼亚州 94305
`jpennin@stanford.edu`, `richard@socher.org`, `manning@stanford.edu`

### 摘要

近年来，学习词语的向量空间表示（即“词向量”）的方法，在通过向量算术捕捉细粒度的语义和句法规律方面取得了巨大成功。然而，这些规律是如何在词向量中产生的，其根源至今仍不甚明朗。在本文中，我们分析并明确了模型需要具备何种特性，才能让这些规律在词向量中涌现。我们的研究最终催生出一种新的模型，我们称之为全局对数双线性回归模型（global log-bilinear regression model）。该模型融合了文献中两大主流模型家族的优点：全局矩阵分解（global matrix factorization）和局部上下文窗口（local context window）方法。

我们的模型通过仅在词-词共现矩阵中的非零元素上进行训练，从而高效地利用了语料库的统计信息，避免了在整个稀疏矩阵或大型语料库的单个上下文窗口上进行训练的低效。该模型最终生成的词向量空间具有富有意义的子结构，这体现在其于近期的词语类比（word analogy）任务上取得了 75% 的准确率。此外，在词语相似度（similarity）任务和命名实体识别（named entity recognition）任务中，它的表现也超越了其他相关模型。

### 1. 引言

语义向量空间模型用一个实数向量来表示每个词语。这些向量可作为特征被应用于多种任务，例如信息检索 (Manning et al., 2008)、文档分类 (Sebastiani, 2002)、问答系统 (Tellex et al., 2003)、命名实体识别 (Turian et al., 2010) 以及句法分析 (Socher et al., 2013)。

大多数词向量方法主要依赖于词向量对之间的距离或夹角来评估其内在质量。近期，Mikolov 等人 (2013c) 引入了一种基于词语类比的新评估方案，该方案旨在探索词向量空间中更精细的结构。它不再仅仅关注词向量之间的标量距离，而是审视它们在不同维度上的差异。例如，类比关系“国王”之于“女王”犹如“男人”之于“女人”，这层关系应当通过向量空间中的一个方程来编码，即：$king - queen = man - woman$。这种评估方案偏爱那些能产生蕴含意义的维度的模型，从而捕捉到分布式表示中的多重聚类思想 (Bengio, 2009)。

目前，用于学习词向量的主流模型家族主要有两个：1) 全局矩阵分解方法，如潜在语义分析 (Latent Semantic Analysis, LSA) (Deerwester et al., 1990)；2) 局部上下文窗口方法，如 Mikolov 等人 (2013c) 提出的 skip-gram 模型。当前，这两个家族都存在显著的缺陷。像 LSA 这样的方法虽然能高效地利用统计信息，但在词语类比任务上表现相对较差，这表明其向量空间结构并非最优。而像 skip-gram 这样的方法虽然在类比任务上可能表现更佳，但它们对语料库统计信息的利用却很不够，因为它们是在独立的局部上下文窗口上进行训练，而不是在全局的共现计数上。

在这项工作中，我们分析了要产生蕴含意义的线性方向所必需的模型属性，并论证了全局对数双线性回归模型是实现这一目标的合适选择。我们提出了一种特定的加权最小二乘模型，该模型直接在全局的词-词共现计数上进行训练，从而高效地利用了统计信息。该模型产生的词向量空间具有富有意义的子结构，这一点从它在词语类比数据集上达到当前最佳的 75% 准确率就得以证明。我们还展示了我们的方法在多个词语相似度任务以及一个常见的命名实体识别（NER）基准测试中，其性能均优于其他现有方法。

我们提供了该模型的源代码以及训练好的词向量，地址为：http://nlp.stanford.edu/projects/glove/ 。

### 2. 相关工作

**矩阵分解方法 (Matrix Factorization Methods)**。利用矩阵分解生成低维词向量的方法，其历史可以追溯到潜在语义分析（LSA）。这类方法利用低秩近似（low-rank approximation）来分解大型矩阵，这些矩阵捕捉了关于语料库的统计信息。矩阵所捕捉的具体信息类型因应用而异。在 LSA 中，矩阵是“词项-文档”类型，即行对应词语或词项，列对应语料库中的不同文档。相比之下，以“语言的超空间类比”（Hyperspace Analogue to Language, HAL）模型 (Lund and Burgess, 1996) 为例，它利用的是“词项-词项”类型的矩阵，即行和列都对应词语，矩阵中的元素则对应一个词在另一个词的上下文中出现的次数。

HAL 及其相关方法的一个主要问题是，最高频的词语对相似度度量的贡献过大。例如，两个词与 `the` 或 `and` 共同出现的次数会对它们的相似度产生巨大影响，尽管这几乎不能反映它们的语义关联性。为了解决 HAL 的这一缺陷，出现了一些技术，例如 COALS 方法 (Rohde et al., 2006)。在该方法中，共现矩阵首先会通过基于熵或相关的归一化方法进行转换。这种转换的一个优点是，原始的共现计数（在一个规模适中的语料库中，其数值范围可能跨越 8 到 9 个数量级）被压缩，从而在一个较小的区间内更均匀地分布。许多较新的模型也沿用了这一思路，其中一项研究 (Bullinaria and Levy, 2007) 指出，正点互信息（Positive Pointwise Mutual Information, PPMI）是一种很好的转换方式。最近，一种形如 Hellinger PCA (HPCA) 的平方根类型变换 (Lebret and Collobert, 2014) 被认为是学习词向量的一种有效方法。

**基于浅层窗口的方法 (Shallow Window-Based Methods)**。另一类方法是通过在局部上下文窗口内进行预测来学习词向量。例如，Bengio 等人 (2003) 引入了一个模型，它在一个用于语言建模的简单神经网络架构中学习词向量表示。Collobert 和 Weston (2008) 将词向量的训练与下游任务的训练目标解耦，这为 Collobert 等人 (2011) 使用一个词的完整上下文（而不仅仅是像语言模型中那样只使用前面的上下文）来学习词向量铺平了道路。

近年来，完整的神经网络结构对于学习有用的词向量表示的重要性受到了质疑。Mikolov 等人 (2013a) 提出的 skip-gram 和连续词袋（Continuous Bag-of-Words, CBOW）模型，提出了一种基于两个词向量内积的简单单层架构。Mnih 和 Kavukcuoglu (2013) 也提出了与之密切相关的向量对数双线性模型 vLBL 和 ivLBL。Levy 等人 (2014) 则提出了一种基于 PPMI 指标的显式词嵌入方法。

在 skip-gram 和 ivLBL 模型中，目标是根据一个词本身来预测其上下文；而在 CBOW 和 vLBL 模型中，目标则是根据上下文来预测一个词。通过在词语类比任务上的评估，这些模型展示了它们能够将语言模式学习为词向量之间的线性关系。

与矩阵分解方法不同，基于浅层窗口的方法有一个缺点，即它们不直接作用于语料库的共现统计数据。相反，这些模型在整个语料库中扫描上下文窗口，这未能充分利用数据中存在的大量重复信息。

### 3. GloVe 模型

语料库中词语出现的统计数据，是所有无监督词向量学习方法可用的主要信息来源。尽管现在已经存在许多这类方法，但一个问题仍然悬而未决：意义是如何从这些统计数据中产生的？以及最终得到的词向量又可能如何表示这些意义？在本节中，我们将对这个问题进行一些阐释。我们利用我们的洞见构建了一个新的词向量表示模型，我们称之为 GloVe，即“全局向量”（Global Vectors）的缩写，因为全局的语料库统计信息被模型直接捕捉。

首先，我们建立一些符号。设词-词共现计数的矩阵为 $X$，其元素 $X_{ij}$ 表示词语 $j$ 出现在词语 $i$ 上下文中的次数。设 $X_i = \sum_k X_{ik}$ 为任意词语出现在词语 $i$ 上下文中的总次数。最后，设 $P_{ij} = P(j|i) = X_{ij}/X_i$ 为词语 $j$ 出现在词语 $i$ 上下文中的概率。

我们从一个简单的例子开始，展示如何从共现概率中直接提取出意义的某些方面。考虑两个词 $i$ 和 $j$，它们展现出我们感兴趣的某个特定方面；为具体起见，假设我们对热力学相态（thermodynamic phase）这个概念感兴趣，为此我们可以选择 $i = \text{ice}$（冰）和 $j = \text{steam}$（蒸汽）。这两个词的关系可以通过研究它们与各种探针词（probe words）$k$ 的共现概率之比来考察。对于与 `ice` 相关但与 `steam` 无关的词 $k$，比如说 $k = \text{solid}$（固体），我们期望比率 $P_{ik}/P_{jk}$ 会很大。同样地，对于与 `steam` 相关但与 `ice` 无关的词 $k$，比如说 $k = \text{gas}$（气体），这个比率应该很小。而对于像 $water$（水）或 $fashion$（时尚）这样既与 `ice` 和 `steam` 都相关，或都无关的词 $k$，这个比率应该接近于 1。表 1 展示了一个大型语料库中这些概率及其比率，这些数据证实了我们的预期。与原始概率相比，这个比率更能区分相关的词（`solid` 和 `gas`）与不相关的词（`water` 和 `fashion`），并且也能更好地区分这两个相关的词。

---
**表 1**: 在一个 60 亿词元的语料库中，目标词 `ice` 和 `steam` 与选定上下文词的共现概率。只有在比率中，来自非区分性词（如 `water` 和 `fashion`）的噪声才会被抵消掉，因此较大的值（远大于1）与 `ice` 的特有属性高度相关，而较小的值（远小于1）则与 `steam` 的特有属性高度相关。

| 概率和比率 | $k = \text{solid}$ | $k = \text{gas}$ | $k = \text{water}$ | $k = \text{fashion}$ |
| :--- | :--- | :--- | :--- | :--- |
| $P(k \| \text{ice})$ | $1.9 \times 10^{-4}$ | $6.6 \times 10^{-5}$ | $3.0 \times 10^{-3}$ | $1.7 \times 10^{-5}$ |
| $P(k \| \text{steam})$ | $2.2 \times 10^{-5}$ | $7.8 \times 10^{-4}$ | $2.2 \times 10^{-3}$ | $1.8 \times 10^{-5}$ |
| $P(k \| \text{ice}) / P(k \| \text{steam})$ | 8.9 | $8.5 \times 10^{-2}$ | 1.36 | 0.96 |

---

上述论证表明，学习词向量的合适起点应该是共现概率的比率，而不是概率本身。注意到比率 $P_{ik}/P_{jk}$ 依赖于三个词 $i, j, k$，因此最通用的模型形式如下：

$$
F(w_i, w_j, \tilde{w}_k) = \frac{P_{ik}}{P_{jk}}
\tag{1}
$$

其中 $w \in \mathbb{R}^d$ 是词向量，而 $\tilde{w} \in \mathbb{R}^d$ 是独立的上下文词向量，其作用将在 4.2 节中讨论。在这个方程中，右侧是从语料库中提取的，而 $F$ 可能依赖于一些尚未指定的参数。$F$ 的可能性是无穷的，但通过强制执行一些我们期望的特性（desiderata），我们可以筛选出一个唯一的选择。首先，我们希望 $F$ 能够将比率 $P_{ik}/P_{jk}$ 中蕴含的信息编码到词向量空间中。由于向量空间本质上是线性结构，最自然的方式就是通过向量差来实现。基于此，我们可以将考虑范围限制在那些仅依赖于两个目标词之差的函数 $F$ 上，从而将公式 (1) 修改为：

$$
F(w_i - w_j, \tilde{w}_k) = \frac{P_{ik}}{P_{jk}}
\tag{2}
$$

接下来，我们注意到公式 (2) 中 $F$ 的参数是向量，而右侧是标量。虽然 $F$ 可以是一个复杂的函数（例如，由一个神经网络参数化），但这样做会模糊我们试图捕捉的线性结构。为了避免这个问题，我们可以先对参数取点积（dot product）：

$$
F((w_i - w_j)^T \tilde{w}_k) = \frac{P_{ik}}{P_{jk}}
\tag{3}
$$

这可以防止 $F$ 以不希望的方式混合向量的各个维度。接下来，注意到对于词-词共现矩阵，一个词和上下文词之间的区别是任意的，我们可以自由地交换这两个角色。为了保持一致性，我们不仅需要交换 $w \leftrightarrow \tilde{w}$，还需要交换 $X \leftrightarrow X^T$。我们最终的模型应该在这种重新标记下保持不变，但公式 (3) 并非如此。然而，这种对称性可以通过两个步骤来恢复。首先，我们要求 $F$ 是群 $(\mathbb{R}, +)$ 和 $(\mathbb{R}_{>0}, \times)$ 之间的同态（homomorphism），即：

$$
F((w_i - w_j)^T \tilde{w}_k) = \frac{F(w_i^T \tilde{w}_k)}{F(w_j^T \tilde{w}_k)}
\tag{4}
$$

根据公式 (3)，上式的解为：

$$
F(w_i^T \tilde{w}_k) = P_{ik} = \frac{X_{ik}}{X_i}
\tag{5}
$$

公式 (4) 的解是 $F = \exp$，即：

$$
w_i^T \tilde{w}_k = \log(P_{ik}) = \log(X_{ik}) - \log(X_i)
\tag{6}
$$

接下来我们注意到，若非右侧的 $\log(X_i)$ 项，公式 (6) 本应表现出交换对称性。然而，该项与 $k$ 无关，因此可以被吸收到一个偏置项 $b_i$ 中，这个偏置项属于 $w_i$。最后，为 $\tilde{w}_k$ 添加一个额外的偏置项 $\tilde{b}_k$ 即可恢复对称性：

$$
w_i^T \tilde{w}_k + b_i + \tilde{b}_k = \log(X_{ik})
\tag{7}
$$

公式 (7) 是对公式 (1) 的一个巨大简化，但实际上它是有问题的（ill-defined），因为当其参数为零时，对数函数会发散。解决这个问题的一个方法是在对数中加入一个加性偏移，即 $\log(X_{ik}) \to \log(1 + X_{ik})$，这既保持了 $X$ 的稀疏性，又避免了发散。将共现矩阵的对数进行因式分解的想法与 LSA 密切相关，我们将使用这个最终得到的模型作为我们实验的基准。这个模型的一个主要缺点是，它对所有共现赋予了同等的权重，即使是那些很少发生或从未发生的共现。这类罕见的共现充满了噪声，并且比频繁的共现携带的信息要少得多——然而，仅仅是零值的元素就占了 $X$ 中 75-95% 的数据，具体取决于词汇量大小和语料库。

我们提出了一个新的加权最小二乘回归模型来解决这些问题。将公式 (7) 看作一个最小二乘问题，并在代价函数中引入一个权重函数 $f(X_{ij})$，我们得到如下模型：

$$
J = \sum_{i,j=1}^{V} f(X_{ij}) (w_i^T \tilde{w}_j + b_i + \tilde{b}_j - \log X_{ij})^2
\tag{8}
$$

其中 $V$ 是词汇表的大小。这个权重函数应遵循以下性质：
1.  $f(0) = 0$。如果将 $f$ 视为一个连续函数，它应该在 $x \to 0$ 时足够快地趋近于 0，以使得 $\lim_{x\to0} f(x) \log^2 x$ 是有限的。
2.  $f(x)$ 应该是一个非递减函数，这样罕见的共现就不会被赋予过高的权重。
3.  $f(x)$ 对于较大的 $x$ 值应该相对较小，这样频繁的共现也不会被赋予过高的权重。

当然，有很多函数满足这些性质，但我们发现有一类函数效果很好，其参数化形式如下：

$$
f(x) = \begin{cases} (x/x_{\max})^\alpha & \text{if } x < x_{\max} \\ 1 & \text{otherwise} \end{cases}
\tag{9}
$$

---

**图 1**: 权重函数 $f(x)$，其中 $\alpha = 3/4$。

---

模型的性能对截断值 $x_{\max}$ 依赖较弱，在我们所有的实验中，我们都将其固定为 $x_{\max} = 100$。我们发现 $\alpha = 3/4$ 比 $\alpha = 1$ 的线性版本有适度的改进。尽管我们为选择 $3/4$ 这个值只提供了经验性的动机，但有趣的是，在 Mikolov 等人 (2013a) 的研究中，也发现了一个类似的次幂缩放（fractional power scaling）能带来最佳性能。

#### 3.1. 与其他模型的关系

由于所有用于学习词向量的无监督方法最终都基于语料库的出现统计，这些模型之间应该存在共性。尽管如此，某些模型在这方面仍然有些不透明，特别是像 skip-gram 和 ivLBL 这样近期的基于窗口的方法。因此，在本小节中，我们将展示这些模型与我们提出的模型（如公式 (8) 所定义）之间的关系。

skip-gram 或 ivLBL 方法的出发点是一个模型 $Q_{ij}$，它表示词语 $j$ 出现在词语 $i$ 上下文中的概率。为具体起见，我们假设 $Q_{ij}$ 是一个 softmax 函数：

$$
Q_{ij} = \frac{\exp(w_i^T \tilde{w}_j)}{\sum_{k=1}^V \exp(w_i^T \tilde{w}_k)}
\tag{10}
$$

这些模型的大多数细节与我们的目的无关，除了它们试图在上下文窗口扫描整个语料库时最大化对数概率。训练是以在线、随机的方式进行的，但其隐含的全局目标函数可以写为：

$$
J = - \sum_{i \in \text{corpus}} \sum_{j \in \text{context}(i)} \log Q_{ij}
\tag{11}
$$

对这个求和中的每一项评估 softmax 的归一化因子都是代价高昂的。为了实现高效训练，skip-gram 和 ivLBL 模型引入了对 $Q_{ij}$ 的近似。然而，如果我们首先将那些具有相同 $i$ 和 $j$ 值的项组合在一起，公式 (11) 中的求和可以更有效地计算：

$$
J = - \sum_{i=1}^V \sum_{j=1}^V X_{ij} \log Q_{ij}
\tag{12}
$$

在这里，我们利用了这样一个事实：相同项的数量由共现矩阵 $X$ 给出。回忆一下我们的符号 $X_i = \sum_k X_{ik}$ 和 $P_{ij} = X_{ij}/X_i$，我们可以将 $J$ 重写为：

$$
J = - \sum_{i=1}^V X_i \sum_{j=1}^V P_{ij} \log Q_{ij} = \sum_{i=1}^V X_i H(P_i, Q_i)
\tag{13}
$$

其中 $H(P_i, Q_i)$ 是分布 $P_i$ 和 $Q_i$ 的交叉熵，我们以类似于 $X_i$ 的方式定义它。作为一个交叉熵误差的加权和，这个目标函数与公式 (8) 的加权最小二乘目标函数有某种形式上的相似之处。实际上，直接优化公式 (13) 是可能的，而不是使用 skip-gram 和 ivLBL 模型中使用的在线训练方法。我们可以将这个目标函数解释为一个“全局 skip-gram”模型，对其进行进一步研究可能会很有趣。另一方面，公式 (13) 表现出一些不理想的性质，在将其采纳为学习词向量的模型之前应当予以解决。

首先，交叉熵误差只是概率分布之间众多可能距离度量中的一种，它有一个不幸的性质，即带有长尾的分布通常被建模得很差，因为对不太可能发生的事件给予了过多的权重。此外，为了使该度量有界，它要求模型分布 $Q$ 被正确归一化。这在计算上造成了瓶颈，因为需要在整个词汇表上进行求和（见公式 (10)），因此我们希望考虑一种不需要 $Q$ 具有此属性的不同距离度量。一个自然的选择是最小二乘目标函数，其中 $Q$ 和 $P$ 中的归一化因子被舍弃：

$$
\hat{J} = \sum_{i,j} X_i (\hat{P}_{ij} - \hat{Q}_{ij})^2
\tag{14}
$$

其中 $\hat{P}_{ij} = X_{ij}$ 和 $\hat{Q}_{ij} = \exp(w_i^T \tilde{w}_j)$ 是未归一化的分布。在这个阶段，出现了另一个问题，即 $X_{ij}$ 通常会取非常大的值，这可能使优化变得复杂。一个有效的补救措施是最小化 $P$ 和 $Q$ 的对数的平方误差：

$$
\begin{align} \hat{J} &= \sum_{i,j} X_i (\log \hat{P}_{ij} - \log \hat{Q}_{ij})^2 \tag{15} \\ &= \sum_{i,j} X_i (w_i^T \tilde{w}_j - \log X_{ij})^2 \end{align}
$$

最后，我们观察到，虽然权重因子 $X_i$ 是由 skip-gram 和 ivLBL 模型固有的在线训练方法预先决定的，但它绝不保证是最优的。事实上，Mikolov 等人 (2013a) 观察到，通过过滤数据以减少高频词的有效权重因子，可以提高性能。考虑到这一点，我们引入了一个更通用的权重函数，我们可以自由地让它也依赖于上下文词。结果是：

$$
\hat{J} = \sum_{i,j} f(X_{ij}) (w_i^T \tilde{w}_j - \log X_{ij})^2
\tag{16}
$$

这与我们之前推导出的公式 (8) 的代价函数是等价的（参考：我们也可以在公式 (16) 中包含偏置项。）。

#### 3.2. 模型的复杂度

从公式 (8) 和权重函数 $f(X)$ 的显式形式可以看出，模型的计算复杂度取决于矩阵 $X$ 中非零元素的数量。由于这个数量总是小于矩阵的总条目数，模型的复杂度最差不会超过 $O(|V|^2)$。乍一看，这似乎比基于浅层窗口的方法（其复杂度与语料库大小 $|C|$ 成正比）有了实质性的改进。然而，典型的词汇表有数十万个词，所以 $|V|^2$ 可能是数千亿，这实际上比大多数语料库要大得多。因此，确定是否能对 $X$ 的非零元素数量给出一个更紧的界限是很重要的。

为了对 $X$ 中非零元素的数量做出任何具体的陈述，有必要对词共现的分布做一些假设。特别是，我们将假设词 $i$ 和词 $j$ 的共现次数 $X_{ij}$ 可以被建模为该词对的频率排名 $r_{ij}$ 的一个幂律函数：

$$
X_{ij} = \frac{k}{(r_{ij})^\alpha}
\tag{17}
$$

语料库中的总词数与共现矩阵 $X$ 的所有元素之和成正比：

$$
|C| \sim \sum_{ij} X_{ij} = \sum_{r=1}^{|X|} \frac{k}{r^\alpha} = k H_{|X|, \alpha}
\tag{18}
$$

这里我们用广义调和数 $H_{n,m}$ 重写了最后的和。和的上限 $|X|$ 是最大频率排名，它与矩阵 $X$ 中非零元素的数量一致。这个数字也等于公式 (17) 中使得 $X_{ij} \geq 1$ 的 $r$ 的最大值，即 $|X| = k^{1/\alpha}$。因此，我们可以将公式 (18) 写成：

$$
|C| \sim |X|^\alpha H_{|X|, \alpha}
\tag{19}
$$

我们感兴趣的是当 $|X|$ 和 $|C|$ 都很大时，它们之间的关系；因此我们可以自由地展开方程右侧关于大 $|X|$ 的表达式。为此，我们使用广义调和数的展开式 (Apostol, 1976)：

$$
H_{x,s} = \frac{x^{1-s}}{1-s} + \zeta(s) + O(x^{-s}) \quad \text{if } s > 0, s \neq 1
\tag{20}
$$

这得到：

$$
|C| \sim \frac{|X|}{1-\alpha} + \zeta(\alpha)|X|^\alpha + O(1)
\tag{21}
$$

其中 $\zeta(s)$ 是黎曼 zeta 函数。在 $|X|$ 很大的极限下，公式 (21) 右侧的两项中只有一项是相关的，具体是哪一项取决于 $\alpha$ 是否大于 1：

$$
|X| = \begin{cases} O(|C|) & \text{if } \alpha < 1, \\ O(|C|^{1/\alpha}) & \text{if } \alpha > 1 \end{cases}
\tag{22}
$$

对于本文研究的语料库，我们观察到 $X_{ij}$ 可以很好地被公式 (17) 建模，其中 $\alpha = 1.25$。在这种情况下，我们有 $|X| = O(|C|^{0.8})$。因此我们得出结论，该模型的复杂度远优于最坏情况的 $O(|V|^2)$，实际上它甚至比那些复杂度为 $O(|C|)$ 的在线窗口方法要好一些。

### 4. 实验

#### 4.1. 评估方法

我们在 Mikolov 等人 (2013a) 的词语类比任务、一系列词语相似度任务（如 Luong 等人 (2013) 中所述）以及 CoNLL-2003 共享的命名实体识别（NER）基准数据集 (Tjong Kim Sang and De Meulder, 2003) 上进行了实验。

**词语类比 (Word analogies)**。词语类比任务包含形如“a 之于 b 犹如 c 之于 __?” 的问题。该数据集包含 19,544 个此类问题，分为一个语义子集和一个句法子集。语义问题通常是关于人物或地点的类比，例如“雅典之于希腊犹如柏林之于 __?”。句法问题通常是关于动词时态或形容词形式的类比，例如“dance 之于 dancing 犹如 fly 之于 __?”。为了正确回答问题，模型应能唯一确定缺失的词，并且只有完全精确的匹配才算作正确。我们通过寻找一个词 $d$ 来回答问题“a 之于 b 犹如 c 之于 __?”，这个词的表示 $w_d$ 在余弦相似度上最接近于 $w_b - w_a + w_c$。（参考：http://lebret.ch/words/ ；http://code.google.com/p/word2vec/ ；Levy 等人 (2014) 引入了一种乘法类比评估方法 3CosMul，并报告了 68.24% 的准确率。该数值是在数据集的一个子集上评估的，因此未包含在表 2 中。在我们几乎所有的实验中，3CosMul 的表现都比余弦相似度差。）

---

**表 2**: 词语类比任务的结果，以准确率百分比给出。下划线分数是相似规模模型组中的最佳；**粗体**分数是总体最佳。HPCA 向量是公开可用的（参考：http://lebret.ch/words/ ）；(i)vLBL 的结果来自 (Mnih et al., 2013)；skip-gram (SG) 和 CBOW 的结果来自 (Mikolov et al., 2013a,b)；我们使用 `word2vec` 工具（参考：http://code.google.com/p/word2vec/ ）训练了 SG† 和 CBOW*。详见正文关于 SVD 模型的描述。

| 模型 | 维度 | 大小 | 语义 | 句法 | 总计 |
|:--- |:--- |:--- |:--- |:--- |:--- |
| ivLBL | 100 | 1.5B | 55.9 | 50.1 | 53.2 |
| HPCA | 100 | 1.6B | 4.2 | 16.4 | 10.8 |
| **GloVe** | **100** | **1.6B** | **67.5** | **54.3** | **60.3** |
| SG | 300 | 1B | $\underline{61}$ | $\underline{61}$ | $\underline{61}$ |
| CBOW | 300 | 1.6B | 16.1 | 52.6 | 36.1 |
| vLBL | 300 | 1.5B | 54.2 | 64.8 | 60.0 |
| ivLBL | 300 | 1.5B | 65.2 | 63.0 | 64.0 |
| **GloVe** | **300** | **1.6B** | **$\underline{80.8}$** | **61.5** | **$\underline{70.3}$** |
| SVD | 300 | 6B | 6.3 | 8.1 | 7.3 |
| SVD-S | 300 | 6B | 36.7 | 46.6 | 42.1 |
| SVD-L | 300 | 6B | 56.6 | 63.0 | 60.1 |
| CBOW† | 300 | 6B | 63.6 | 67.4 | 65.7 |
| SG† | 300 | 6B | $\underline{73.0}$ | 66.0 | $\underline{69.1}$ |
| **GloVe** | **300** | **6B** | **$\underline{77.4}$** | **$\underline{67.0}$** | **$\underline{71.7}$** |
| CBOW* | 1000 | 6B | 57.3 | 68.9 | 63.7 |
| SG* | 1000 | 6B | 66.1 | 65.1 | 65.6 |
| SVD-L | 300 | 42B | 38.4 | 58.2 | 49.2 |
| **GloVe** | **300** | **42B** | **$\underline{81.9}$** | **$\underline{69.3}$** | **$\underline{75.0}$** |

---

**词语相似度 (Word similarity)**。虽然类比任务是我们主要的关注点，因为它测试了有趣的向量空间子结构，我们也在表 3 中的一系列词语相似度任务上评估了我们的模型。这些任务包括 WordSim-353 (Finkelstein et al., 2001)、MC (Miller and Charles, 1991)、RG (Rubenstein and Goodenough, 1965)、SCWS (Huang et al., 2012) 和 RW (Luong et al., 2013)。

**命名实体识别 (Named entity recognition)**。CoNLL-2003 英文基准数据集（NER）是从路透社新闻文章中收集的一组文档，标注了四种实体类型：person（人名）、location（地名）、organization（组织机构）和 miscellaneous（其他）。我们在 CoNLL-03 训练集上训练模型，并在三个测试集上进行测试：1) CoNLL-03 测试数据；2) ACE Phase 2 (2001-02) 和 ACE-2003 数据；以及 3) MUC7 Formal Run 测试集。我们采用了 BIO2 标注标准，以及 Wang and Manning (2013) 中描述的所有预处理步骤。我们使用了一套全面的离散特征，这些特征随附于斯坦福 NER 模型的标准分发版中 (Finkel et al., 2005)。为 CoNLL-2003 训练数据集总共生成了 437,905 个离散特征。此外，我们还为一个五词上下文中的每个词添加了 50 维的向量，并将其作为连续特征使用。以这些特征为输入，我们训练了一个条件随机场（CRF）模型，其设置与 Wang and Manning (2013) 中的 $CRF_{join}$ 模型完全相同。

#### 4.2. 语料库与训练细节

我们在五个不同规模的语料库上训练了我们的模型：一个包含 10 亿词元的 2010 年维基百科转储；一个包含 16 亿词元的 2014 年维基百科转储；包含 43 亿词元的 Gigaword 5；Gigaword 5 与 Wikipedia 2014 的组合，共计 60 亿词元；以及来自 Common Crawl 的 420 亿词元的网络数据（参考：为了展示模型的可扩展性，我们还在一个更大的第六个语料库上进行了训练，该语料库包含 8400 亿词元的网络数据，但在这种情况下我们没有将词汇小写化，因此结果不具有直接可比性。对于在 Common Crawl 数据上训练的模型，我们使用了约 200 万词的大词汇表。）。我们使用斯坦福分词器对每个语料库进行分词并转为小写，构建一个包含 40 万个最常用词的词汇表，然后构建一个共现计数矩阵 $X$。在构建 $X$ 时，我们必须选择上下文窗口的大小，以及是否区分左侧上下文和右侧上下文。我们在下面探讨了这些选择的影响。在所有情况下，我们都使用一个递减的权重函数，即距离为 $d$ 个词的词对对总计数的贡献为 $1/d$。这是一种考虑以下事实的方式：距离非常远的词对预期包含的关于词语关系的信息较少。

在我们所有的实验中，我们设置 $x_{\max} = 100, \alpha = 3/4$，并使用 AdaGrad (Duchi et al., 2011) 来训练模型，从 $X$ 中随机抽样非零元素，初始学习率为 0.05。对于小于 300 维的向量，我们运行 50 次迭代，否则运行 100 次迭代（关于收敛速度的更多细节见 4.6 节）。除非另有说明，我们使用的上下文是左右各十个词。

该模型会生成两组词向量，$W$ 和 $\tilde{W}$。当 $X$ 是对称的时，$W$ 和 $\tilde{W}$ 是等价的，仅因其随机初始化而有所不同；这两组向量应该表现相当。另一方面，有证据表明，对于某些类型的神经网络，训练网络的多个实例然后将结果结合起来，可以帮助减少过拟合并降低噪声，从而普遍改善结果 (Ciresan et al., 2012)。考虑到这一点，我们选择使用 $W + \tilde{W}$ 的和作为我们的词向量。这样做通常会带来性能上的小幅提升，其中最大的提升出现在语义类比任务中。

---

**(a) 对称上下文**


**(b) 对称上下文**


**(c) 非对称上下文**

**图 2**: 类比任务的准确率随向量大小和窗口大小/类型变化的函数。所有模型都在 60 亿词元的语料库上训练。在 (a) 中，窗口大小为 10。在 (b) 和 (c) 中，向量大小为 100。

---

我们将我们的结果与各种最先进模型已发表的结果，以及我们自己使用 `word2vec` 工具和几种基于 SVD 的基线模型产生的结果进行比较。对于 `word2vec`，我们在 60 亿词元的语料库（Wikipedia 2014 + Gigaword 5）上训练了 skip-gram (SG†) 和连续词袋 (CBOW*) 模型，词汇表为前 40 万个最常用词，上下文窗口大小为 10。我们使用了 10 个负采样，这在 4.6 节中我们证明是该语料库的一个好选择。

对于 SVD 基线，我们生成一个截断矩阵 $X_{\text{trunc}}$，该矩阵仅保留每个词与前 10,000 个最常用词的共现频率信息。这一步在许多基于矩阵分解的方法中很典型，因为额外的列会贡献不成比例的零元素，而且这些方法在计算上也很昂贵。

该矩阵的奇异向量构成了基线 “SVD”。我们还评估了两个相关的基线：“SVD-S”，即我们对 $\sqrt{X_{\text{trunc}}}$ 进行 SVD 分解；以及 “SVD-L”，即我们对 $\log(1+X_{\text{trunc}})$ 进行 SVD 分解。这两种方法都有助于压缩 $X$ 中原本很大的数值范围。（参考：我们还研究了其他几种用于转换 $X$ 的加权方案；我们在此报告的是表现最好的。许多加权方案如 PPMI 会破坏 $X$ 的稀疏性，因此无法实际用于大型词汇表。对于较小的词汇表，这些信息论转换确实在词语相似度度量上表现良好，但它们在词语类比任务上表现很差。）

#### 4.3. 结果

我们在表 2 中展示了词语类比任务的结果。GloVe 模型的表现显著优于其他基线，通常在向量维度更小和语料库更小的情况下也是如此。我们使用 `word2vec` 工具得到的结果比大多数先前发表的结果要好一些。这归因于多种因素，包括我们选择使用负采样（通常比层次 softmax 效果更好）、负采样的数量以及语料库的选择。

我们证明了该模型可以轻松地在大型的 420 亿词元语料库上进行训练，并相应地获得了显著的性能提升。我们注意到，增大语料库大小并不能保证其他模型的结果会改善，正如 SVD-L 模型在更大语料库上性能下降所见。这个基本的 SVD 模型无法很好地扩展到大型语料库的事实，进一步证明了我们模型中提出的那种加权方案的必要性。

---

**表 3**: 词语相似度任务上的斯皮尔曼等级相关系数。所有向量均为 300 维。CBOW* 向量来自 `word2vec` 网站，其不同之处在于它们包含短语向量。

| 模型 | 大小 | WS353 | MC | RG | SCWS | RW |
|:--- |:--- |:--- |:--- |:--- |:--- |:--- |
| SVD | 6B | 35.3 | 35.1 | 42.5 | 38.3 | 25.6 |
| SVD-S | 6B | 56.5 | 71.5 | 71.0 | 53.6 | 34.7 |
| SVD-L | 6B | 65.7 | 72.7 | 75.1 | 56.5 | 37.0 |
| CBOW† | 6B | 57.2 | 65.6 | 68.2 | 57.0 | 32.5 |
| SG† | 6B | 62.8 | 65.2 | 69.7 | 58.1 | 37.2 |
| **GloVe** | **6B** | **65.8** | **72.7** | **77.8** | **53.9** | **38.1** |
| SVD-L | 42B | 74.0 | 76.4 | 74.1 | 58.3 | 39.9 |
| **GloVe** | **42B** | **75.9** | **83.6** | **82.9** | **59.6** | **47.8** |
| CBOW* | 100B | 68.4 | 79.6 | 75.4 | 59.4 | 45.5 |

---

表 3 显示了在五个不同词语相似度数据集上的结果。相似度分数是通过首先对词汇表中的每个特征进行归一化，然后计算余弦相似度得到的。我们计算了这个分数与人类判断之间的斯皮尔曼等级相关系数。CBOW* 表示 `word2vec` 网站上提供的向量，这些向量是在 1000 亿词的新闻数据上用词和短语向量训练的。GloVe 在使用不到一半大小的语料库的情况下表现优于它。

表 4 显示了在 NER 任务上使用 CRF 模型的结果。L-BFGS 训练在开发集上连续 25 次迭代没有改善时终止。除此之外，所有配置都与 Wang 和 Manning (2013) 使用的相同。标记为 `Discrete` 的模型是使用一套全面的离散特征的基线，这些特征来自斯坦福 NER 模型的标准分发版，但没有词向量特征。除了前面讨论的 HPCA 和 SVD 模型外，我们还与 Huang 等人 (2012) 的模型 (HSMN) 和 Collobert and Weston (2008) 的模型 (CW) 进行了比较。我们使用 `word2vec` 工具训练了 CBOW 模型。（参考：我们使用了与上面相同的参数，只是在这种情况下我们发现 5 个负采样比 10 个效果稍好。）GloVe 模型在所有评估指标上都优于所有其他方法，除了 CoNLL 测试集，HPCA 方法在该测试集上略胜一筹。我们得出结论，GloVe 向量在下游 NLP 任务中是有用的，正如 Turian 等人 (2010) 首次对神经向量所展示的那样。

---

**表 4**: 在 NER 任务上的 F1 分数，使用 50 维向量。`Discrete` 是不使用词向量的基线。我们使用了 HPCA、HSMN 和 CW 的公开可用向量。详见正文。

| 模型 | Dev | Test | ACE | MUC7 |
|:--- |:--- |:--- |:--- |:--- |
| Discrete | 91.0 | 85.4 | 77.4 | 73.4 |
| SVD | 90.8 | 85.7 | 77.3 | 73.7 |
| SVD-S | 91.0 | 85.5 | 77.6 | 74.3 |
| SVD-L | 90.5 | 84.8 | 73.6 | 71.5 |
| HPCA | 92.6 | **88.7** | 81.7 | 80.7 |
| HSMN | 90.5 | 85.7 | 78.7 | 74.7 |
| CW | 92.2 | 87.4 | 81.7 | 80.2 |
| CBOW | 93.1 | 88.2 | 82.2 | 81.1 |
| **GloVe** | **93.2** | **88.3** | **82.9** | **82.2** |

---

#### 4.4. 模型分析：向量长度与上下文大小

在图 2 中，我们展示了改变向量长度和上下文窗口的实验结果。一个延伸到目标词左右两侧的上下文窗口被称为对称的，而一个只延伸到左侧的被称为非对称的。在 (a) 中，我们观察到对于大于约 200 维的向量，回报递减。在 (b) 和 (c) 中，我们研究了改变对称和非对称上下文窗口大小的影响。对于小的、非对称的上下文窗口，句法子任务的性能更好，这与句法信息主要来自直接上下文并且强烈依赖于词序的直觉相符。另一方面，语义信息更频繁地是非局部的，并且可以通过更大的窗口大小捕捉到更多。

#### 4.5. 模型分析：语料库大小

在图 3 中，我们展示了在不同语料库上训练的 300 维向量在词语类比任务上的性能。在句法子任务上，随着语料库大小的增加，性能呈单调增长。这是意料之中的，因为更大的语料库通常能产生更好的统计数据。有趣的是，对于语义子任务，同样的趋势并不成立，在较小的维基百科语料库上训练的模型比在较大的 Gigaword 语料库上训练的模型表现更好。这可能是由于类比数据集中有大量的基于城市和国家的类比，而维基百科对于大多数这样的地点都有相当全面的文章。此外，维基百科的条目会更新以吸收新知识，而 Gigaword 则是一个固定的新闻库，信息可能过时甚至不正确。

---

**图 3**: 在不同语料库上训练的 300 维向量在类比任务上的准确率。

---

#### 4.6. 模型分析：运行时间

总运行时间分为填充 $X$ 矩阵和训练模型两部分。前者取决于许多因素，包括窗口大小、词汇量大小和语料库大小。虽然我们没有这样做，但这一步可以很容易地在多台机器上并行化（例如，参见 Lebret 和 Collobert (2014) 的一些基准测试）。在一台双 2.1GHz Intel Xeon E5-2658 机器的单线程上，使用 10 个词的对称上下文窗口、40 万词的词汇表和一个 60 亿词元的语料库，填充 $X$ 大约需要 85 分钟。给定 $X$ 后，训练模型所需的时间取决于向量大小和迭代次数。对于上述设置的 300 维向量（并使用上述机器的所有 32 个核心），单次迭代需要 14 分钟。学习曲线的图表见图 4。

#### 4.7. 模型分析：与 `word2vec` 的比较

对 GloVe 与 `word2vec` 进行严格的定量比较是复杂的，因为存在许多对性能有强烈影响的参数。我们通过将向量长度、上下文窗口大小、语料库和词汇量大小设置为前一小节中提到的配置，来控制我们在 4.4 和 4.5 节中确定的主要变异来源。

需要控制的最重要的剩余变量是训练时间。对于 GloVe，相关参数是训练迭代的次数。对于 `word2vec`，显而易见的选择是训练轮数（epochs）。不幸的是，该代码目前只为单轮训练设计：它指定了一个特定于单次遍历数据的学习计划，使得修改为多轮训练成为一项不小的任务。另一个选择是改变负采样的数量。增加负采样有效地增加了模型看到的训练词的数量，因此在某些方面它类似于额外的轮数。

我们将任何未指定的参数设置为它们的默认值，假设它们接近最优，尽管我们承认这种简化在更彻底的分析中应该被放宽。

在图 4 中，我们绘制了类比任务的总体性能随训练时间变化的函数。底部的两个 x 轴表示 GloVe 对应的训练迭代次数和 `word2vec` 的负采样数量。我们注意到，如果负采样的数量增加到大约 10 以上，`word2vec` 的性能实际上会下降。这大概是因为负采样方法没有很好地逼近目标概率分布。（参考：相比之下，噪声对比估计是一种近似方法，其性能会随着负采样数量的增加而提高。在 Mnih 等人 (2013) 的表 1 中，类比任务的准确率是负采样数量的非递减函数。）

对于相同的语料库、词汇表、窗口大小和训练时间，GloVe 始终优于 `word2vec`。它能更快地达到更好的结果，并且无论速度如何，都能获得最佳结果。

---

**(a) GloVe vs CBOW**

**(b) GloVe vs Skip-Gram**

**图 4**: 词语类比任务的总体准确率随训练时间变化的函数，训练时间由 GloVe 的迭代次数和 CBOW (a) 与 skip-gram (b) 的负采样数量决定。在所有情况下，我们都在相同的 60 亿词元语料库（Wikipedia 2014 + Gigaword 5）上训练 300 维向量，使用相同的 40 万词词汇表，并使用大小为 10 的对称上下文窗口。

---

### 5. 结论

最近，关于分布式词语表示应该从基于计数的方法还是基于预测的方法学习的问题引起了相当大的关注。目前，基于预测的模型获得了大量支持；例如，Baroni 等人 (2014) 认为这些模型在一系列任务中表现更佳。在这项工作中，我们认为这两类方法在根本层面上并没有显著不同，因为它们都探究了语料库底层的共现统计数据，但基于计数的方法捕捉全局统计数据的效率可能更具优势。

我们构建了一个模型，它利用了计数数据的主要好处，同时捕捉了像 `word2vec` 这样的近期基于对数双线性预测的方法中普遍存在的有意义的线性子结构。结果就是 GloVe，一种新的用于无监督学习词语表示的全局对数双线性回归模型，它在词语类比、词语相似度和命名实体识别任务上的表现优于其他模型。

### 致谢

我们感谢匿名审稿人提出的宝贵意见。斯坦福大学感谢美国国防威胁降低局（DTRA）通过空军研究实验室（AFRL）合同 FA8650-10-C-7020 的支持，以及美国国防高级研究计划局（DARPA）的“深度探索与文本过滤”（DEFT）项目通过 AFRL 合同 FA8750-13-2-0040 的支持。本材料中表达的任何观点、发现、结论或建议均为作者的观点，不一定反映 DTRA、AFRL、DEFT 或美国政府的观点。